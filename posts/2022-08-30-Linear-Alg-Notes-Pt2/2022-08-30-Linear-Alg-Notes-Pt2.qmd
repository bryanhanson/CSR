---
title: "Notes on Linear Algebra Part 2"
description: "Motivations - What connects all this stuff?"
date: "2022-08-14"
categories: [R, Linear Algebra]
draft: true
---

#### TL;DR

* Linear systems of equations are at the heart, not surprisingly, of linear algebra.
* A key application is linear regression.
* Solving the needed equations requires inverting a matrix.
* something something
* Other methods are built on top of the basic operations.
* Notations and symbols are killer.

---

For Part 1 of this series, see [here](https://chemospec.org/posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html).

If you open a linear algebra text, it's quickly apparent at how complex the field is.  There are so many special types of matrices, so many different decompositions of matrices.  Why are all these needed?  What's really important?  What are the threads that tie the different concepts together?  As someone who is trying to improve their understanding of the field, especially with regard to its applications in chemometrics, it can be a tough slog.

In this post I'm going to try to make an argument about how to look at the field so that one can tie the topics together and progress in one's understanding. I'm looking for the core concepts and their connections.  Though I cover some math details here, they are pretty light: the conceptual connections are far more important.  We can deal with the math details later, probably much more easily if we know why are doing the harder work.

In this post, matrices, including row and column vectors, will be shown in bold e.g. $\mathbf{A}$ while scalars and variables will be shown in italics, e.g. $n$.

## Systems of Equations

If you've had algebra, you have certainly run into "system of equations" such as the following:

$$
\begin{multline}
x + 2y -3z = 3 \\
2x - y - z = 11 \\
3x + 2y + z = -5 \\
\end{multline}
$$ {#eq-1}

In algebra, such systems can be solved several ways, for instance by isolating one or more variables and substituting, or geometrically (particularly for 2D systems, by plotting the lines and looking for the intersection).  Once there are more than a few variables however, the only manageable way to solve them is with matrix operations, or more explicitly, linear algebra.  **This sort of problem is the core of linear algebra, and the reason the field is called linear algebra.**

To solve the system above using linear algebra, we have to write it in the form of matrices and column vectors:

$$
\begin{bmatrix}
1 & 2 & -3 \\ 2 & -1 & -1 \\ 3 & 2 & 1 \\
\end{bmatrix}
\begin{bmatrix}
x \\ y \\ z
\end{bmatrix} = 
\begin{bmatrix}
3 \\ 11 \\ -5
\end{bmatrix}
$$ {#eq-2}

or more generally

$$
\mathbf{A}\mathbf{x} = \mathbf{b}
$$ {#eq-3}

where $\mathbf{A}$ is the matrix of coefficients, $\mathbf{x}$ is the column vector of variable names[^6] and $\mathbf{b}$ is a column vector of constants.  Notice that these matrices are conformable:[^3]

$$
\mathbf{A}^{3 \times 3}\mathbf{x}^{3 \times 1} = \mathbf{b}^{3 \times 1}
$$ {#eq-4}

To solve such a system, when we have $n$ unknowns, we need $n$ equations.[^1]  *This means that $\mathbf{A}$ has to be a square matrix, and square matrices play a special role in linear algebra.* I'm not sure this point is always conveyed clearly when this material is introduced.

To find the values of $\mathbf{x} = x, y, z$[^7], we can do a little rearranging following the rules of linear algebra and matrix operations.  First we pre-multiply both sides by the inverse of $\mathbf{A}$, which then gives us the identity matrix $\mathbf{I}$, which drops out.[^2]

$$
\mathbf{A}^{-1}\mathbf{A}\mathbf{x} = \mathbf{I}\mathbf{x} = \mathbf{x} = \mathbf{A}^{-1}\mathbf{b}
$$ {#eq-5}

So it's all sounding pretty simple right?  Ha.  This is actually where things break down.  For this to work, $\mathbf{A}$ must be invertible, which is not always the case.[^4]  If there is no inverse, then the system of equations either has no solution or infinite solutions.  *So finding the inverse of a matrix, or discovering it doesn't exist, is essential to solving these systems of linear equations.*  More on this eventually, but for now, *we know $\mathbf{A}$ must be a square matrix and we hope it is invertible.*

### R Functions

## A Key Application

We learn in algebra that a line takes the form $y = mx + b$. If one has measurements in the form of $x, y$ pairs that one expects to fit to a line, we need linear regression.  Carrying out a linear regression is arguably one of the most important, and certainly a very common application of the linear systems described above.  One can get the values of $m$ and $b$ by hand using algebra, but any computer will solve the system using a matrix approach.[^5]  Consider this data:

$$
\begin{matrix}
x & y \\
2.1 & 11.8 \\
0.9 & 7.2 \\
3.9 & 21.5 \\
3.2 & 17.2 \\
5.1 & 26.8 \\
\end{matrix}
$$ {#eq-6}

To express this in a matrix form, we would recast

$$
y = mx + b
$$ {#eq-6}

into

$$
\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}
$$ {#eq-10}

where:

* $\mathbf{y}$ is the column vector of $y$ values.  That seems sensible.
* $\mathbf{X}$ is a matrix composed of a column of ones plus a column of the $x$ values.  This is called a [design matrix](https://en.wikipedia.org/wiki/Design_matrix#Simple_linear_regression).  At least here $\mathbf{X}$ contains only $x$ values as variables.
* $\mathbf{\beta}$ is a column vector of coefficients (including, as we will see, the values of $m$ and $b$ if we are thinking back $y = mx + b$).
* $\mathbf{\epsilon}$ is new, it is a column vector giving the errors at each point.

With our data above, this looks like:

$$
\begin{bmatrix}
11.8 \\
7.2 \\
21.5 \\
17.2 \\
26.8 \\
\end{bmatrix}
=
\begin{bmatrix}
1 & 11.8 \\
1 & 7.2 \\
1 & 21.5 \\
1 & 17.2 \\
1 & 26.8 \\
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \epsilon_5
\end{bmatrix}
$$ {#eq-8}

If we multiply this out, each row works out to be an instance of $y_i = \beta_1 x_i + \beta_0$ so you can hopefully see that $\beta_1$ corresponds to $m$ and $\beta_0$ corresponds to $b$.[^8]

This looks similar to $\mathbf{A}\mathbf{x} = \mathbf{b}$ seen in @eq-3, if you set $\mathbf{A}$ to $\mathbf{X}$, $\mathbf{x}$ to $\beta$ and $\mathbf{b}$ to $\mathbf{y}$:

$$
\begin{multline}
\mathbf{y} \approx \mathbf{X}\mathbf{\beta} \\
\mathbf{b} \approx \mathbf{A}\mathbf{x} \\
\end{multline}
$$ {#eq-11}

This contortion of symbols is pretty fubar'd but actually we can salvage it.

The difference is that being composed of real data, presumably with measurement errors, there is not an exact solution to $\mathbf{A}\mathbf{x} = \mathbf{b}$ due to the error term.  There is however, an approximate solution, which is what is meant when we say we are looking for the line of best fit.  This is how linear regression is carried out on a computer.  The relevant equation is:

$$
\hat{\beta} = (\mathbf{X}^{\mathsf{T}}\mathbf{X})^{-1}\mathbf{X}^{\mathsf{T}}y
$$ {#eq-9}

The key point here is that once again we need to invert a matrix to solve this.  The details of where @eq-9 comes from are covered in a number of places (but I will add here that $\hat{\beta}$ refers to the best estimate of $\beta$).

### R Functions




 (Singh 1.8.4). So finding the inverse is important.  If there is no inverse then the system of equations either has no solutions or infinite solutions (Singh pg 115, also section 1.7.2 for graphical definition, BS 3.2).




[^1]: Remember "story problems" where you had to read closely to express what was given in terms of equations, and find enough equations? "If Sally bought 10 pieces of candy and a drink for $1.50..."
[^2]: The inverse of a matrix is analogous to dividing a variable by itself, since it leads to that variable canceling out and thus simplifying the equation.  However, strictly speaking there is no operation that qualifies as division in the matrix world.
[^3]: Conformable means that the number of columns in the first matrix equals the number of rows in the second matrix. This is necessary because of the dot product definition of matrix multiplication.  More details [here](https://chemospec.org/posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html).
[^4]: For a matrix $\mathbf{A}$ to be invertible, there must exist another matrix $\mathbf{B}$ such that $\mathbf{A}\mathbf{B} = $\mathbf{B}\mathbf{A} = 1$.  However, this definition doesn't offer any clues about how we might find the inverse.
[^5]: A very good discussion of the algebraic approach is available [here](https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line).
[^6]: Here we have the slightly unfortunate circumstance where symbol conventions cannot be completely harmonized.  We are saying that $\mathbf{x} = x, y, z$ which seems a bit silly since vector $\mathbf{x}$ contains $y$ and $z$ components in addition to $x$.  I ask you to accept this for two reasons: First, most linear algebra texts use the symbols in @eq-3 as the general form for this topic, so if you go to study this further that's what you'll find. Second, I feel like using $x$, $y$ and $z$ in @eq-1 will be familar to the most people.  If you want to get rid of this infelicity, then you have to write @eq-1 (in part) as $x_1 + 2x_2 + 3x_3 = 3$ which I think clouds the interpretation.  Perhaps  however you feel my choices are equally bad.
[^7]: We could also write this as $\mathbf{x} = (x, y, z)^\mathsf{T}$ to emphasize that it is a column vector. One might prefer this because the only vector one can write in a row of text is a row vector, so if we mean a column vector we ought to show it transposed.
[^8]: This is another example of an infelicity of symbol conventions. The typical math/statistics text symbols are not the same as the symbols a student in Physics 101 would likely encounter.