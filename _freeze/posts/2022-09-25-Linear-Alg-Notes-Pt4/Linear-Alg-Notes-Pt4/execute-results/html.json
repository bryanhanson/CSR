{
  "hash": "0ff2873b842c4c256f28fc7b2daa57da",
  "result": {
    "markdown": "---\ntitle: \"Notes on Linear Algebra Part 4\"\ndescription: \"A Taxonomy of Matrices\"\ndate: \"2022-09-25\"\ncategories: [R, Linear Algebra]\nformat:\n  html:\n    toc: true\n---\n\n\nSeries: [Part 1](https://chemospec.org/posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html) [Part 2](https://chemospec.org/posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html) [Part 3](https://chemospec.org/posts/2022-09-10-Linear-Alg-Notes-Pt3/Linear-Alg-Notes-Pt3.html)\n\nBack in Part 2 I mentioned some of the challenges of learning linear algebra.  One of those challenges is making sense of all the special types of matrices one encounters.  In this post I hope to shed a little light on that topic.\n\n# A Taxonomy of Matrices\n\nI am strongly drawn to thinking in terms of categories and relationships.  I find visual presentations like phylogenies showing the relationships between species very useful.  In the course of my linear algebra journey, I came across an interesting Venn diagram developed by the very creative thinker [Kenji Hiranabe](https://github.com/kenjihiranabe/The-Art-of-Linear-Algebra). The diagram is discussed at [Matrix World](https://anagileway.com/2020/09/29/matrix-world-in-linear-algebra-for-everyone/), but the latest version is at the Github link.  A Venn diagram is a useful format, but I was inspired to recast the information in different format.  @fig-flow shows a taxonomy I created using a portion of the information in Hiranabe's Venn diagram.[^6]  The taxonomy is primarily organized around what I am calling the *structure* of a matrix: what does it look like upon visual inspection (which of course only really works for small matrices)?  To me at least, structure is one of the most obvious characteristics of a matrix: an upper triangular matrix really stands out as for instance.  Secondarily, the taxonomy includes a number of queries that one can ask about a matrix: for instance, is the matrix invertible?  We'll need to expand on all of this of course, but first take a look at the figure.[^1]\n\n\n\n```{mermaid}\n%%| label: fig-flow\n%%| fig-cap: \"Heiarchical relationships between different types of matrices. *Blue Rectangles* denote matrices with particular, recognizable *structures*.  *Pink Hexagons* indicate properties that can be *queried*.\"\nflowchart TD\nA(all matrices <br/> n x m) --> C(row matrices <br/> 1 x n)\nA --> D(column matrices <br/> n x 1)\nA ---> B(square matrices <br/> n x n)\nB --> E(upper triangular<br/>matrices)\nB --> F(lower triangular<br/>matrices)\nB --> G{{<b>either</b><br/>is singular?}}\nB --> H{{<b>or</b><br/>is invertible?}}\nH --> I{{is diagonalizable?}}\nI --> J{{is normal?}}\nJ --> K(symmetric)\nK --> L(diagonal)\nL --> M(identity)\nJ --> N{{is orthogonal?}}\nN --> M\nstyle G fill:#FFF0F5\nstyle H fill:#FFF0F5\nstyle I fill:#FFF0F5\nstyle J fill:#FFF0F5\nstyle N fill:#FFF0F5\n```\n\n\n# Unpacking the Taxonomy\n\n## Structure Examples\n\nLet's use `R` to construct and inspect examples of each type of matrix.  We'll use integer matrices to keep the print output nice and neat, but of course real numbers could be used as well. Most of these are pretty straightforward so we'll we'll keep comments to a minimum for the simple cases.\n\n### Rectangular Matrix $m \\times n$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_rect <- matrix(1:12, nrow = 3) # if you give nrow,\nA_rect # R will compute ncol from the length of the data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n```\n:::\n:::\n\n\nNotice that `R` is \"column major\" meaning data fills the first column, then the second column and so forth.\n\n### Row Matrix/Vector $1 \\times n$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_row <- matrix(1:4, nrow = 1) # row matrix/vector\nA_row\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n```\n:::\n:::\n\n\n### Column Matrix/Vector $m \\times 1$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_col <- matrix(1:4, ncol = 1) # column matrix/vector\nA_col\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[4,]    4\n```\n:::\n:::\n\n\nKeep in mind that to save space in a text-dense document one would often write `A_col^T` as its transpose.[^2]\n\n### Square Matrix $n \\times n$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_sq <- matrix(1:9, nrow = 3)\nA_sq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n```\n:::\n:::\n\n\n### Upper and Lower Triangular Matrices\n\nCreating an upper triangular matrix requires a few more steps.  Function `upper.tri()` returns a logical matrix which can be used as a mask to select entries.  Function `lower.tri()` can be used similarly.  Both functions have an argument `diag = TRUE/FALSE` indicating whether to include the diagonal.[^4]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nupper.tri(A_sq, diag = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1]  [,2] [,3]\n[1,]  TRUE  TRUE TRUE\n[2,] FALSE  TRUE TRUE\n[3,] FALSE FALSE TRUE\n```\n:::\n\n```{.r .cell-code}\nA_upper <- A_sq[upper.tri(A_sq)] # gives a logical matrix\nA_upper # notice that a vector is returned, not quite what might have been expected!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4 7 8\n```\n:::\n\n```{.r .cell-code}\nA_upper <- A_sq # instead, create a copy to be modified\nA_upper[lower.tri(A_upper)] <- 0L # assign the lower entries to zero\nA_upper\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    0    5    8\n[3,]    0    0    9\n```\n:::\n:::\n\nNotice to create an upper triangular matrix we use `lower.tri()` to assign zeros to the lower part of an existing matrix.\n\n### Identity Matrix\n\nIf you give `diag()` a single value it defines the  dimensions and creates a matrix with ones on the diagonal, giving an identity matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_ident <- diag(4)\nA_ident\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n```\n:::\n:::\n\n\n### Diagonal Matrix\n\nIf instead you give `diag()` a vector of values these go on the diagonal and the length of the vector determines the dimensions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_diag <- diag(1:4)\nA_diag\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    2    0    0\n[3,]    0    0    3    0\n[4,]    0    0    0    4\n```\n:::\n:::\n\n\n### Symmetric Matrices\n\nMatrices created by `diag()` are symmetric matrices, but any matrix where $a_{ij} = a_{ji}$ is symmetric.  There is no general function to create symmetric matrices since there is no way to know what data should be used.  However, one can ask if a matrix is symmetric, using the function `isSymmetric()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nisSymmetric(A_diag)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n## The Queries\n\nLet's take the queries in the taxonomy in order, as the heirarchy is important.\n\n### Is the Matrix Singular or Invertible?\n\nA singular matrix is one in which one or more rows are multiples of another row, or alternatively, one or more columns are multiples of another column.  Why do we care? Well, it turns out a singular matrix is a bit of a dead end, you can't do much with it.  An invertible matrix, however, is a very useful entity and has many applications.  What is an invertible matrix?  In simple terms, being invertible means the matrix has an inverse.  This is not the same as the algebraic definition of an inverse, which is related to division:\n\n$$\nx^{-1} = \\frac{1}{x}\n$$ {#eq-1}\n\nInstead, for matrices, invertibility of $\\mathbf{A}$ is defined as the existence of another matrix $\\mathbf{B}$ such that\n\n$$\n\\mathbf{A}\\mathbf{B} = \\mathbf{B}\\mathbf{A} = \\mathbf{I}\n$$ {#eq-2}\n\nJust as $x^{-1}$ cancels out $x$ in $x^{-1}x = \\frac{x}{x} = 1$, $\\mathbf{B}$ cancels out $\\mathbf{A}$ to give the identity matrix.  In other words, $\\mathbf{B}$ is really $\\mathbf{A}^{-1}$.\n\nA singular matrix has determinant of zero.  On the other hand, an invertible matrix has a non-zero determinant.  So to determine which type of matrix we have before us, we can simply compute the determinant.\n\nLet's look at a few simple examples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_singular <- matrix(c(1, -2, -3, 6), nrow = 2, ncol = 2)\nA_singular # notice that col 2 is col 1 * -3, they are not independent\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    1   -3\n[2,]   -2    6\n```\n:::\n\n```{.r .cell-code}\ndet(A_singular)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nA_invertible <- matrix(c(2, 2, 7, 8), nrow = 2, ncol = 2)\nA_invertible\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    2    7\n[2,]    2    8\n```\n:::\n\n```{.r .cell-code}\ndet(A_invertible)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\n### Is the Matrix Diagonalizable?\n\nA matrix $\\mathbf{A}$ that is diagonalizable can be expressed as:\n\n$$\n\\mathbf{\\Lambda} = \\mathbf{X}^{-1}\\mathbf{A}\\mathbf{X}\n$$ {#eq-3}\n\nwhere $\\mathbf{\\Lambda}$ is a diagonal matrix -- the diagonalized version of the original matrix $\\mathbf{A}$.  How do we find out if this is possible, and if possible, what are the values of $\\mathbf{X}$ and $\\mathbf{\\Lambda}$?  The answer is to decompose $\\mathbf{A}$ using the eigendecomposition:\n\n$$\n\\mathbf{A} = \\mathbf{X}\\mathbf{\\Lambda}\\mathbf{X}^{-1}\n$$ {#eq-4}\n\nNow there is a lot to know about the eigendecomposition, but for now let's just focus on a few key points:\n\n* The columns of $\\mathbf{X}$ contains the eigenvectors.  Eigenvectors are the most natural basis for describing the data in $\\mathbf{A}$.[^3]\n* $\\mathbf{\\Lambda}$ is a diagonal matrix with the eigenvalues on the diagonal, in descending order.  The individual eigenvalues are typically denoted $\\lambda_i$.\n* Eigenvectors and eigenvalues always come in pairs.\n\nWe can answer the original question by using the `eigen()` function in `R`.  Let's do an example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_eigen <- matrix(c(1, 0, 2, 2, 3, -4, 0, 0, 2), ncol = 3)\nA_eigen\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    2    0\n[2,]    0    3    0\n[3,]    2   -4    2\n```\n:::\n\n```{.r .cell-code}\neA <- eigen(A_eigen)\neA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1] 3 2 1\n\n$vectors\n           [,1] [,2]       [,3]\n[1,]  0.4082483    0  0.4472136\n[2,]  0.4082483    0  0.0000000\n[3,] -0.8164966    1 -0.8944272\n```\n:::\n:::\n\n\nSince `eigen(A_eigen)` was successful, we can conclude that `A_eigen` was diagonalizable.  You can see the eigenvalues and eigenvectors in the returned value.  We can reconstruct `A_eigen` using @eq-4:\n\n\n::: {.cell}\n\n```{.r .cell-code}\neA$vectors %*% diag(eA$values) %*% solve(eA$vectors)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    2    0\n[2,]    0    3    0\n[3,]    2   -4    2\n```\n:::\n:::\n\nRemember, `diag()` creates a matrix with the values along the diagonal, and `solve()` computes the inverse when it gets only one argument.\n\n\nThe only loose end is which matrices are *not* diagonalizable?  These are covered in this [Wikipedia article](https://en.wikipedia.org/wiki/Diagonalizable_matrix#Matrices_that_are_not_diagonalizable); briefly, most non-diagonalizable matrices are fairly exotic and real data sets will likely not be a problem.\n\n::: {.callout-tip collapse=\"true\"}\n## Nuances About the Presentation of \"Eigenstuff\"\n\nIn texts, eigenvalues and eigenvectors are universally introduced as a scaling relationship\n\n$$\n\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\n$$ {#eq-4a}\n\nwhere $\\mathbf{v}$ is a column eigenvector and $\\lambda$ is scalar eigenvalue.  One says \"$\\mathbf{A}$ scales $\\mathbf{v}$ by a factor of $\\lambda$.\" A single vector is used as one can readily illustrate how that vector grows or shrinks in length when multiplied by $\\lambda$.  Let's call this the \"bottom up\" explanation.\n\nLet's check that is true using our values from above by extracting the first eigenvector and eigenvalue from `eA`. Notice that we are using regular multiplication on the right-hand-side, i.e. `*`, rather than `%*%`, because `eA$values[1]` is a scalar. Also on the right-hand-side, we have to add `drop = FALSE` to the subsetting process or the result is no longer a matrix.[^5]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nisTRUE(all.equal(\n  A_eigen %*% eA$vectors[,1],\n  eA$values[1] * eA$vectors[,1, drop = FALSE]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nIf instead we start from @eq-4 and rearrange it to show the relationship between $\\mathbf{A}$ and $\\mathbf{\\Lambda}$ we get:\n\n$$\n\\mathbf{A}\\mathbf{X} = \\mathbf{X}\\mathbf{\\Lambda}\n$$ {#eq-4b}\n\nLet's call this the \"top down\" explanation.  We can verify this as well, making sure to convert `eA$values` to a diagonal matrix as the values are stored as a vector to save space.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nisTRUE(all.equal(A_eigen %*% eA$vectors, eA$vectors %*% diag(eA$values)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nNotice that in @eq-4b $\\Lambda$ is on the right of $\\mathbf{X}$, but in @eq-4a the corresponding value, $\\lambda$, is to the left of $\\mathbf{v}$.  This is a bit confusing until one realizes that @eq-4a could have been written\n\n$$\n\\mathbf{A}\\mathbf{v} = \\mathbf{v}\\lambda\n$$\n\nsince $\\lambda$ is a scalar.  It's too bad that the usual, bottom up, presentation seems to conflict with the top down approach.  Perhaps @eq-4a is a historical artifact.\n\n:::\n\n### Is the Matrix Normal?\n\nA normal matrix is one where $\\mathbf{A}^{\\mathsf{T}}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{\\mathsf{T}}$. As far as I know, there is no function in `R` to check this condition, but we'll write our own in a moment.  One reason being \"normal\" is interesting is if $\\mathbf{A}$ is a normal matrix, then the results of the eigendecomposition change slightly:\n\n$$\n\\mathbf{A} = \\mathbf{O}\\mathbf{\\Lambda}\\mathbf{O}^{-1}\n$$ {#eq-5}\n\nwhere $\\mathbf{O}$ is an orthogonal matrix, which we'll talk about next.\n\n### Is the Matrix Orthogonal?\n\nAn orthogonal matrix takes the definition of a normal matrix one step further: $\\mathbf{A}^{\\mathsf{T}}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{\\mathsf{T}} = \\mathbb{I}$.  If a matrix is orthogonal, then its transpose is equal to its inverse: $\\mathbf{A}^{-1} = \\mathbf{A}^{\\mathsf{T}}$, which of course makes any special computation of the inverse unnecessary.  This is a significant advantage in computations.\n\nTo aid our learning, let's write a simple function that will report if a matrix is normal, orthogonal, or neither.[^7]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_or_orthogonal <- function(M) {\n  if (!inherits(M, \"matrix\")) stop(\"M must be a matrix\")\n  norm <- orthog <- FALSE\n  tst1 <- M %*% t(M)\n  tst2 <- t(M) %*% M\n  norm <- isTRUE(all.equal(tst1, tst2))\n  if (norm) orthog <- isTRUE(all.equal(tst1, diag(dim(M)[1])))\n  if (orthog) message(\"This matrix is orthogonal\\n\") else \n    if (norm) message(\"This matrix is normal\\n\") else\n    message(\"This matrix is neither orthogonal nor normal\\n\")\n  invisible(NULL)\n}\n```\n:::\n\n\nAnd let's run a couple of tests.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_or_orthogonal(A_singular)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThis matrix is neither orthogonal nor normal\n```\n:::\n\n```{.r .cell-code}\nN <- matrix(c(1, 0, 1, 1, 1, 0, 0, 1, 1), nrow = 3)\nnormal_or_orthogonal(N)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThis matrix is normal\n```\n:::\n\n```{.r .cell-code}\nnormal_or_orthogonal(diag(3)) # the identity matrix is orthogonal\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThis matrix is orthogonal\n```\n:::\n\n```{.r .cell-code}\nO <- matrix(c(0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0), nrow = 4)\nnormal_or_orthogonal(O)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThis matrix is orthogonal\n```\n:::\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n## Some other properties of an orthogonal matrix\n\nThe columns of an orthogonal matrix are orthogonal to each other.  We can show this by taking the dot product between any pair of columns. Remember is the dot product is zero the vectors are orthogonal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt(O[,1]) %*% O[,2] # col 1 dot col 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    0\n```\n:::\n\n```{.r .cell-code}\nt(O[,1]) %*% O[,3] # col 1 dot col 3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    0\n```\n:::\n:::\n\n\nFinally, not only are the columns orthogonal, but each column vector has length one, making them orthonormal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(sum(O[,1]^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n:::\n\n### Appreciating the Queries\n\nTaking these queries together, we see that symmetric and diagonal matrices are necessarily invertible, diagonalizable and normal.  They are not however orthogonal.  Identity matrices however, have all these properties.  Let's double-check these statements.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_sym <- matrix(\n  c(1, 5, 4, 5, 2, 9, 4, 9, 3),\n  ncol = 3) # symmetric matrix, not diagonal\nA_sym\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    5    4\n[2,]    5    2    9\n[3,]    4    9    3\n```\n:::\n\n```{.r .cell-code}\nnormal_or_orthogonal(A_sym)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThis matrix is normal\n```\n:::\n\n```{.r .cell-code}\nnormal_or_orthogonal(diag(1:3)) # diagonal matrix, symmetric, but not the identity matrix\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThis matrix is normal\n```\n:::\n\n```{.r .cell-code}\nnormal_or_orthogonal(diag(3)) # identity matrix (also symmetric, diagonal)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThis matrix is orthogonal\n```\n:::\n:::\n\n\nSo what's the value of these queries?  As mentioned, they help us understand the relationships between different types of matrices, so they help us learn more deeply.  On a practical computational level they may not have much value, especially when dealing with real-world data sets.  However, there are some other interesting aspects of these queries that deal with decompositions and eigenvalues.  We might cover these in the future.\n\n# Annotated Bibliography\n\nThese are the main sources I relied on for this post.\n\n* *The No Bullshit Guide to Linear Algebra* by [Ivan Savov](https://minireference.com).\n  + Section 6.2 Special types of matrices\n  + Section 6.6 Eigendecomposition\n* *Linear Algebra: step by step* by Kuldeep Singh, Oxford Univerity Press, 2014.\n  + Section 4.4 Orthogonal Matrices\n  + Section 7.3.2 Diagonalization\n  + Section 7.4 Diagonalization of Symmetric Matrices\n* Wikipedia articles on the types of matrices.\n  \n[^1]: I'm using the term taxonomy a little loosely of course, you can call it whatever you want.  The name is not so important really, what is important is the heirarchy of concepts.\n[^2]: Usually in written text a row matrix, sometimes called a row vector, is written as $\\mathbf{x} = \\begin{bmatrix}1 & 2 & 3\\end{bmatrix}$.  In order to save space in documents, rather than writing $\\mathbf{x} = \\begin{bmatrix}1 \\\\ 2 \\\\ 3\\end{bmatrix}$, a column matrix/vector can be kept to a single line by writing it as its transpose: $\\mathbf{x} = \\begin{bmatrix}1 & 2 & 3\\end{bmatrix}^{\\mathsf{T}}$, but this requires a little mental gymnastics to visualize.\n[^3]: This idea of the \"most natural basis\" is most easily visualized in two dimensions. If you have some data plotted on $x$ and $y$ axes, determining the line of best fit is one way of finding the most natural basis for describing the data.  However, more generally and in more dimensions, principal component analysis (PCA) is the most rigorous way of finding this natural basis, and PCA can be calculated with the `eigen()` function.  Lots more information [here](https://bryanhanson.github.io/LearnPCA/).\n[^4]: Upper and lower triangular matrices play a special role in linear algebra.  Because of the presence of many zeros, multiplying them and inverting them is relatively easy, because the zeros cause terms to drop out.\n[^5]: The `drop` argument to subsetting/extracting defaults to `TRUE` which means if subsetting reduces the necessary number of dimensions, the unneeded dimension attributes are dropped.  Under the default, selecting a single column of a matrix leads to a vector, not a one column vector. In this `all.equal()` expression we need both sides to evaluate to a matrix.\n[^6]: I'm only using a portion because the Hiranbe's original contains a bit too much information for someone trying to get their footing in the field.\n[^7]: One might ask why `R` does not provide a user-facing version of such a function.  My guess is that any possible error conditions are handled internally in other functions.  I also think a good argument can be made that the authors of `R` passed down a robust and largely complete set of linear algebra functions, geared toward getting work done.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}