{
  "hash": "8717d774dc81d1f5918639d727d7c4ca",
  "result": {
    "markdown": "---\ntitle: \"Notes on Linear Algebra Part 3\"\ndescription: \"Base R Functions Related to Linear Algebra\"\ndate: \"2022-09-10\"\ncategories: [R, Linear Algebra]\nformat:\n  html\n---\n\n\n\nSeries: [Part 1](https://chemospec.org/posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html) [Part 2](https://chemospec.org/posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html)\n\n*Update 19 September 2022: in \"Use of outer() for Matrix Multiplication\", corrected use of \"cross\" to be \"outer\" and added example in `R`. Also added links to work by Hiranabe.*\n\nThis post is a survey of the linear algebra-related functions from base `R`.  Some of these I've disccused in other posts and some I may discuss in the future, but this post is primarily an inventory: these are the key tools we have available.  \"Notes\" in the table are taken from the help files.\n\nMatrices, including row and column vectors, will be shown in bold e.g. $\\mathbf{A}$ or $\\mathbf{a}$ while scalars and variables will be shown in script, e.g. $n$. `R` code will appear like `x <- y`.\n\nIn the table, $\\mathbf{R}$ or $\\mathbf{U}$ is an upper/right triangular matrix.  $\\mathbf{L}$  is a lower/left triangular matrix (triangular matrices are square). $\\mathbf{A}$ is a generic matrix of dimensions $m \\times n$.  $\\mathbf{M}$ is a square matrix of dimensions $n \\times n$.\n\n| Function | Uses | Notes |\n|:-|:-|:-|:-|\n| **operators** | | |\n| `*` | scalar multiplication | |\n| `%*%` | matrix multiplication | two vectors $\\rightarrow$ the dot product; vector + matrix $\\rightarrow$ cross product (vector will be promoted as needed)[^1] |\n| **basic functions** | | |\n| `t()` | transpose | interchange rows and columns |\n| `crossprod()` | matrix multiplication | faster version of `t(A) %*% A` |\n| `tcrossprod()` | matrix multiplication | faster version of `A %*% t(A)` |\n| `outer()` | outer product & more | see discussion below |\n| `det()`| computes determinant | uses the LU decomposition; determinant is a volume |\n| `isSymmetric()`| name says it all | |\n| `Conj()`| computes complex conjugate | |\n| **decompositions** | | |\n| `backsolve()` | solves $\\mathbf{Rx} = \\mathbf{b}$ |  |\n| `forwardsolve()` | solves $\\mathbf{Lx} = \\mathbf{b}$ |  |\n| `solve()` | solves $\\mathbf{Mx} = \\mathbf{b}$ and $\\mathbf{M}^{-1}$ | e.g. linear systems; if given only one matrix returns the inverse |\n| `qr()` | solves $\\mathbf{A} = \\mathbf{QR}$| $\\mathbf{Q}$ is an orthogonal matrix; can be used to solve $\\mathbf{Ax} = \\mathbf{b}$; see `?qr` for several `qr.*` extractor functions |\n| `chol()` | solves $\\mathbf{M} = \\mathbf{L}\\mathbf{L}^{\\mathsf{T}} = \\mathbf{U}^{\\mathsf{T}}\\mathbf{U}$ | Only applies to positive semi-definite matrices (where $\\lambda \\ge 0$); related to LU decomposition |\n| `chol2inv()` | computes $\\mathbf{M}^{-1}$ from the results of `chol(M)` | |\n| `svd()` | singular value decomposition | input $\\mathbf{A}^{(m \\times n)}$; can compute PCA; [details](https://bryanhanson.github.io/LearnPCA/) |\n| `eigen()` | eigen decomposition | requires $\\mathbf{M}^{(n \\times n)}$; can compute PCA; [details](https://bryanhanson.github.io/LearnPCA/) |\n\n\nOne thing to notice is that there is no LU decomposition in base `R`.  It is apparently used \"under the hood\" in `solve()` and there are versions available in contributed packages.[^3]\n\n\n::: {.callout-tip collapse=\"true\"}\n## What is the use of outer()?\n\nAs seen in [Part 1](https://chemospec.org/posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html) calling `outer()` on two vectors does indeed give the cross product (technically corresponding to `tcrossprod()`).  This works because the defaults carry out multiplication.[^2]  However, looking through the `R` source code for uses of `outer()`, the function should really be thought of in simple terms as creating all possible combinations of the two inputs. In that way it is similar to `expand.grid()`.  Here are two illustrations of the flexibility of `outer()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate a grid of x,y values modified by a function\n# from ?colorRamp\nm <- outer(1:20, 1:20, function(x,y) sin(sqrt(x*y)/3))\nstr(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n num [1:20, 1:20] 0.327 0.454 0.546 0.618 0.678 ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate all combinations of month and year\n# modified from ?outer; any function accepting 2 args can be used\nouter(month.abb, 2000:2002, FUN = paste)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1]       [,2]       [,3]      \n [1,] \"Jan 2000\" \"Jan 2001\" \"Jan 2002\"\n [2,] \"Feb 2000\" \"Feb 2001\" \"Feb 2002\"\n [3,] \"Mar 2000\" \"Mar 2001\" \"Mar 2002\"\n [4,] \"Apr 2000\" \"Apr 2001\" \"Apr 2002\"\n [5,] \"May 2000\" \"May 2001\" \"May 2002\"\n [6,] \"Jun 2000\" \"Jun 2001\" \"Jun 2002\"\n [7,] \"Jul 2000\" \"Jul 2001\" \"Jul 2002\"\n [8,] \"Aug 2000\" \"Aug 2001\" \"Aug 2002\"\n [9,] \"Sep 2000\" \"Sep 2001\" \"Sep 2002\"\n[10,] \"Oct 2000\" \"Oct 2001\" \"Oct 2002\"\n[11,] \"Nov 2000\" \"Nov 2001\" \"Nov 2002\"\n[12,] \"Dec 2000\" \"Dec 2001\" \"Dec 2002\"\n```\n:::\n:::\n\n\nBottom line: `outer()` can be used for linear algebra but its main uses lie elsewhere.  You don't need it for linear algebra!\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n## Using outer() for matrix multiplication\nHere's an interesting connection discussed in this [Wikipedia entry](https://en.wikipedia.org/wiki/Outer_product#Connection_with_the_matrix_product). In Part 1 we demonstrated how the repeated application of the dot product underpins matrix multiplication.  The first row of the first matrix is multiplied element-wise by the first column of the second matrix, shown in red, to give the first element of the answer matrix.  This process is then repeated so that every row (first matrix) has been multiplied by every column (second matrix).\n\n$$\n\\begin{multline}\n\\mathbf{A}\\mathbf{B} = \\mathbf{C} =\n\\begin{bmatrix}\n\\textcolor{red}{a_{11}} & \\textcolor{red}{a_{12}} & \\textcolor{red}{a_{13}} \\\\\na_{21} & a_{22} & a_{23} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\textcolor{red}{b_{11}} & b_{12} & b_{13} \\\\\n\\textcolor{red}{b_{21}} & b_{22} & b_{23} \\\\\n\\textcolor{red}{b_{31}} & b_{32} & b_{33} \\\\\n\\end{bmatrix} = \\\\\n\\begin{bmatrix}\n\\textcolor{red}{a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31}} & a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32} & a_{11}b_{13} + a_{12}b_{23} + a_{13}b_{33}\\\\\na_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31} & a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32} & a_{21}b_{13} + a_{22}b_{23} + a_{23}b_{33}\\\\\n\\end{bmatrix}\n\\end{multline}\n$$ {#eq-1}\n\nIf instead, we treat the first *column* of the first matrix as a column vector and outer multiply it by the first *row* of the second matrix as a row vector, we get the following matrix:\n\n$$\n\\begin{multline}\n\\begin{bmatrix}\n\\textcolor{red}{a_{11}} & a_{12} & a_{13} \\\\\n\\textcolor{red}{a_{21}} & a_{22} & a_{23} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\textcolor{red}{b_{11}} & \\textcolor{red}{b_{12}} & \\textcolor{red}{b_{13}} \\\\\nb_{21} & b_{22} & b_{23} \\\\\nb_{31} & b_{32} & b_{33} \\\\\n\\end{bmatrix} \\Rightarrow\n\\begin{bmatrix}\n\\textcolor{red}{a_{11}b_{11}} & \\textcolor{red}{a_{11}b_{12}} & \\textcolor{red}{a_{11}b_{13}}\\\\\n\\textcolor{red}{a_{21}b_{11}} & \\textcolor{red}{a_{21}b_{12}} & \\textcolor{red}{a_{21}b_{13}}\\\\\n\\end{bmatrix}\n\\end{multline}\n$$ {#eq-2}\n\nNow if you repeat this process for the second column of the first matrix and the second row of the second matrix, you get another matrix.  And if you do it one more time using the third column/third row, you get a third matrix.  If you then add these three matrices together, you get $\\mathbf{C}$ as seen in @eq-1.  Notice how each element in $\\mathbf{C}$ in @eq-1 is a sum of three terms?  Each of those terms comes from one of the three matrices just described.\n\nTo sum up, one can use the dot product on each row (first matrix) by each column (second matrix) to get the answer, or you can use the outer product on the columns sequentially (first matrix) by rows sequentially (second matrix) to get several matrices, which one then sums to get the answer.  It's pretty clear which option is less work and easier to follow, but I think it's an interesting connection between operations.  The first case corresponds to view \"MM1\" in *The Art of Linear Algebra* while the second case is view \"MM4\". See this work by [Kenji Hiranabe](https://github.com/kenjihiranabe/The-Art-of-Linear-Algebra).\n\nHere's a simple proof in `R`.\n\n::: {.cell}\n\n```{.r .cell-code}\nM1 <- matrix(1:6, nrow = 3, byrow = TRUE)\nM1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n```\n:::\n\n```{.r .cell-code}\nM2 <- matrix(7:10, nrow = 2, byrow = TRUE)\nM2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    7    8\n[2,]    9   10\n```\n:::\n\n```{.r .cell-code}\ntst1 <- M1 %*% M2 # uses dot product\n# next line is sum of sequential outer products:\n# 1st col M1 by 1st row M2 + 2nd col M1 by 2nd row M2\ntst2 <- outer(M1[,1], M2[1,]) + outer(M1[,2], M2[2,])\n\nall.equal(tst1, tst2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n:::\n\n[^1]: For details see the discussion in [Part 1](https://chemospec.org/posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html).\n[^2]: In fact, for the default `outer()`, `FUN = \"*\"`, `outer()` actually calls `tcrossprod()`.\n[^3]: Discussed in this [Stackoverflow question](https://stackoverflow.com/q/51687808/633251), which also has an implementation.\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}