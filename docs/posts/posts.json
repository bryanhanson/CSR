[
  {
    "path": "posts/2021-10-13-GSOC-CS-Graphics/",
    "title": "GSOC 2021: New Graphics for ChemoSpec",
    "description": "Major improvements!",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2021-10-13",
    "categories": [
      "R",
      "ChemoSpec",
      "ChemoSpecUtils",
      "ChemoSpec2D",
      "GSOC"
    ],
    "contents": "\n\n\n\nIt’s been quiet around this blog because supervising two students for Google Summer of Code has kept me pretty busy! But we have some news…\n\nThanks to Mr. Tejasvi Gupta and the support of GSOC, ChemoSpec and ChemoSpec2D were extended to produce ggplot2 graphics and plotly graphics! ggplot2 is now the default output, and the ggplot2 object is returned, so if one doesn’t like the choice of theme or any other aspect, one can customize the object to one’s desire. The ggplot2 graphics output are generally similar in layout and spirit to the base graphics output, but significant improvements have been made in labeling data points using the ggrepel package. The original base graphics are still available as well. Much of this work required changes in ChemoSpecUtils which supports the common needs of both packages.\nTejasvi did a really great job with this project, and I think users of these packages will really like the results. We have greatly expanded the pre-release testing of the graphics, and as far as we can see every thing works as intended. Of course, please file an issue if you see any problems or unexpected behavior.\nTo see more about how the new graphics options work, take a look at GraphicsOptions. Here are the functions that were updated:\nplotSpectra\nsurveySpectra\nsurveySpectra2\nreviewAllSpectra (formerly loopThruSpectra)\nplotScree (resides in ChemoSpecUtils)\nplotScores (resides in ChemoSpecUtils)\nplotLoadings (uses patchwork and hence plotly isn’t available)\nplot2Loadings\nsPlotSpectra\npcaDiag\nplotSampleDist\naovPCAscores\naovPCAloadings (uses patchwork and hence plotly isn’t available)\nTejasvi and I are looking forward to your feedback. There are many other smaller changes that we’ll let users discover as they work. And there’s more work to be done, but other projects need attention and I need a little rest!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-23T19:15:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-22-GSOC-hyperSpec-ChemoSpec/",
    "title": "GSOC 2021: hyperSpec and ChemoSpec!",
    "description": "I'm gonna be busy!",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2021-05-22",
    "categories": [
      "R",
      "hyperSpec",
      "ChemoSpec",
      "GSOC"
    ],
    "contents": "\nI’m really happy to announce that this summer I’ll be a co-mentor on two Google Summer of Code spectroscopy projects:\nOnce again, I’ll co-mentor with Claudia and Vilmantas to continue the work Erick started last summer on hyperSpec (see here for Erick’s wrap up blog post at the end of last year). Sang Truong is the very talented student who will be joining us. Sang’s project is described here.\nNew this year: ChemoSpec will be upgraded to use ggplot2 graphics along with interactive graphics for many of the plots that are currently rendered in base graphics. Erick, who was the student working on hyperSpec last summer, will be my co-mentor on this project. We are looking forward to having Tejasvi Gupta as the student on this project.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-23T15:02:14-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-19-Search-GH-Topics/",
    "title": "Automatically Searching Github Repos by Topic",
    "description": "How to find packages of interest",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2021-04-19",
    "categories": [
      "R",
      "Github",
      "FOSS",
      "httr"
    ],
    "contents": "\n\n\n\nOne of the projects I maintain is the FOSS for Spectroscopy web site. The table at that site lists various software for use in spectroscopy. Historically, I have used the Github or Python Package Index search engines to manually search by topic such as “NMR” to find repositories of interest. Recently, I decided to try to automate at least some of this process. In this post I’ll present the code and steps I developed to search Github by topics. Fortunately, I wasn’t starting from scratch, as I had learned some basic web-scraping techniques when I wrote the functions that get the date of the most recent repository update. All the code for this website and project can be viewed here. The steps reported here are current as of the publication of this post, but are subject to change in the future.1\nFirst off, did you know Github allows repository owners to tag their repositories using topical keywords? I didn’t know this for a long time. So add topics to your repositories if you don’t have them already. By the way, the Achilles heel of this project is that good pieces of software may not have any topical tags at all. If you run into this, perhaps you would consider creating an issue to ask the owner to add tags.\nThe Overall Approach\nIf you look at the Utilities directory of the project, you’ll see the scripts and functions that power this search process.\nSearch Repos for Topics Script.R supervises the whole process. It sources:\nsearchRepos.R (a function)\nsearchTopic.R (a function)\nFirst let’s look at the supervising script. First, the necessary preliminaries:\n\n\nlibrary(\"jsonlite\")\nlibrary(\"httr\")\nlibrary(\"stringr\")\nlibrary(\"readxl\")\nlibrary(\"WriteXLS\")\n\nsource(\"Utilities/searchTopic.R\")\nsource(\"Utilities/searchRepos.R\")\n\n\n\nNote that this assumes one has the top level directory, FOSS4Spectroscopy, as the working directory (this is a bit easier than constantly jumping around).\nNext, we pull in the Excel spreadsheet that contains all the basic data about the repositories that we already know about, so we can eventually remove those from the search results.\n\n\nknown <- as.data.frame(read_xlsx(\"FOSS4Spec.xlsx\"))\nknown <- known$name\n\n\n\nNow we define some topics and run the search (more on the search functions in a moment):\n\n\ntopics <- c(\"NMR\", \"EPR\", \"ESR\")\nres <- searchRepos(topics, \"github_token\", known.repos = known)\n\n\n\nWe’ll also talk about that github_token in a moment. With the search results in hand, we have a few steps to make a useful file name and save it in the Searches folder for future use.\n\n\nfile_name <- paste(topics, collapse = \"_\")\nfile_name <- paste(\"Search\", file_name, sep = \"_\")\nfile_name <- paste(file_name, \"xlsx\", sep = \".\")\nfile_name <- paste(\"Searches\", file_name, sep = \"/\")\nWriteXLS(res, file_name,\n      row.names = FALSE, col.names = TRUE, na = \"NA\")\n\n\n\nAt this point, one can open the spreadsheet in Excel and check each URL (the links are live in the spreadsheet). After vetting each site,2 one can append the new results to the existing FOSS4Spec.xlsx data base and refresh the entire site so the table is updated.\nTo make this job easier, I like to have the search results spreadsheet open and then open all the URLs using the as follows. Then I can quickly clean up the spreadsheet (it helps to have two monitors for this process).\n\n\nfound <- as.data.frame(read_xlsx(file_name))\nfor (i in 1:nrow(found)) {\n  if (grepl(\"^https?://\", found$url[i], ignore.case = TRUE)) BROWSE(found$url[i])\n}\n\n\n\nAuthentificating\nIn order to use the Github API, you have to authenticate. Otherwise you will be severely rate-limited. If you are authenticated, you can make up to 5,000 API queries per hour.\nTo authenticate, you need to first establish some credentials with Github, by setting up a “key” and a “secret”. You can set these up here by choosing the “Oauth Apps” tab. Record these items in a secure way, and be certain you don’t actually publish them by pushing.\nNow you are ready to authenticate your R instance using “Web Application Flow”.3\n\n\nmyapp <- oauth_app(\"FOSS\", key = \"put_your_key_here\", secret = \"put_your_secret_here\")\ngithub_token <- oauth2.0_token(oauth_endpoints(\"github\"), myapp)\n\n\n\nIf successful, this will open a web page which you can immediately close. In the R console, you’ll need to choose whether to do a one-time authentification, or leave a hidden file behind with authentification details. I use the one-time option, as I don’t want to accidently publish the secrets in the hidden file (since they are easy to overlook, being hidden and all).\nsearchTopic\nsearchTopic is a function that accesses the Github API to search for a single topic.4 This function is “pretty simple” in that it is short, but there are six helper functions defined in the same file. So, “short not short”. This function does all the heavy lifting; the major steps are:\nCarry out an authenticated query of the topics associated with all Github repositories. This first “hit” returns up to 30 results, and also a header than tells how many more pages of results are out there.\nProcess that first set of results by converting the response to a JSON structure, because nice people have already built functions to handle such things (I’m looking at you httr).\nCheck that structure for a message that will tell us if we got stopped by Github access issues (and if so, report access stats).\nExtract only the name, description and repository URL from the huge volume of information captured.\n\nInspect the first response to see how many more pages there are, then loop over page two (we already have page 1) to the number of pages, basically repeating step 2.\nAlong the way, all the results are stored in a data.frame.\nsearchRepos\nsearchRepos does two simple things:\nLoops over all topics, since searchTopic only handles one topic at a time.\nOptionally, dereplicates the results by excluding any repositories that we already know about.\nOther Stuff to Make Life Easier\nThere are two other scripts in the Utilities folder that streamline maintenance of the project.\nmergeSearches.R which will merge several search results into one, removing duplicates along the way.\nmergeMaintainers.R which will query CRAN for the maintainers of all packages in FOSS4Spec.xlsx, and add this info to the file.5 Maintainers are not currently displayed on the main website. However, I hope to eventually e-mail all maintainers so they can fine-tune the information about their entries.\nFuture Work / Contributing\nClearly it would be good for someone who knows Python to step in and write the analogous search code for PyPi.org. Depending upon time contraints, I may use this as an opportunity to learn more Python, but really, if you want to help that would be quicker!\nAnd that folks, is how the sausage is made.\nThis code has been tested on a number of searches and I’ve captured every exception I’ve encountered. If you have problems using this code, please file an issue. It’s nearly impossible that it is perfect at this point!↩︎\nSome search terms produce quite a few false positives. I also review each repository to make sure the project is actually FOSS, is not a student project etc (more details on the main web site).↩︎\nWhile I link to the documentation for completeness, the steps described next do all the work.\n\n↩︎\nSee notes in the file: I have not been able to get the Github API to work with multiple terms, so we search each one individually.↩︎\nWant to contribute? If you know the workings of the PyPi.org API it would be nice to automatically pull the maintainer’s contact info.↩︎\n",
    "preview": {},
    "last_modified": "2021-11-22T19:58:47-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-11-GHA-drat/",
    "title": "Using Github Actions & drat to Deploy R Packages",
    "description": "Automating a tedious task",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "R",
      "Github Actions",
      "drat",
      "GSOC",
      "hyperSpec"
    ],
    "contents": "\nLast summer, a GSOC project was approved for work on the hyperSpec package which had grown quite large and hard to maintain.1 The essence of the project was to break the original hyperSpec package into smaller packages.2 As part of that project, we needed to be able to:\nProvide development versions of packages\nProvide large data-only packages (potentially too large to be hosted on CRAN).\nIn this post I’ll describe how we used Dirk Eddelbuettel’s drat package and Github Actions to automate the deployment of packages between repositories.\nWhat is drat?\ndrat is a package that simplifies the creation and modification of CRAN-like repositories. The structure of a CRAN-like repository is officially described briefly here.3 Basically, there is required set of subdirectories, required files containing package metadata, and source packages that are the result of the usual build and check process. One can also have platform-specific binary packages. drat will create the directories and metadata for you, and provides utilities that will move packages to the correct location and update the corresponding metadata.4 The link above provides access to all sorts of documentation. My advice is to not overthink the concept. A repository is simply a directory structure and a couple of required metadata files, which must be kept in sync with the packages present. drat does the heavy-lifting for you.\nWhat are Github Actions?\nGithub Actions are basically a series of tasks that one can have Github run when there is an “event” on a repo, like a push or pull. Github Actions are used extensively for continuous integrations tasks, but they are not limited to such use. Github Actions are written in a simply yaml-like script that is rather easy to follow even if the details are not familiar. Github Actions uses shell commands, but much of the time the shell simply calls Rscript to run native R functions. One can run tasks on various hardware and OS versions.\nThe Package Repo\nThe deployed packages reside on the gh-pages branch of r-hyperspec/pkg-repo in the form of the usual .tar.gz source archives, ready for users to install. One of the important features of this repo is the table of hosted packages displayed in the README. The table portion of README.md file is generated automatically whenever someone, or something, pushes to this repo. I include the notion that something might push because as you will see next, the deploy process will automatically push archives to this repo from the repo where they are created. The details of how this README.md is generated are in drat--update-readme.yaml. If you take a look, you’ll see that we use some shell-scripting to find any .tar.gz archives and create a markdown-ready table structure, which Github then automatically displays (as it does with all README.md files at the top level of a repo). The yaml file also contains a little drat action that will refresh the repo in case that someone manually removes an archive file by git operations. Currently we do not host binary packages at this repo, but that is certainly possible by extension of the methods used for the source packages.\nThe Automatic Deploy Process\nThe automatic deploy process is used in several r-hyperSpec repos. I’ll use the chondro repo to illustrate the process. chondro is a simple package containing a > 2 Mb data set. If the package is updated, the package is built and checked and then deployed automatically to r-hyperSpec/pkg-repo (described above). The magic is in drat--insert-package.yaml. The first part of this file does the standard build and check process.5 The second part takes care of deploying to r-hyperspec/pkg-repo. The basic steps are given next (study the file for the details). It is essential to keep in mind that each task in Github Actions starts from the same top level directory.6 Tasks are set off by the syntax - name: task description.\nConfigure access to Github. Note that we employ a Github user name and e-mail that will uniquely identify the repo that is pushing to r-hyperSpec/pkg-repo. This is helpful for troubleshooting.\nClone r-hyperSpec/pkg-repo into a temporary directory and checkout the gh-pages branch.\nSearch for any .tar.gz files in the check folder, which is where we directed Github Actions to carry out the build and check process (the first half of this workflow).7 Note that the argument full.names = TRUE is essential to getting the correct path. Use drat to insert the .tar.gz files into the cloned r-hyperSpec/pkg-repo temporary directory.\nMove to the temporary directory, then use git commands to send the updated r-hyperspec/pkg-repo branch back to its home, now with the new .tar.gz files included. Use a git commit message that will show where the new tar ball came from.\nThanks for reading. Let me know if you have any questions, via the comments, e-mail, etc.\nAcknowledgements\nThis portion of the hyperSpec GSOC 2020 project was primarily the work of hyperSpec team members Erick Oduniyi, Bryan Hanson and Vilmantas Gegzna. Erick was supported by GSOC in summer 2020.\nThe work continues this summer, hopefully again with the support of GSOC.↩︎\nProject background and results.↩︎\nA more loquacious description that may be slightly dated is here.↩︎\ndrat is using existing R functions, mainly from the tools package. They are just organized and presented from the perspective of a user who wants to create a repo.\n\n↩︎\nModified from the recipes here.↩︎\nThe toughest part of writing this workflow was knowing where one was in the directory tree of the Github Actions workspace. We made liberal use of getwd(), list.files() and related functions during troubleshooting. All of these “helps” have been removed from the mature version of the workflow. As noted in the workflow, the top directory is /home/runner/work/${{ REPOSITORY_NAME }}/${{ REPOSITORY_NAME }}.↩︎\nIt’s helpful to understand in a general way what happens during the build and check process (e.g. the directories and files created).↩︎\n",
    "preview": {},
    "last_modified": "2021-11-23T15:06:27-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-27-Spec-Suite-update/",
    "title": "Spectroscopy Suite Update",
    "description": "Updates for the new version of R",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2021-03-27",
    "categories": [
      "R",
      "ChemoSpec",
      "ChemoSpec2D",
      "ChemoSpecUtils",
      "readJDX"
    ],
    "contents": "\n\n\n\nMy suite of spectroscopy R packages has been updated on CRAN. There are only a few small changes, but they will be important to some of you:\nChemoSpecUtils now provides a set of colorblind-friendly colors, see ?colorSymbol. These are available for use in ChemoSpec and ChemoSpec2D.\nAt the request of several folks, readJDX now includes a function, splitMultiblockDX, that will split a multiblock JCAMP-DX file into separate files, which can then be imported via the usual functions in the package.\nAll packages are built against the upcoming R 4.1 release (due in April).\nHere are the links to the documentation:\nChemoSpec\nChemSpec2D\nChemoSpecUtils\nreadJDX\nAs always, let me know if you discover trouble or have questions.\n\n\n\n",
    "preview": "posts/2021-03-27-Spec-Suite-update/emAnnotated.png",
    "last_modified": "2021-11-23T16:58:42-06:00",
    "input_file": {},
    "preview_width": 1512,
    "preview_height": 432
  },
  {
    "path": "posts/2021-02-08-PLS/",
    "title": "Interfacing ChemoSpec to PLS",
    "description": "It's easy to connect the two packages",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2021-02-08",
    "categories": [
      "R",
      "PLS",
      "ChemoSpec"
    ],
    "contents": "\n\n\n\nThe ChemoSpec package carries out exploratory data analysis (EDA) on spectroscopic data. EDA is often described as “letting that data speak”, meaning that one studies various descriptive plots, carries out clustering (HCA) as well as dimension reduction (e.g. PCA), with the ultimate goal of finding any natural structure in the data.\nAs such, ChemoSpec does not feature any predictive modeling functions because other packages provide the necessary tools. I do however hear from users several times a year about how to interface a ChemoSpec object with these other packages, and it seems like a post about how to do this is overdue. I’ll illustrate how to carry out partial least squares (PLS) using data stored in a ChemoSpec object and the package chemometrics by Peter Filzmoser and Kurt Varmuza (Filzmoser and Varmuza 2017). One can also use the pls package (Mevik, Wehrens, and Liland 2020).\nPLS is a technique related to regression and PCA that tries to develop a mathematical model between a matrix of sample vectors, in our case, spectra, and one or more separately measured dependent variables that describe the same samples (typically, chemical analyses). If one can develop a reliable model, then going forward one can measure the spectrum of a new sample and use the model to predict the value of the dependent variables, presumably saving time and money. This post will focus on interfacing ChemoSpec objects with the needed functions in chemometrics. I won’t cover how to evaluate and refine your model, but you can find plenty on this in Varmuza and Filzmoser (2009) chapter 4, along with further background (there’s a lot of math in there, but if you aren’t too keen on the math, gloss over it to get the other nuggets). Alternatively, take a look at the vignette that ships with chemometrics via browseVignettes(\"chemometrics\").\nAs our example we’ll use the marzipan NIR data set that one can download in Matlab format from here.1 The corresponding publication is (Christensen et al. 2004). This data set contains NIR spectra of marzipan candies made with different recipes and recorded using several different instruments, along with data about moisture and sugar content. We’ll use the data recorded on the NIRSystems 6500 instrument, covering the 400-2500 nm range. The following code chunk gives a summary of the data set and shows a plot of the data. Because we are focused on how to carry out PLS, we won’t worry about whether this data needs to be normalized or otherwise pre-processed (see the Christensen paper for lots of details).\n\n\nlibrary(\"ChemoSpec\")\nload(\"Marzipan.RData\")\nsumSpectra(Marzipan)\n\n\n\n Marzipan NIR data set from www.models.life.ku.dk/Marzipan \n\n    There are 32 spectra in this set.\n    The y-axis unit is absorbance.\n\n    The frequency scale runs from\n    450 to 2448 wavelength (nm)\n    There are 1000 frequency values.\n    The frequency resolution is\n    2 wavelength (nm)/point.\n\n\n    The spectra are divided into 9 groups: \n\n  group no.     color symbol alt.sym\n1     a   5 #FB0D16FF      1       a\n2     b   4 #FFC0CBFF     16       b\n3     c   4 #2AA30DFF      2       c\n4     d   4 #9BCD9BFF     17       d\n5     e   3 #700D87FF      3       e\n6     f   3 #A777F2FF      8       f\n7     g   2 #FD16D4FF      4       g\n8     h   3 #B9820DFF      5       h\n9     i   4 #B9820DFF      5       i\n\n\n*** Note: this is an S3 object\nof class 'Spectra'\n\nplotSpectra(Marzipan, which = 1:32, lab.pos = 3000)\n\n\n\n\nIn order to carry out PLS, one needs to provide a matrix of spectroscopic data, with samples in rows (let’s call it \\(X\\), you’ll see why in a moment). Fortunately this data is available directly from the ChemoSpec object as Marzipan$data.2 One also needs to provide a matrix of the additional dependent data (let’s call it \\(Y\\)). It is critical that the order of rows in \\(Y\\) correspond to the order of rows in the matrix of spectroscopic data, \\(X\\).\nSince we are working in R we know there are a lot of ways to do most tasks. Likely you will have the additional data in a spreadsheet, so let’s see how to bring that into the workspace. You’ll need samples in rows, and variables in columns. For your sanity and error-avoidance, you should include a header of variable names and the names of the samples in the first column. Save the spreadsheet as a csv file. I did these steps using the sugar and moisture data from the original paper. Read the file in as follows.\n\n\nY <- read.csv(\"Marzipan.csv\", header = TRUE)\nstr(Y)\n\n\n'data.frame':   32 obs. of  3 variables:\n $ sample  : chr  \"a1\" \"a2\" \"a3\" \"a4\" ...\n $ sugar   : num  32.7 34.9 33.9 33.2 33.2 ...\n $ moisture: num  15 14.9 14.7 14.9 14.9 ...\n\nThe function we’ll be using wants a matrix as input, so convert the data frame that read.csv generates to a matrix. Note that we’ll select only the numeric variables on the fly, as unlike a data frame, a matrix can only be composed of one data type.\n\n\nY <- as.matrix(Y[, c(\"sugar\", \"moisture\")])\nstr(Y)\n\n\n num [1:32, 1:2] 32.7 34.9 33.9 33.2 33.2 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:2] \"sugar\" \"moisture\"\n\nNow we are ready to carry out PLS. Since we have a multivariate \\(Y\\), we need to use the appropriate function (use pls1_nipals if your \\(Y\\) matrix is univariate).\n\n\nlibrary(\"chemometrics\")\npls_out <- pls2_nipals(X = Marzipan$data, Y, a = 5)\n\n\n\nAnd we’re done! Be sure to take a look at str(pls_out) to see what you got back from the calculation. For the next steps in evaluating your model, see section 3.3 in the chemometrics vignette.\n\n\nChristensen, Jakob, Lars Nørgaard, Hanne Heimdal, Joan Grønkjær Pedersen, and Søren Balling Engelsen. 2004. “Rapid Spectroscopic Analysis of Marzipan—Comparative Instrumentation.” Journal of Near Infrared Spectroscopy 12 (1): 63–75. https://doi.org/10.1255/jnirs.408.\n\n\nFilzmoser, Peter, and Kurt Varmuza. 2017. Chemometrics: Multivariate Statistical Analysis in Chemometrics. https://CRAN.R-project.org/package=chemometrics.\n\n\nMevik, Bjørn-Helge, Ron Wehrens, and Kristian Hovde Liland. 2020. Pls: Partial Least Squares and Principal Component Regression. https://CRAN.R-project.org/package=pls.\n\n\nVarmuza, K., and P. Filzmoser. 2009. Introduction to Multivariate Statistical Analysis in Chemometrics. CRC Press.\n\n\nI have converted the data from Matlab to a ChemoSpec object; if anyone wants to know how to do this let me know and I’ll put up a post on that process.↩︎\nstr(Marzipan) will show you the structure of the ChemoSpec object (or in general, any R object). The official definition of a ChemoSpec object can be seen via ?Spectra.\n\n↩︎\n",
    "preview": "posts/2021-02-08-PLS/2021-02-08-PLS_files/figure-html5/showSpectra-1.png",
    "last_modified": "2021-11-22T19:55:05-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-09-08-GSOC-hyperSpec/",
    "title": "GSOC Wrap Up",
    "description": "Guest post by Erick Oduniyi",
    "author": [
      {
        "name": "Erick Oduniyi",
        "url": {}
      }
    ],
    "date": "2020-09-08",
    "categories": [
      "R",
      "hyperSpec",
      "GSOC",
      "Guest Post"
    ],
    "contents": "\n\n\n\nWell, things have been busy lately! As reported back in May, I’ve been participating in Google Summer of Code which has now wrapped up. This was very rewarding for me, but today I want to share a guest post by Erick Oduniyi, the very talented student on the project. Bryan\n\nChecking in from Kansas!\nThis past summer (2020) I had the amazing opportunity to participate in the Google Summer of Code (GSoC or GSOC). As stated on the the GSOC website, GSOC is a “global program focused on bringing more student developers into open source software development. Students work with an open-source organization on a 3-month programming project during their break from school.”\nThis was a particularly meaningful experience as it was my last undergraduate summer internship. I’m a senior studying computer engineering at the University of Kansas, and at the beginning of the summer I still didn’t feel super comfortable working on public (open-source) projects. So, I thought this program would help build my confidence as a computer and software engineer. Moreover:\nI wanted to work with the R organization because that is my favorite programming language.\nI wanted to work with r-hyperspec because I thought that would be the most impactful in terms of practicing project management and software ecosystem development.\nIn the process I hoped to:\nBecome proficient using Git/Github, including continuous integration\nBecome proficient in using Trello\nBecome proficient in using R\nBecome familiar with the spectroscopy community\nBecome inspired to code more\nBecome inspired to document and write more open source projects.\nBecome excited to collaborate more across various industrial, academic, and community domains.\nAnd through a lot of hard work all of those things came to be! Truthfully, even though the summer project was successful there is still a lot of work to do:\nFortify hyperSpec for baseline with bridge packages\nFortify hyperSpec for EMSC with bridge packages\nFortify hyperSpec for matrixStats with bridge packages.\nSo, I’m excited to continue to work with the team! I think there are a ton of ideas I and the team have and hopefully we will get to explore them in deeper context. Speaking of the team, I have them to thank for an awesome GSOC 2020 experience. If you are interested in the journey that was the GSoC 2020 experience (perhaps you might be interested in trying the program next year), then please feel free to jump around here to get a feel for the things that I learned and how I worked with the r-hyperspec team this summer.\nBest, E. Oduniyi\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-23T19:18:51-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-06-28-Sim-Spec-Data-Pt1/",
    "title": "Simulating Spectroscopic Data Part 1",
    "description": "Faking it...",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-06-28",
    "categories": [
      "R",
      "Simulated Data",
      "SpecHelpers",
      "Baseline"
    ],
    "contents": "\n\n\n\nIt is well-recognized that one of the virtues of the R language is the extensive tools it provides for working with distributions. Functions exist to generate random number draws, determine quantiles, and examine the probability density and cumulative distribution curves that describe each distribution.\nThis toolbox gives one the ability to create simulated data sets for testing very easily. If you need a few random numbers from a Gaussian distribution then rnorm is your friend:\n\n\nrnorm(3)\n\n\n[1]  0.1440807 -1.6972968 -1.4168098\n\nImagine you were developing a new technique to determine if two methods of manufacturing widgets produced widgets of the same mass.1 Even before the widgets were manufactured, you could test your code by simulating widget masses using rnorm:\n\n\nwidget_1_masses <- rnorm(100, 5.0, 0.5) # mean mass 5.0\nwidget_2_masses <- rnorm(100, 4.5, 0.5) # mean mass 4.5\n\n\n\n\n\n\nVariations on this approach can be used to simulate spectral data sets.2 The information I will share here is accumulated knowledge. I have no formal training in the theory behind the issues discussed, just skills I have picked up in various places and by experimenting. If you see something that is wrong or needs clarification or elaboration, please use the comments to set me straight!\nPeak Shapes\nWhat peak shape is expected for a given type of spectroscopy? In principle this is based on the theory behind the method, either some quantum mechanical model or an approximation of it. For some methods, like NMR, this might be fairly straightforward, at least in simple systems. But the frequencies involved in some spectroscopies not too different from others, and coupling is observed. Two examples which “interfere” with each other are:\nElectronic transitions in UV spectra which are broadened by interactions with vibrational states.\nVibrational transitions in IR spectroscopy (bonds stretching and bond angles bending in various ways) are coupled to electronic transitions.\nAfter theoretical considerations, we should keep in mind that all spectroscopies have some sort of detector, electronic components and basic data processing that can affect peak shape. A CCD on a UV detector is one of the simpler situations. FT-IR has a mechanical interferometer, and the raw signal from both IR and NMR is Fourier-transformed prior to use. So there are not only theoretical issues to think about, but also engineering, instrument tuning, electrical engineering and mathematical issues to consider.\nEven with myriad theoretical and practical considerations, a Gaussian curve is a good approximation to a simple peak, and more complex peaks can be built by summing Gaussian curves. If we want to simulate a simple peak with a Gaussian shape, we can use the dnorm function, which gives us the “density” of the distribution:\n\n\nstd_deviations <- seq(-5, 5, length.out = 100)\nGaussian_1 <- dnorm(std_deviations)\nplot(std_deviations, Gaussian_1, type = \"l\",\n  xlab = \"standard deviations\", ylab = \"Gaussian Density\")\n\n\n\n\nIf we want this to look more like a “real” peak, we can increase the x range and use x values with realistic frequency values. And if we want our spectrum to be more complex, we can add several of these curves together. Keep in mind that the area under the density curve is 1.0, and the peak width is determined by the value of argument sd (the standard deviation). For example if you want to simulate the UV spectrum of vanillin, which has maxima at about 230, 280 and 315 nm, one can do something along these lines:\n\n\nwavelengths <- seq(220, 350, by = 1.0)\nPeak1 <- dnorm(wavelengths, 230, 22)\nPeak2 <- dnorm(wavelengths, 280, 17)\nPeak3 <- dnorm(wavelengths, 315, 17)\nPeaks123 <- colSums(rbind(1.6 * Peak1, Peak2, Peak3))\nplot(wavelengths, Peaks123, type = \"l\",\n  xlab = \"wavelengths (nm)\", ylab = \"arbitrary intensity\")\n\n\n\n\nThe coefficient on Peak1 is needed to increase the contribution of that peak in order to better resemble the linked spectrum (note that the linked spectrum y-axis is \\(log \\epsilon\\); we’re just going for a rough visual approximation).\nIt’s a simple, if tedious, task to add Gaussian curves in this manner to simulate a single spectrum. One can also create several different spectra, and then combine them in various ratios to create a data set representing samples composed of mixtures of compounds. UV spectra are tougher due to the vibrational coupling; NMR spectra are quite straightforward since we know the area of each magnetic environment in the structure (but we also have to deal with doublets etc.). If you plan to do a lot of this, take a look at the SpecHelpers package, which is designed to streamline these tasks.\nA relatively minor exception to the typical Gaussian peak shape is NMR. Peaks in NMR are typically described as “Lorentzian”, which corresponds to the Cauchy distribution (Goldenberg 2016). This quick comparison shows that NMR peaks are expected to be less sharp and have fatter tails:\n\n\nGaussian_1 <- dnorm(std_deviations)\nCauchy_1 <- dcauchy(std_deviations)\nplot(std_deviations, Gaussian_1, type = \"l\",\n  xlab = \"standard deviations\", ylab = \"density\")\nlines(std_deviations, Cauchy_1, col = \"red\")\n\n\n\n\nBaselines\nFor many types of spectroscopies there is a need to correct the baseline when processing the data. But if you are simulating spectroscopic (or chromatographic) data, how can you introduce baseline anomalies? Such anomalies can take many forms, for instance a linear dependence on wavelength (i.e. a steadily rising baseline without curvature). But more often one sees complex rolling baseline issues.\nLet’s play with introducing different types of baseline abberations. First, let’s create a set of three simple spectra. We’ll use a simple function to scale the set of spectra so the range is on the interval [0…1] for ease of further manipulations.\n\n\nwavelengths <- 200:800\nSpec1 <- dnorm(wavelengths, 425, 30)\nSpec2 <- dnorm(wavelengths, 550, 20) * 3 # boost the area\nSpec3 <- dnorm(wavelengths, 615, 15)\nSpec123 <- rbind(Spec1, Spec2, Spec3)\ndim(Spec123) # matrix with samples in rows\n\n\n[1]   3 601\n\n\n\nscale01 <- function(M) {\n  # scales the range of the matrix to [0...1]\n  mn <- min(M)\n  M <- M - mn\n  mx <- max(M)\n  M <- M/mx\n}\n\n\n\nHere are the results; the dotted line is the sum of the three spectra, offset vertically for ease of comparison.\n\n\nSpec123 <- scale01(Spec123)\nplot(wavelengths, Spec123[1,], col = \"black\", type = \"l\",\n  xlab = \"wavelength (nm)\", ylab = \"intensity\",\n  ylim = c(0, 1.3))\nlines(wavelengths, Spec123[2,], col = \"red\")\nlines(wavelengths, Spec123[3,], col = \"blue\")\nlines(wavelengths, colSums(Spec123) + 0.2, lty = 2)\n\n\n\n\nOne clever way to introduce baseline anomalies is to use a Vandermonde matrix. This is a trick I learned while working with the team on the hyperSpec overhaul funded by GSOC.3 It’s easiest to explain by an example:\n\n\nvander <- function(x, order) outer(x, 0:order, `^`)\nvdm <- vander(wavelengths, 2)\ndim(vdm)\n\n\n[1] 601   3\n\nvdm[1:5, 1:3]\n\n\n     [,1] [,2]  [,3]\n[1,]    1  200 40000\n[2,]    1  201 40401\n[3,]    1  202 40804\n[4,]    1  203 41209\n[5,]    1  204 41616\n\nvdm <- scale(vdm, center = FALSE, scale = c(1, 50, 2000))\n\n\n\nLooking at the first few rows of vdm, you can see that the first column is a simple multiplier, in this case an identity vector. This can be viewed as an offset term.4 The second column contains the original wavelength values, in effect a linear term. The third column contains the square of the original wavelength values. If more terms had been requested, they would be the cubed values etc. In the code above we also scaled the columns of the matrix so that the influence of the linear and especially the squared terms don’t dominate the absolute values of the final result. Scaling does not affect the shape of the curves.\nTo use this Vandermonde matrix, we need another matrix which will function as a set of coefficients.\n\n\ncoefs <- matrix(runif(nrow(Spec123) * 3), ncol = 3)\ncoefs\n\n\n          [,1]      [,2]      [,3]\n[1,] 0.1409040 0.3487193 0.3402532\n[2,] 0.4740782 0.8426986 0.8967488\n[3,] 0.9564882 0.4438944 0.6544713\n\nIf we multiply the coefficients by the tranposed Vandermonde matrix, we get back a set of offsets which are the rows of the Vandermonde matrix modified by the coefficients. We’ll scale things so that Spec123 and offsets are on the same overall scale and then further scale so that the spectra are not overwhelmed by the offsets in the next step.\n\n\noffsets <- coefs %*% t(vdm)\ndim(offsets) # same dimensions as Spec123 above\n\n\n[1]   3 601\n\noffsets <- scale01(offsets) * 0.1\n\n\n\nThese offsets can then be added to the original spectrum to obtain our spectra with a distorted baseline. Here we have summed the individual spectra. We have added a line based on extrapolating the first 20 points of the distorted data, which clearly shows the influence of the squared term.\n\n\nFinalSpec1 <- offsets + Spec123\nplot(wavelengths, colSums(FinalSpec1), type = \"l\", col = \"red\",\n  xlab = \"wavelength (nm)\", ylab = \"intensity\")\nlines(wavelengths, colSums(Spec123))\nfit <- lm(colSums(FinalSpec1)[1:20] ~ wavelengths[1:20])\nlines(wavelengths, fit$coef[2]*wavelengths + fit$coef[1],\n  col = \"red\", lty = 2) # good ol' y = mx + b\n\n\n\n\nThe Vandermonde matrix approach works by creating offsets that are added to the original spectrum. However, it is limited to creating baseline distortions that generally increase at higher values. To create other types of distortions, you can use your imagination. For instance, you could reverse the order of the rows of offsets and/or use higher terms, scale a row, etc. One could also play with various polynomial functions to create the desired effect over the wavelength range of interest. For instance, the following code adds a piece of an inverted parabola to the original spectrum to simulate a baseline hump.\n\n\nhump <- -1*(15*(wavelengths - 450))^2 # piece of a parabola\nhump <- scale01(hump)\nFinalSpec2 <- hump * 0.1 + colSums(Spec123)\nplot(wavelengths, FinalSpec2, type = \"l\",\n  xlab = \"wavelengths (nm)\", ylab = \"intensity\")\nlines(wavelengths, hump * 0.1, lty = 2) # trace the hump\n\n\n\n\nIn the plot, the dotted line traces out the value of hump * 0.1, the offset.\nIn the next post we’ll look at ways to introduce noise into simulated spectra.\n\n\nGoldenberg, David P. 2016. Principles of NMR Spectroscopy: An Illustrated Guide. University Science Books.\n\n\nOf course, this is simply the t-test.↩︎\nFor that matter, you can also simulate chromatograms using the methods we are about to show. It’s even possible to introduce tailing of a peak. For a function to do this, see the SpecHelpers package.↩︎\nThe work I’m showing here is based on original code in package hyperSpec by Claudia Belietes.↩︎\nAs a vector of 1’s it will have no effect on the calculations to come. However, you could multiply this column by a value to add an offset to your simulated spectra. This would be a means of simulating a steady electronic bias in an instrument’s raw data.\n\n↩︎\n",
    "preview": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1_files/figure-html5/widgetPlot-1.png",
    "last_modified": "2021-11-23T15:15:26-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-05-07-GSOC-hyperSpec/",
    "title": "Fortifying hyperSpec: Getting Ready for GSOC",
    "description": "Re-factoring hyperSpec",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-05-07",
    "categories": [
      "R",
      "hyperSpec",
      "GSOC"
    ],
    "contents": "\nhyperSpec is an R package for working with hyperspectral data sets. Hyperspectral data can take many forms, but a common application is a series of spectra collected over an x, y grid, for instance Raman imaging of medical specimens. hyperSpec was originally written by Claudia Beleites and she currently guides a core group of contributors.1\nClaudia, regular hyperSpec contributor Roman Kiselev and myself have joined forces this summer in a Google Summer of Code project to fortify hyperSpec. We are pleased to report that the project was accepted by R-GSOC administrators, and, as of a few days ago, the excellent proposal written by Erick Oduniyi was approved by Google. Erick is a senior computer engineering major at Wichita State University in Kansas. Erick gravitates toward interdisciplinary projects. This, and his experience with R, Python and related skills gives him an excellent background for this project.\nThe focus of this project is to fortify the infrastructure of hyperSpec. Over the years, keeping hyperSpec up-to-date has grown a bit unwieldy. While to-do lists always evolve, the current interrelated goals include:\nDistill2 hyperSpec: Prune hyperSpec back to it’s core functionality to keep it lightweight. Relocate portions, such as importing data, into their own dedicated packages.\nShield hyperSpec: Analyze the ecosystem of hyperSpec with an eye to reducing dependencies as much as possible and ensuring that necessary dependencies are the best choices. Avoid “re-inventing the wheel”, as long as the available “wheels” are computationally efficient and stable (code base and API).\nBridge hyperSpec: Having decided on how to reorganize hyperSpec and which dependencies are necessary and optimal, ensure that hyperSpec, the constellation of new sub-packages, and all dependencies are integrated efficiently. There are a number of data pre-processing and plotting functions that need to be streamlined and interfaced to external packages more consistently. Some portions may need substantial refactoring.\nAddressing each of these goals will make hyperSpec much easier to maintain, less fragile, and easier for others to contribute. Every step will bring enhanced documentation and vignettes, along with new unit tests. Work will begin in earnest on June 1st, and we are looking forward to a very productive summer.\nFinally, on behalf of all participants, let me just say how grateful we are to Google for establishing the GSOC program and for supporting Erick’s work this summer!\nA little history for the curious: the hyperSpec and ChemoSpec packages were written around the same time, independent of each other (~2009). Eventually, Claudia and I became aware of each other’s work, and we have collaborated in ways large and small ever since (I like working with Claudia because I always learn from her!). We have jointly mentored GSOC students twice before. One side project is hyperChemoBridge, a small package that converts hyperSpec objects into Spectra objects (the native ChemoSpec format) and vice-versa.↩︎\nThe descriptors here are Erick’s clever choice of words.\n\n↩︎\n",
    "preview": {},
    "last_modified": "2021-11-22T19:47:27-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-27-CSU-update/",
    "title": "ChemoSpecUtils Update",
    "description": "Improvments to ChemoSpecUtils",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-04-27",
    "categories": [
      "R",
      "ChemoSpecUtils"
    ],
    "contents": "\nChemoSpecUtils, a package that supports the common needs of ChemoSpec and ChemoSpec2D, has been updated to fix an unfortunate distance calculation error in version 0.4.38, released in January of this year. From the NEWS file for version 0.4.51:\nFunction rowDist, which supports a number of functions, was overhauled to address confusion in the documentation, and in my head, about distances vs. similarities. Also, different definitions found in the literature were documented more clearly. The Minkowski distance option was removed (ask if you want it back), code was cleaned up, documentation greatly improved, an example was added and unit tests were added. Plot scales were also corrected as necessary. Depending upon which distance option is chosen, this change affects hcaSpectra, plotSpectraDist, sampleDist and hcaScores in package ChemoSpec as well as hats_alignSpectra2D and hcaScores in package ChemoSpec2D.\nThis brings to mind a Karl Broman quote I think about frequently:\n\n“Open source means everyone can see my stupid mistakes. Version control means everyone can see every stupid mistake I’ve ever made.”\n– Karl Broman\n\nKarl Broman quote source\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-22T19:46:32-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-25-Heatmaps/",
    "title": "Spectral Heatmaps",
    "description": "Insights into how samples and frequencies affect clustering",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-04-25",
    "categories": [
      "R",
      "heatmap",
      "seriation",
      "ChemoSpec"
    ],
    "contents": "\n\n\n\nMost everyone is familiar with heatmaps in a general way. It’s hard not to run into them. Let’s consider some variations:\nA heatmap is a 2D array of rectangular cells colored by value. Generally, the rows and columns are ordered in some purposeful manner. These are very commonly encountered in microarrays for example.\nAn image is a type of heatmap in which the ordering of the rows and columns is defined spatially – it would not make sense to reorder them. This kind of data arises from the physical dimensions of a sensor, for instance the sensor on a digital camera or a raman microscope. An image might also arise by a decision to subset and present data in a “square” format. An example would be the topographic maps provided by the US government which cover a rectangular latitude/longitude range. This type of data can also be presented as a contour plot. See the examples in ?image for image and contour plots of the classic Maunga Whau volcano data, as well as an overlay of the contours on the image plot.\nA chloropleth is a map with irregular geographic boundaries and regions colored by some value. These are typically used in presenting social or political data. A chloropleth is not really a heatmap but it is often mis-characterized as one.\nThese three types of plots are conceptually unified in that they require a 3D data set. In the case of the heatmap and the image, the underlying data are on a regular x, y grid of values; mathematically, a matrix. The row and column indices are mapped to the x, y values, and the matrix entries are the z values. A chloropleth can be thought of as a very warped matrix where the cells are not on a regular grid but instead a series of arbitrary connected paths, namely the geographic boundaries. There is a value inside each connected path (the z value), but naturally the specification of the paths requires a completely different data structure. An intermediate type would be the cartogram heatmap described by Wilke.\nHeatmaps in Spectroscopy\nThe hmapSpectra function in ChemoSpec displays a heatmap to help you focus on which frequencies drive the separation of your samples.1 We’ll use the example from ?hmapSpectra which uses the built-in SrE.IR data set. This data set is a series of IR spectra of commercial Serenoa repens oils which are composed of mixtures of triglycerides and free fatty acids (see ?SrE.IR for more information). Thus the carbonyl region is of particular interest. The example narrows the frequency range to the carbonyl region for easy interpretation. Let’s look first at the spectra.\nNote: rather than link every mention of a help page in this post, remember you can see all the documentation at this site.\n\n\nlibrary(\"ChemoSpec\")\ndata(SrE.IR) # load the data set\n# limit to the carbonyl region\nIR <- removeFreq(SrE.IR, rem.freq = SrE.IR$freq > 1775 | SrE.IR$freq < 1660)\nplotSpectra(IR, which = 1:16, lab.pos = 1800)\n\n\n\n\nThe blue and green spectra are samples composed only of triglycerides, and hence the ester carbonyl is the primary feature. All other samples are clearly mixtures of ester and carboxylic acid stretching peaks. And now for the heatmap, using defaults:\n\n\nres <- hmapSpectra(IR)\n\n\n\n\nIn this default display, you’ll notice that the rows and column labels are indices to the underlying sample names and frequency list. This is not so helpful. The color scheme is not so exciting either. hmapSpectra uses the package seriation which in turn uses the heatmap.2 function in package gplots. Fortunately we can use the ... argument to pass additional arguments to heatmap.2 to get a much more useful plot.\nCustomizing the hmapSpectra Display\n\n\n# Label samples and frequencies by passing arguments to heatmap.2\n# Also make a few other nice plot adjustments\nres <- hmapSpectra(IR,\n  col = heat.colors(5),\n  row_labels = IR$names, col_labels = as.character(round(IR$freq)),\n  margins = c(4, 6)\n)\n\n\n\n\nThis is a lot nicer plot, since the rows are labeled with the sample names, and the columns with frequencies. Note that not every column is labeled, only every few frequencies. If you need the actual frequencies, which you probably will, they can be obtained from the returned object (res in this case; see the end of this post for an example).\nInterpreting the Plot\nHow do we interpret this plot? This is a seriated heatmap, which means the rows and columns have been re-ordered according to some algorithm (more on this in a moment). The ordering puts the frequencies most important in distinguishing the samples in the upper left and lower right (the yellow regions). In the lower right corner, we see the two outlier samples TJ_OO and SV_EPO grouped together. On the frequency axis, we see that ester stretching peaks around 1740 \\(\\mathrm{cm}^1\\) are characteristic for these samples. In the upper left corner, we see several samples grouped together, and associated with the fatty acid carboxylic acid peak around 1710 \\(\\mathrm{cm}^1\\). From these two observations, we can conclude that these two peak ranges are most important in separating the samples. Of course, in this simple example using a small part of the spectrum, this answer was already clear by simple inspection. Using a simple/limited range of data helps us to be sure we understand what’s happening when we try a new technique.\nUsing a Different Distance Measure & Seriation Method\nThe default data treatments for hmapSpectra are inherited from hmap in package seriation. The default distance between the samples is the Euclidean distance. The default seriation method is “OLO” or “optimal leaf ordering”. The full list of seriation methods is described in ?seriate. There are more than 20 options. As with the display details, we can change these defaults via the ... arguments. Let’s use the cosine distance (the same as the Pearson distance), and seriate using the Gruvaeus-Wainer algorithm (there’s a brief explanation of this algorithm at ?seriate).\n\n\ncosine_dist <- function(x) as.dist(1 - cor(t(x)))\nres <- hmapSpectra(IR,\n  col = heat.colors(5),\n  row_labels = IR$names, col_labels = as.character(round(IR$freq)),\n  margins = c(4, 6),\n  distfun = cosine_dist,\n  method = \"GW\"\n)\n\n\n\n\nYou can see that using different distance measures and seriation algorithms gives a rather different result: the ester “hot spots” which were in the lower right corner are now almost in the lower left corner. Which settings are best will depend on your data set, the goal of your analysis, and there are a lot of options from which to choose. The settings used here are simply for demonstration purposes, I make no claim these settings are appropriate!\nFinally, if you want to capture the re-ordered frequencies, you can access them in the returned object:\n\n\nround(IR$freq[res$colInd$order])\n\n\n [1] 1767 1765 1763 1761 1759 1757 1755 1753 1751 1749 1747 1745 1743\n[14] 1741 1740 1738 1736 1734 1732 1730 1768 1774 1770 1772 1728 1726\n[27] 1724 1722 1720 1718 1716 1714 1713 1711 1709 1707 1705 1703 1701\n[40] 1699 1697 1695 1693 1691 1689 1687 1660 1664 1666 1678 1680 1686\n[53] 1682 1676 1674 1672 1662 1668 1670 1684\n\nOther functions in ChemoSpec that can help you explore which frequencies are important are plotLoadings, plot2Loadings and sPlotSpectra.\n\n↩︎\n",
    "preview": "posts/2020-04-25-Heatmaps/2020-04-25-Heatmaps_files/figure-html5/showSpectra-1.png",
    "last_modified": "2021-11-22T19:45:57-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-21-Data-Formats-Pt1/",
    "title": "Data Sharing in the Age of Coronavirus, Part 1",
    "description": "ASCII & JCAMP-DX formats",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-03-21",
    "categories": [
      "Data Formats",
      "JCAMP-DX",
      "ASCII"
    ],
    "contents": "\n\n\n\nThis is Part 1 of a series of posts about data formats for sharing spectroscopic data. Many folks are working from home due to a certain global pandemic. I hope you are all healthy and practicing proper social distancing!\nSharing data is intrinsic to any spectroscopic work. For many tasks, the data need never leave the instrument’s native format. Nowadays the data often goes immediately to some type of shared server, to be available for multiple users. So much of the time we don’t need to worry about format at all, especially if the acquisition software can do the chemometric analysis you need.\nIncreasingly however, publishers want all data deposited and documented somewhere in machine-readable, vendor-neutral form. This is one aspect of reproducible research, where all data and the scripts or steps needed to analyze them are provided electronically with every paper. Or, your data may be headed to one of the many databases out there, where specific formats are required for submission. And while most data acquisition softwares provide some analysis options, if you need to do serious chemometric analysis you likely need to get the data off the machine in a vendor-neutral form. So there are several reasons one should be familiar with the various means of sharing spectroscopic data.\nData sharing/exchange is admittedly a potentially mundane topic. After all, we just want to get on with the scientific question. However, it’s worth knowing something about the options and considering the future of the field. In general I’d say things are a bit of a mess with no clear path to a common format. This series of posts will cover several different vendor-neutral data sharing formats, their pros and cons and their future prospects.\nASCII Files\nAlmost all spectroscopic instruments have some means of exporting data as simple ASCII format files. For 1D data, these usually take the form of columns of wavelengths (or the equivalent) and some form of intensity values. There may or may not be metadata and/or headers in the file. The resolution of the data in the file is usually sufficient, but it can be as low as 8-bit precision. Simple inspection is usually enough to understand these files, and eventually, read them in with R or Python, since other than the metadata these files are simply x and y values in columns.\n2D NMR data in ASCII format are a bit more tedious to decipher. Assuming we are talking about data that has been processed, there are choices to be made about ordering the data and no standardization is evident in the wild. Do you export the data by rows (F2 values at fixed F1 value), by columns (F1 values at fixed F2 value), or an entire matrix? Do you export in a format that mirrors how we typically look at the data, namely the lowest F1 values are first and the lowest F2 values last? 2D NMR is unique among 2D plots in not having 0,0 in any corner. Or do you export in an increasing order, as though you were starting from 0,0? While there are a lot of combinations possible, through trial-and-error one can determine how the data was exported. This is naturally easier if you have a reference spectrum for comparison. I can say from experience that this task is do-able but annoying. Some vendors also export hypercomplex data, in which there is a copy of the data that has been transformed only along F2 and a copy in which transformation has occurred on both dimensions.\nIn addition to deciphering how the data is stored in an exported ASCII file, one needs to keep in mind file size, because ASCII values are not compressed. If one is dealing with IR or UV-Vis data, the typically small number of data points means the files are not large, making ASCII export a good option. For 1D NMR data with typically > 16K data points, the size of the files begins to matter a bit, especially if you have large collections of spectra, which are becoming increasingly common with autosamplers. With 2D NMR, spectra in ASCII format begin to take up some serious space, and the time needed to read in the data becomes noticable.\nPros & Cons of the ASCII Format\n\n\n\n\n\nPros\n\n\nCons\n\n\nNear-universal availability\n\n\nRarely any metadata\n\n\nHuman readible\n\n\nRarely any documentation\n\n\n\n\nSlow to parse for large data sets\n\n\n\n\nFor 2D NMR, internal order must be deciphered\n\n\nThe Future of the ASCII Format\nBecause of it’s relative simplicity, and near-universal implementation in vendor software, ASCII formatted export files are here to stay.\nJCAMP-DX Files\nThe History of JCAMP-DX Format\nThe JCAMP-DX format and standard began at a time when hard drive space was expensive and read/write/transmission errors by hardware were a real issue. This was way before the internet: we are talking about transferring data via telephone/modem, magnetic tape and simple OCR. Hence, three key concerns were to compress the data, to build in data integrity checks and to be flexible for future expansion. Two spectroscopists working with IR data, Robert McDonald and Paul Wilks Jr., published the first standard in 1988 (McDonald and Wilks 1988), with input from instrument manufacturers. From the begininng JCAMP-DX was a project of JCAMP, the Joint Committee on Atomic and Physical Data, a committee of the IUPAC. Refinements were published in 1991 (Grasselli 1991), support for NMR was added in 1993 (A. Davies and Lampen 1993), and MS in 1994 (Lampen et al. 1994) by which time the standard was at version 5 (Lampen et al. 1999). Extensions for CD (Woollett et al. 2012), ion mobility spectrometry (Baumbach et al. 2001) and electron magnetic resonance have been proposed (Cammack et al. 2006). Interestingly, there was also an attempt to describe structure (connectivity) using the format (Gasteiger et al. 1991). In 2001 a JCAMP-DX standard for NMR pulse sequences was published (A. N. Davies et al. 2001).\nAn Example\nAnother goal for the format was to have the format be both human and machine readible. The format is composed of metadata describing the data and then the compressed data. There are several compression formats possible; some are more human readible than others! Here is a simple example of a JCAMP-DX file containing part of an IR spectrum. The blue box contains the metadata, which is clearly human readible and indeed, most meanings are immediately obvious. The orange box contains the compressed data in the “DIFFDUP” format. In another post we might dissect how that works, but for now, we can clearly read the characters but they need to be translated into actual numerical values.\n\nPros & Cons of the JCAMP-DX Format\n\n\nPros\n\n\nCons\n\n\nNear-universal availability\n\n\nMinimal compression by modern standards\n\n\nMetadata human readible\n\n\nError checking makes parsing slow\n\n\nCompression formats can be manually detangled for checking\n\n\nError checking probably no longer needed\n\n\n\n\nVendors do not always follow the standard exactly\n\n\nFuture of the JCAMP-DX Format\nBecause of its long history and universal availability, the JCAMP-DX format appears to be here for the long-haul in spite of its limitations. Future posts in this series will cover data sharing formats that may eventually replace JCAMP-DX.\n\n\n\nBaumbach, JI, AN Davies, P Lampen, and H Schmidt. 2001. “JCAMP-DX. A Standard Format for the Exchange of Ion Mobility Spectrometry Data - (IUPAC recommendations 2001).” Pure and Applied Chemistry 73 (11): 1765–82. https://doi.org/10.1351/pac200173111765.\n\n\nCammack, R, Y Fann, RJ Lancashire, JP Maher, PS McIntyre, and R Morse. 2006. “JCAMP-DX for electron magnetic resonance(EMR).” Pure and Applied Chemistry 78 (3): 613–31. https://doi.org/10.1351/pac200678030613.\n\n\nDavies, AN, and P Lampen. 1993. “JCAMP-DX for NMR.” Applied Spectroscopy 47 (8): 1093–99. https://doi.org/10.1366/0003702934067874.\n\n\nDavies, Antony N., Jörg Lambert, Robert J. Lancashire, and Peter Lampen. 2001. “Guidelines for the Representation of Pulse Sequences for Solution-State Nuclear Magnetic Resonance Spectroscopy.” Pure and Applied Chemistry 73 (11): 1749–64.\n\n\nGasteiger, J., B. M. P. Hendricks, Hoever P., Jochum C., and Somberg H. 1991. “JCAMP-CS: A Standard Exchange Format for Chemical Structure Information in a Computer-Readible Form.” Applied Spectroscopy 45 (1): 4–11.\n\n\nGrasselli, JG. 1991. “JCAMP-DX, A Standard Format for Exchange of Infrared-Spectra in Computer Readible Form.” Pure and Applied Chemistry 63 (12): 1781–92. https://doi.org/10.1351/pac199163121781.\n\n\nLampen, P, H Hillig, AN Davies, and M Linscheid. 1994. “JCAMP-DX for Mass Spectrometry.” Applied Spectroscopy 48 (12): 1545–52.\n\n\nLampen, P, J Lambert, RJ Lancashire, RS McDonald, PS McIntyre, DN Rutledge, T Frohlich, and AN Davies. 1999. “An Extension to the JCAMP-DX Standard File Format, JCAMP-DX V.5.01 (IUPAC Recommendations 1999).” Pure and Applied Chemistry 71 (8): 1549–56. https://doi.org/10.1351/pac199971081549.\n\n\nMcDonald, RS, and PA Wilks. 1988. “JCAMP-DX, A Standard Format for Exchange of Infrared-Spectra in Computer Readible Form.” Applied Spectroscopy 42 (1): 151–62. https://doi.org/10.1366/0003702884428734.\n\n\nWoollett, Benjamin, Daniel Klose, Richard Cammack, Robert W. Janes, and B. A. Wallace. 2012. “JCAMP-DX for circular dichroism spectra and metadata (IUPAC Recommendations 2012).” Pure and Applied Chemistry 84 (10): 2171–82. https://doi.org/10.1351/PAC-REC-12-02-03.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-22T19:32:32-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-03-04-NMR-Align-Pt3/",
    "title": "Aligning 2D NMR Spectra Part 3",
    "description": "Putting it all together",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-03-04",
    "categories": [
      "R",
      "ChemoSpec2D",
      "Alignment",
      "NMR"
    ],
    "contents": "\nThis is Part 3 of a series on aligning 2D NMR, as implemented in the package ChemoSpec2D. Part 1 Part2\nLet’s get to work. The function to carry out alignment is hats_alignSpectra2D. The arguments maxF1 and maxF2 define the space that will be considered as the two spectra are shifted relative to each other. The space potentially covered is -maxF1 to maxF1 and similarly for the F2 dimension. dist_method, thres and minimize refer to the objective function, as described in Part 1. In this example we will consider two spectra succcessfully aligned when we get below the threshold. When one shifts one spectrum relative to the other, part of the shifted spectrum gets cutoff and part of it is empty space. fill = \"noise\" instructs the function to fill the empty space with an estimate of the noise from the original spectrum. We’ll set plot = FALSE here because the output is extensive. I’ll provide sample plotting output in a moment.\n\n\n\n\n\nlibrary(\"ChemoSpec2D\")\ndata(MUD2)\nset.seed(123)\n\n\n\n\n\nMUD2a <- hats_alignSpectra2D(MUD2,\n  maxF1 = 5, maxF2 = 5,\n  dist_method = \"euclidean\", thres = 40, minimize = TRUE,\n  fill = \"noise\",\n  plot = FALSE)\n\n\n[ChemoSpec2D] Processing row  1  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 7 \n    with sample(s) 4 \n[ChemoSpec2D] Best alignment is to shift F2 by  0  and F1 by  -1 \n\n[ChemoSpec2D] Processing row  2  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 6 \n    with sample(s) 3 \n[ChemoSpec2D] Best alignment is to shift F2 by  0  and F1 by  -1 \n\n[ChemoSpec2D] Processing row  3  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 5 \n    with sample(s) 2 \n[ChemoSpec2D] Best alignment is to shift F2 by  0  and F1 by  -1 \n\n[ChemoSpec2D] Processing row  4  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 8 \n    with sample(s) 1 \n[ChemoSpec2D] Best alignment is to shift F2 by  0  and F1 by  -1 \n\n[ChemoSpec2D] Processing row  5  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 1, 8 \n    with sample(s) 9 \n[ChemoSpec2D] Best alignment is to shift F2 by  2  and F1 by  1 \n\n[ChemoSpec2D] Processing row  6  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 2, 5 \n    with sample(s) 3, 6 \n[ChemoSpec2D] Best alignment is to shift F2 by  2  and F1 by  0 \n\n[ChemoSpec2D] Processing row  7  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 4, 7 \n    with sample(s) 10 \n[ChemoSpec2D] Best alignment is to shift F2 by  0  and F1 by  3 \n\n[ChemoSpec2D] Processing row  8  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 2, 3, 5, 6 \n    with sample(s) 1, 8, 9 \n[ChemoSpec2D] Best alignment is to shift F2 by  0  and F1 by  3 \n\n[ChemoSpec2D] Processing row  9  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 1, 2, 3, 5, 6, 8, 9 \n    with sample(s) 4, 7, 10 \n[ChemoSpec2D] Best alignment is to shift F2 by  -5  and F1 by  0 \n\n[ChemoSpec2D] Alignment steps and results:\n       Ref                Mask F2shift F1shift\n1        4                   7       0      -1\n2        3                   6       0      -1\n3        2                   5       0      -1\n4        1                   8       0      -1\n5        9                1, 8       2       1\n6     3, 6                2, 5       2       0\n7       10                4, 7       0       3\n8  1, 8, 9          2, 3, 5, 6       0       3\n9 4, 7, 10 1, 2, 3, 5, 6, 8, 9      -5       0\n\nAs the alignment proceeds, updates from the function are prefixed with [ChemoSpec2D]. In the first step we get a message that row 1 of 9 of the guide tree is being processed, in which sample 7 is being aligned with sample 4. The guide tree is shown below. One can see that samples 7 and 4 are very similar, so they are aligned first. If you inspect the output above, you can see that the four most similar pairs of spectra are aligned first, followed by groups of spectra according to similarity. For each alignment the needed shifts are reported. The last part of the output is a summary of all the alignments carried out. Note that the vertical scale on the guide tree is the same as the scale on the sampleDist plot in Part 1 (using the Euclidean distance).\n\nDiagnostics on Space\nTo save space, I suppressed the plotting of the results. However, there are plots! In fact there is a set of plots for each alignment step. Here are two of the plots produced if plot = TRUE; these deal with the X-Space which is the search space (the terminology comes from the mlrMBO package which is designed to handle many types of optimization). This plot is for Step 7. The upper plot shows the search space. Axis x1 corresponds to the F1 dimension, and axis x2 the F2 dimension. The red squares represent the initial experimental design, using the results from the objective function. The blue circles represent additional points added as the search proceeds. These represent new points on the response surface defined by the surrogate function (see Part 2 for background). The orange diamond is the best alignment, which in this case has no shift along F2 but a three data point shift along F1; this corresponds to the output above. The green triangle is the last position tested.\nThe lower plot represents the progress of the search over time. Axis “dob” stands for “date of birth” which is basically the time index of when the test point was added.\n\nDiagnostics on the Objective Function\nThis second set of plots deals with what mlrMBO considers the Y-Space, which concerns the values of the objective function. The top plot is a histogram of the distance (objective function) values; in this case most of them were pretty bad (high, meaning a larger distance between the spectra). The middle plot is the value of the distance over time (dob). In this example the optimal alignment is found at dob = 4, but there is no particular significance to when the optimum is found. The lower plot shows the expected improvement (ei) at each dob. It is lowest when the optimum has been found. For more details about what’s going on under the hood, see the Arxiv paper.\n\nThe Aligned Spectra\nDid this process work? This final plot shows that it did. Let’s be clear that the task here was not terribly hard: MUD2 is an artificial example in which the shifts are pretty modest and global in nature. But still, it’s satisfying. I welcome everyone to give hats_alignSpectra2D a try and report any problems or suggestions.\n\n\nmylvls <- seq(0, 30, length.out = 10)\nplotSpectra2D(MUD2a, which = c(1, 6), showGrid = TRUE,\n  lvls = LofL(mylvls, 2),\n  cols = LofC(c(\"red\", \"black\"), 2, length(mylvls), 2),\n  main = \"Aligned MUD2 Spectra 1 & 6\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3_files/figure-html5/plotMUD2a-1.png",
    "last_modified": "2021-11-23T18:45:02-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-02-NMR-Align-Pt2/",
    "title": "Aligning 2D NMR Spectra Part 2",
    "description": "Implementing the HATS-PR algorithm",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-03-02",
    "categories": [
      "R",
      "ChemoSpec2D",
      "Alignment",
      "NMR"
    ],
    "contents": "\nThis is Part 2 of a series on aligning 2D NMR, as implemented in the package ChemoSpec2D. Part 1 Part 3\nThe HATS-PR Algorithm\nIn Part 1 I briefly mentioned that we would be using the HATS-PR algorithm of Robinette et al. (Robinette et al. 2011). I also discussed the choice of objective function which is used to report on the quality of the alignment. HATS-PR stands for “Hierachical Alignment of Two-Dimensional Spectra - Pattern Recognition”. In ChemoSpec2D the algorithm is implemented in the hats_alignSpectra2D function. Here are the major steps of the HATS-PR algorithm:\nContruct a guide tree using hierarchical clustering (HCA): compute the distance between the spectra, and use these distances to construct a dendrogram (the guide tree). As the name suggests, this tree is used to guide the alignment. The most similar spectra are aligned first, then the next most similar, and so on. In later rounds one applies the alignment procedure to sets of spectra that have already been aligned. In Robinette et al. they use the Pearson correlation coefficient as the distance measure. In ChemoSpec2D you can choose from a number of distance measures. I encourage you to experiment with the choices and see how they affect the alignment process for your data sets.\nFor each alignment event, check the alignment using the objective function, which recall is a distance measure. If the objective function is below the threshold, no alignment is needed (“below” assumes we are minimizing the objective function, but we might also be maximizing and hence trying to exceed the threshold). If alignment is necessary, move one of the spectra relative to the other in some fashion, checking each new postion with the objective function until the best alignment is found. This is an exercise in optimization.\nFinding the Optimal Alignment\nThe heart of the task is in the phrase in some fashion. At one extreme, one can imagine holding one spectrum fixed, and sliding the other spectrum left and right, up and down, over some range of values – essentially a grid of data points. At each position on the virtual grid, evaluate the objective function and keep track of the results. This will always find the answer, but such a brute force search will be very time-consuming and undesirable, especially if the search space is large. Alternatively, do something more efficient! Robinette et al. use a “simple gradient ascent” approach, but there is a vast literature on optimization strategies that we can consider. In ChemoSpec2D we use a machine learning approach (details next), but the function is written in such a way that one can add other optimization approaches seamlessly. Anything is better than a brute force approach.\nOptimizing with mlrMBO\nThe name mlrMBO comes from “machine learning with R model-based optimization.” mlrMBO is a powerful and flexible package for general purpose optimization, especially in the cases where the objective function is computationally expensive. There is a nice introductory vignette.\nThe basic steps in the model-based optimization using mlrMBO as implemented in hats_alignSpectra2D in package ChemoSpec2D are as follows:\nDefine your objective function. Our choice of the Euclidean distance was described in Part 1, along with other options. Most distance measures are not computationally expensive in terms of code. However, the huge number of data points in a typical 2D NMR spectrum bogs things down considerably. The approach taken in model-based optimation mitigates this to a great deal, since the objective function is only used for the initial response surface.\nGenerate an “initial design”, by which we mean a strategy to search the possible optimization space. hats_alignSpectra2D takes arguments maxF1 and maxF2 which define the space that will be considered as the two spectra are shifted relative to each other. The space potentially covered is -maxF1 to maxF1 and similarly for the F2 dimension. We take advantage of concepts from the design of experiments field, and use the lhs package to generate a Latin Hypercube Sample of our space.\nThe sample points selected by lhs are evaluated using the objective function.\nThe values of the objective function at the sample points are used to create a surrogate model, essentially a response surface. The key here is that the surrogate model is computationally fast and will stand in for the actual objective function during the optimization. mlrMBO provides many options for the surrogate model. For hats_alignSpectra2D we use a response surface based on kriging, which is a means of interpolating values that was originally developed in the geospatial statistics world.\nNew samples points are suggested by the kriging algorithm, evaluated using the surrogate function, and used to update (improve) the model. Each iteration improves the quality of the model.\nAfter reaching the designated threshold or the number of iterations specified, the best answer is returned. In this case the best answer is the optimal shift of one spectrum relative to the other, in each dimension.\nOther Details\nIn addition to the differences noted above, the implementation of HATS-PR in ChemoSpec2D carries out only global alignment. The algorithm described by Robinette et al. includes local alignment steps which I have not implemented. Local alignment is a possible future addition.\nConfigure Your Workspace\nIf you are going to actually execute the code here (as opposed to just reading along), you’ll need the development version of ChemoSpec2D (I improved some of the plots that track the alignment progress since the last CRAN release). And you’ll need certain packages. Here are the steps to install everything:\n\n\nchooseCRANmirror() # choose a CRAN mirror\ninstall.packages(\"remotes\")\nlibrary(\"remotes\")\n# devel branch -- you need 0.4.156 or higher\ninstall_github(repo = \"bryanhanson/ChemoSpec2D@devel\")\nlibrary(\"ChemoSpec2D\")\n# other packages needed\ninstall.packages(\"mlrMBO\") # will also install mlr, smoof, ParamHelpers\ninstall.packages(\"lhs\")\n\n\n\nNow you are ready for the main event! Part 3\n\n\n\nRobinette, Steven L., Ramadan Ajredini, Hasan Rasheed, Abdulrahman Zeinomar, Frank C. Schroeder, Aaron T. Dossey, and Arthur S. Edison. 2011. “Hierarchical Alignment and Full Resolution Pattern Recognition of 2d NMR Spectra: Application to Nematode Chemical Ecology.” Analytical Chemistry 83 (5): 1649–57. https://doi.org/10.1021/ac102724x.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-23T15:22:01-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-20-NMR-Align-Pt1/",
    "title": "Aligning 2D NMR Spectra Part 1",
    "description": "Quantifying the mis-alignment",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-02-20",
    "categories": [
      "ChemoSpec",
      "R",
      "Alignment",
      "NMR"
    ],
    "contents": "\nIn this series of posts, I’ll discuss the alignment process for the case of 2D NMR, as implemented in the package ChemoSpec2D. This is Part 1. Part 2. Part 3.\n\nIn one-dimensional \\(^1\\)H NMR spectroscopy, particularly biomolecular NMR, it is frequently necessary to align spectra before chemometric or metabolomics analysis. Poor alignment arises largely from pH and ionic strength induced shifts in aqueous samples. There are a number of published alignment algorithms for the one-dimensional case. The same issue presumably exists for 2D NMR spectra, but alignment options are limited. Instead, for 2D NMR people often work with tables of peaks. Creating these tables is an extra step and decisions about what to include may leave useful information behind.\nNo doubt you’ve compared two spectra by overlaying them on the screen, or printing them out, placing them on top of each other, and holding them up to the light. Conceptually, one can “align” spectra by a similar method: just slide one of the pieces of paper up/down and left/right until the spectra are optimally aligned. But how would one do this algorithmically? A literature searched turned up only a few publications on this topic. Among these, there was only one that I felt I could implement using the description in the paper: the HATS-PR algorithm of Robinette et al. (Robinette et al. 2011).\nWe’ll discuss the HATS algorithm in a future post. As a first step however, we need to consider how we know when two spectra are properly aligned. Visual inspection won’t work, as we will encounter cases where peaks in one region align, but only at the expense of peaks in another region. How would we rank such cases? To automate this process, we need to use an objective function, basically some kind of equation, that we evaluate as we explore the alignment space. A simple but effective option is to compute the distance between the two spectra. This is done by concatenating each row of the 2D spectra to give a long vector of intensities. The distance between these vectors can then be computed using any of the standard distance definitions. Let’s illustrate, starting by taking a look at some mis-aligned data. ChemoSpec2D contains a mis-aligned data set, MUD2, for just this purpose. Here are two spectra from MUD2; note we are using the new convenience functions LofC and LofL to make it easy to overlay the spectra.\n\n\n\n\n\nlibrary(\"ChemoSpec2D\")\ndata(MUD2)\nmylvls <- seq(0, 30, length.out = 10)\nplotSpectra2D(MUD2, which = c(1, 6), showGrid = TRUE,\n  lvls = LofL(mylvls, 2),\n  cols = LofC(c(\"red\", \"black\"), 2, length(mylvls), 2),\n  main = \"MUD2 Spectra 1 & 6\")\n\n\n\n\nThe function sampleDist allows us to compute the distance between every pair of spectra in the MUD2 data set, and present the results as a heat map. Here are the results using cosine as the distance measure.\n\n\ncos_dist <- sampleDist(MUD2, method = \"cosine\",\n  main = \"Cosine Distance\")\n\n\n\n\nThe actual numerical values are in cos_dist, a matrix. Looking at the heatmap, there are some modest differences among the spectra. However, if one looks at the scale, cosine distances are only defined on [-1 … 1]. While the cosine distance is popular in many spectroscopic contexts, it’s not the best objective function for our purpose because there is little absolute difference between -1 and 1 (and for MUD2 the absolute differences are even smaller, as the range is only 0, 0.99). This limited range affects the alignment process in a subtle way that we won’t cover here (alignment is still successful, however).\nLet’s consider instead the Euclidean distance.\n\n\neu_dist <- sampleDist(MUD2, method = \"euclidean\",\n  main = \"Euclidean Distance\")\n\n\n\n\nIt turns of that the Euclidean distance gives a wider span of distances, which will serve us well in the next steps. Here, the range is roughly 80, 150. Note that in this plot the distance between identical spectra is zero, plotted as a white squares along the diagonal. When we used cosine as the distance, identical spectra were perfectly correlated and hence the diagonal in that plot was red.\nIn the next post I’ll discuss the general flow of the HATS algorithm, and how to carry it out using ChemoSpec2D.\n\n\n\nRobinette, Steven L., Ramadan Ajredini, Hasan Rasheed, Abdulrahman Zeinomar, Frank C. Schroeder, Aaron T. Dossey, and Arthur S. Edison. 2011. “Hierarchical Alignment and Full Resolution Pattern Recognition of 2d NMR Spectra: Application to Nematode Chemical Ecology.” Analytical Chemistry 83 (5): 1649–57. https://doi.org/10.1021/ac102724x.\n\n\n\n\n",
    "preview": "posts/2020-02-20-NMR-Align-Pt1/2020-02-20-NMR-Align-Pt1_files/figure-html5/MUD2-1.png",
    "last_modified": "2021-11-23T18:38:04-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-02-19-CS2D-update/",
    "title": "ChemoSpec2D Update",
    "description": "Improvements to ChemoSpec2D",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-02-19",
    "categories": [
      "R",
      "ChemoSpec2D"
    ],
    "contents": "\nI’m pleased to announce that ChemoSpec2D, a package for exploratory data analysis of 2D NMR spectra, has been updated on CRAN and is coming to a mirror near you. Barring user reports to the contrary, I feel like the package has pretty much stabilized and is pretty robust. The main area for future expansion is to add additional data import routines. Please feel free to ask about your specific use case!\nThe most noteworthy user-facing improvements are:\nFunction import2DSpectra can now handle JCAMP-DX files, Bruker files exported via the TopSpin “totxt” command, and JEOL spectra exported as “generic ascii”. The design allows additional formats to be added if I have test files to play with (hint hint).\nfiles2Spectra2DObject gains a new argument allowSloppy. This experimental feature will allow one to import data sets that do not have the same dimensions. The intent here is to deal with data sets where the number of points in each dimension is similar but not identical. Additional functions will be needed to handle this kind of data. See the documentation for details.\nfiles2Spectra2DObject has also been modified to allow arguments to be passed to list.files and readJDX. This means for instance you can specify a path other than the current working directory, and have the function recurse through sub-directories. This brings files2Spectra2DObject into line with ChemoSpec::files2SpectraObject.\nFunction hats_alignSpectra2D gains new arguments dist_method and maximize which allows the user to pass their choice of distance measure through to the objective function used to evaluate the overlap of the spectra. This greatly improves the quality of the alignment.\nPlotting is simplified with the addition of two new functions to create Lists of Colors, LofC and Lists of Levels, LofL.\nThe basic color scheme for contours was updated to use a perceptually consistent low/blue -> high/red scheme, based on the principles in the colorspace package. The color-handling infrastructure was also changed to allow easy introduction of different color schemes in the future, though the user cannot yet make changes on the fly.\nIn addition, a number of small bugs and annoyances were taken care of, arguments tweaked and documentation improved and expanded. Several functions were rebuilt to make them more robust.\nPlease see the package website for the full changelog and all documentation.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-22T18:42:44-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-25-GH-Topics/",
    "title": "Exploring Github Topics",
    "description": "Scrape Github for Repos of Interest",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-01-25",
    "categories": [
      "R",
      "Github"
    ],
    "contents": "\nAs the code driving FOSS for Spectroscopy has matured, I began to think about how to explore Github in a systematic way for additional repositories with tools for spectroscopy. It turns out that a Github repo can have topics assigned to it, and you can use the Github API to search them. Wait, what? I didn’t know one could add topics to a repo, even though there is a little invite right there under the repo name:\n\nNaturally I turned to StackOverflow to find out how to do this, and quickly encountered this question. It was asked when the topics feature was new, so one needs to do things just a bit differently now, but there is a way forward.\nBefore we get to implementation, let’s think about limitations:\nThis will only find repositories where topics have been set. I don’t know how broadly people use this feature, I had missed it when it was added.\nGithub topics are essentially tags with a controlled vocabulary, so for the best results you’ll need to manually explore the tags and then use these as your search terms.\nThe Github API only returns 30 results at a time for most types of queries. For our purposes this probably doesn’t matter much. The documentation explains how to iterate to get all possible results.\nThe Github API also limits the number of queries you can make to 60/hr unless you authenticate, in which case the limit goes to 6000/hr.\nLet’s get to it! First, create a Github access token on your local machine using the instructions in this gist. Next, load the needed libraries:\n\n\n\n\n\nset.seed(123)\nlibrary(\"httr\")\nlibrary(\"knitr\")\nlibrary(\"kableExtra\")\n\n\n\nSpecify your desired search terms, and create a list structure to hold the results:\n\n\nsearch_terms <- c(\"NMR\", \"infrared\", \"raman\", \"ultraviolet\", \"visible\", \"XRF\", \"spectroscopy\")\nresults <- list()\n\n\n\nCreate the string needed to access the Github API, then GET the results, and stash them in the list we created:\n\n\nnt <- length(search_terms) # nt = no. of search terms\nfor (i in 1:nt) {\n  search_string <- paste0(\"https://api.github.com/search/repositories?q=topic:\", search_terms[i])\n  request <- GET(search_string, config(token = github_token))\n  stop_for_status(request) # converts http errors to R errors or warnings\n  results[[i]] <- content(request)\n}\nnames(results) <- search_terms\n\n\n\nFigure out how many results we have found, set up a data frame and then put the results into the table. The i, j, and k counters required a little experimentation to get right, as content(request) returns a deeply nested list and only certain items are desired.\n\n\nnr <- 0L # nr = no. of responses\nfor (i in 1:nt) { # compute total number of results/items found\n  nr <- nr + length(results[[i]]$items)\n}\n\nDF <- data.frame(term = rep(NA_character_, nr),\n  repo_name = rep(NA_character_, nr),\n  repo_url = rep(NA_character_, nr),\n  stringsAsFactors = FALSE)\n\nk <- 1L\nfor (i in 1:nt) {\n  ni <- length(results[[i]]$items) # ni = no. of items\n  for (j in 1:ni) {\n    DF$term[k] <- names(results)[[i]]\n    DF$repo_name[k] <- results[[i]]$items[[j]]$name\n    DF$repo_url[k] <- results[[i]]$items[[j]]$html_url\n    k <- k + 1L\n  }\n}\n# remove duplicated repos which result when repos have several of our\n# search terms of interest.\nDF <- DF[-which(duplicated(DF$repo_name)),]\n\n\n\nNow put it all in a table we can inspect manually, send to a web page so it’s clickable, or potentially write it out as a csv (If you want this as a csv you should probably write the results out a bit differently). In this case I want the results as a table in web page so I can click the repo links and go straight to them.\n\n\nnamelink <- paste0(\"[\", DF$repo_name, \"](\", DF$repo_url, \")\")\nDF2 <- data.frame(DF$term, namelink, stringsAsFactors = FALSE)\nnames(DF2) <- c(\"Search Term\", \"Link to Repo\")\n\n\n\nWe’ll show just 10 random rows as an example:\n\n\nkeep <- sample(1:nrow(DF2), 10)\noptions(knitr.kable.NA = '')\nkable(DF2[keep, ]) %>%\n  kable_styling(c(\"striped\", \"bordered\"))\n\n\n\n\n\n\n\nSearch Term\n\n\nLink to Repo\n\n\n31\n\n\ninfrared\n\n\npycroscopy\n\n\n79\n\n\nultraviolet\n\n\nwoudc-data-registry\n\n\n51\n\n\ninfrared\n\n\nir-repeater\n\n\n14\n\n\nNMR\n\n\nspectra-data\n\n\n67\n\n\nraman\n\n\nRaman-spectra\n\n\n42\n\n\ninfrared\n\n\nPrecIR\n\n\n50\n\n\ninfrared\n\n\nesp32-room-control-panel\n\n\n118\n\n\nspectroscopy\n\n\nLiveViewLegacy\n\n\n43\n\n\ninfrared\n\n\narduino-primo-tutorials\n\n\n101\n\n\nXRF\n\n\nweb_geochemistry\n\nObviously, these results must be inspected carefully as terms like “infrared” will pick up projects that deal with infrared remote control of robots and so forth. As far as my case goes, I have a lot of new material to look through…\nA complete .Rmd file that carries out the search described above, and has a few enhancements, can be found at this gist.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-23T18:37:06-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-24-CS-update/",
    "title": "ChemoSpec Update",
    "description": "Improvements to ChemoSpec",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-01-24",
    "categories": [
      "R",
      "ChemoSpec"
    ],
    "contents": "\nChemoSpec has just been updated to version 5.2.12, and should be available on the mirrors shortly.\nThe most significant user-facing changes are actually in the update to ChemoSpecUtils from a few days ago. In addition, the following documentation changes were made:\nAdded documentation for updateGroups which has been in ChemoSpecUtils for a while but effectively hidden from users of ChemoSpec.\nFixed the example in mclustSpectra which had an error and used data that was not a good illustration.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-23T15:18:19-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-22-CSU-update/",
    "title": "ChemoSpecUtils Update",
    "description": "New version of ChemoSpecUtils",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-01-22",
    "categories": [
      "R",
      "ChemoSpecUtils"
    ],
    "contents": "\nChemoSpecUtils, a package that supports the common needs of ChemoSpec and ChemoSpec2D, has been updated on CRAN and is coming to a mirror near you. Noteworthy changes:\nThere are new color options available in addition to the auto color scheme used during data importing. These should be useful to normal-vision individuals when there are a lot of categories. The auto option remains the default to avoid breaking anyone’s code. All the built-in color schemes are shown below. They can be used in any of the import functions in either package. The code used to make the figure below is in ?colorSymbol. Note: you probably should get the devel version to ChemoSpec in order to see the documentation about how to use the new colors.\nThe function removeFreq in ChemoSpec now accepts a formula for the specification of the frequencies to remove. This brings it in line with the corresponding function in ChemoSpec2D. This should be a lot easier to use.\nThe function sampleDist is now available and replaces sampleDistSpectra. Again the functions in the two overlying packages are essentially as similar as they can be.\nThis version is compatible with the upcoming release of R 4.0.\n\n\n\n\n\n\n",
    "preview": "posts/2020-01-22-CSU-update/2020-01-22-CSU-update_files/figure-html5/colors-1.png",
    "last_modified": "2021-11-22T18:36:34-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1344
  },
  {
    "path": "posts/2020-01-22-F4S-update/",
    "title": "FOSS for Spectroscopy Update",
    "description": "Improvements to FOSS for Spectroscopy",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-01-22",
    "categories": [
      "R",
      "Python"
    ],
    "contents": "\nFOSS for Spectroscopy has had a significant update! It’s really quite surprising how many projects are out there. There is a lot of variety and not too much overlap.\nAfter a lot of wrestling with Github access issues, the Status column in the table now gives the date of the most recent update to the project that I can find in an automated way.\nThe Notes column is now called Focus and reflects the focus of the projects as far as I can determine things. I’m using a more-or-less controlled vocabulary here, so sorting on the Focus column should bring related projects together.\nThe number of entries is greatly expanded (and I have more in the hopper).\nThe page is now automatically updated weekly, which will keep the links and dates fresh.\nAs always, I welcome your feedback in any form. You can use the comments below, or if you have additions/corrections to the page itself, there is info there about how to submit updates.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-22T18:38:51-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-02-readJDX-update/",
    "title": "readJDX Overhaul",
    "description": "Improvements to readJDX",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-01-02",
    "categories": [
      "R",
      "readJDX",
      "data formats"
    ],
    "contents": "\nreadJDX reads files in the JCAMP-DX format used in the field of spectroscopy. A recent overhaul has made it much more robust, and as such the version is now at 0.4.29.1 Most of the changes were internal, but three important user-facing changes are:\nimproved documentation\nthe addition of more vignettes\nimproved output when debug > 0\n2D NMR files are now handled\nYou can see more about the package here. As always, if you use the package and have troubles, please file an issue. The JCAMP-DX standard is challenging and vendors have a lot of flexibility, so please do share any problematic files you encounter.\nThe current version also includes changes in an unreleased version (0.3.372) in which several bugs were squashed.\n\n↩︎\n",
    "preview": {},
    "last_modified": "2021-11-22T18:35:33-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-01-Intro-F4S/",
    "title": "FOSS for Spectroscopy",
    "description": "A Collection of Free & Open Source Spectroscopy Resources",
    "author": [
      {
        "name": "Bryan Hanson",
        "url": {}
      }
    ],
    "date": "2020-01-01",
    "categories": [
      "R",
      "Python"
    ],
    "contents": "\nFor this inaugural blog post, I’m pleased to share a project I have been working on recently: FOSS4Spectroscopy is an attempt to catalog FOSS spectroscopy software. The page is designed to be updated easily with new or edited entries – see the page for details, and please contribute!\nFor this initial version, I searched the CRAN ecosystem via packagefinder and the Python world via PyPi.org using spectroscopy-related keywords. My expertise is in R so I’m pretty confident I have most of the R packages included. I’m not so confident about coverage of the Python packages (where else should I look?).\nCurrently, the “status” column in the main table is empty. I am working on a version which will update the status with a date giving some indication of the age and activity of the repository. Right now the page is updated when I push to Github, but in the long run I hope to get Travis-CI to run it as a weekly cron job.\nI welcome your feedback in any form!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-22T18:34:06-06:00",
    "input_file": {}
  }
]
