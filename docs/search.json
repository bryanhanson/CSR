[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "The Back Story",
    "section": "",
    "text": "About This Blog\n\n“Open source means everyone can see my stupid mistakes. Version control means everyone can see every stupid mistake I’ve ever made.”\n– Karl Broman\n\nThe purpose of this blog is to provide a place to share my work in the area of Chemometrics & Spectroscopy using FOSS (free and open source) tools. Mostly I work with R but I use other tools as needed. Some areas I expect to write about from time-to-time include:\n\nMajor updates to R packages I author See here\nTutorials on how to accomplish certain tasks, generally inspired by user questions.\nDevelopments in the chemometrics and spectroscopy community.\nOccasionally, discussion of interesting papers.\n\n\n\nAbout Your Host\nYou can reach me at hanson@depauw.edu. Occasionally I am on Twitter.\n\nNow\nI am currently a freelance R consultant with expertise in:\n\nDevelopment of R packages supporting science, especially spectroscopy and chemometrics\nSpectroscopy (NMR, IR, UV-Vis etc)\nChemometrics\nExploratory Data Analysis\nData Visualizations\nData Management: organization, cleaning\nReproducible Research: the automated writing of research reports\nR Training\nWeb page creation and maintenance\nFamiliar with R, markdown, LaTeX, html, css, JavaScript\nResume\n\n\n\nThen\nI retired in June 2018 from DePauw University after 32 years of teaching chemistry and biochemistry. It was a good run. No regrets. Just time to do something else.\n\nKarl Broman quote source"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chemometrics and Spectroscopy Using R",
    "section": "",
    "text": "Mastering Tough Multiplets\n\n\n\nNMR\n\n\n\nAn old dog learns a new trick\n\n\n\n\n\nFeb 6, 2025\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nEF-NMR Part 3: Receiver Software\n\n\n\nEF-NMR\n\nDIY\n\nC\n\nArduino\n\nNMRduino\n\nUtiliDuino\n\n\n\nInstrument control & capturing the FID\n\n\n\n\n\nApr 16, 2024\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nBitwise Operators in C\n\n\n\nC\n\nArduino\n\n\n\nParsing Wild-Type Operators\n\n\n\n\n\nJan 30, 2024\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an EF-NMR Part 2\n\n\n\nEF-NMR\n\nDIY\n\nC\n\nArduino\n\n\n\nProof of concept software, and a heckuva learning curve\n\n\n\n\n\nJan 1, 2024\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an EF-NMR Part 1\n\n\n\nEF-NMR\n\nDIY\n\n\n\nThe polarization coil\n\n\n\n\n\nOct 24, 2023\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nThe n + 1 rule in Earth’s Field NMR\n\n\n\nNMR\n\nEF-NMR\n\nJCS\n\n\n\nI did not know this…\n\n\n\n\n\nSep 18, 2023\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nJEOL’s Delta Now Includes ChemoSpec\n\n\n\nNMR\n\nFOSS\n\n\n\nPretty cool\n\n\n\n\n\nAug 23, 2023\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nFOSS4Spectroscopy Update\n\n\n\nFOSS\n\n\n\nWhat’s changed in a year?\n\n\n\n\n\nAug 15, 2023\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nEarth’s Field NMR\n\n\n\nNMR\n\nEF-NMR\n\n\n\nWhat can we measure? Why or Why Not? Pros? Cons?\n\n\n\n\n\nJul 26, 2023\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nHome Built Photometer\n\n\n\nhardware\n\nelectronics\n\nphotometer\n\n\n\nLearning some practical electronics\n\n\n\n\n\nJul 16, 2023\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nDIY NMR in Earth’s Field\n\n\n\nNMR\n\nEF-NMR\n\nNQR\n\nDIY\n\n\n\nA super simple design\n\n\n\n\n\nJun 12, 2023\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nYou Can Now Subscribe\n\n\n\nHousekeeping\n\n\n\nRemarkably, it seems to work!\n\n\n\n\n\nNov 7, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Linear Algebra Part 4\n\n\n\nR\n\nLinear Algebra\n\n\n\nA Taxonomy of Matrices\n\n\n\n\n\nSep 26, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Linear Algebra Part 3\n\n\n\nR\n\nLinear Algebra\n\n\n\nBase R Functions Related to Linear Algebra\n\n\n\n\n\nSep 10, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Linear Algebra Part 2\n\n\n\nR\n\nLinear Algebra\n\n\n\nMotivations - Bushwhacking through the thicket of linear algebra\n\n\n\n\n\nSep 1, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Linear Algebra Part 1\n\n\n\nR\n\nLinear Algebra\n\n\n\nWhen is a cross product not a cross product? Terminology run amok!\n\n\n\n\n\nAug 14, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nFOSS4Spectroscopy: R vs Python\n\n\n\nFOSS\n\nR\n\nPython\n\nJulia\n\nGithub\n\nPyPi\n\n\n\nNow with more Python and Julia packages, and improved workflow\n\n\n\n\n\nJul 6, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing LearnPCA\n\n\n\nR\n\nPCA\n\n\n\nImprove your understanding of PCA\n\n\n\n\n\nMay 3, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nMetabolic Phenotyping Protocol Part 3\n\n\n\nR\n\nChemoSpec\n\nMetabolomics\n\nPLS\n\nPLS-DA\n\n\n\nImplementing the Statistical Analysis in Metabolic Phenotyping Protocol of Blaise et al.\n\n\n\n\n\nMay 1, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nMetabolic Phenotyping Protocol Part 2\n\n\n\nR\n\nChemoSpec\n\nMetabolomics\n\n\n\nImplementing the Statistical Analysis in Metabolic Phenotyping Protocol of Blaise et al.\n\n\n\n\n\nMar 24, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nChemometrics in Spectroscopy: Key References\n\n\n\nLiterature\n\n\n\nKeep coming back to these sources\n\n\n\n\n\nFeb 18, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nDo You have Stale Imports or Suggests?\n\n\n\nR\n\nUtilities\n\nDevelopers\n\n\n\nHints For Package Developers\n\n\n\n\n\nFeb 9, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nMetabolic Phenotyping Protocol Part 1\n\n\n\nR\n\nChemoSpec\n\nMetabolomics\n\n\n\nImplementing the Statistical Analysis in Metabolic Phenotyping Protocol of Blaise et al.\n\n\n\n\n\nFeb 1, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nGSOC 2021: New Graphics for ChemoSpec\n\n\n\nR\n\nChemoSpec\n\nChemoSpecUtils\n\nChemoSpec2D\n\nGSOC\n\n\n\nMajor improvements!\n\n\n\n\n\nOct 13, 2021\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nGSOC 2021: hyperSpec and ChemoSpec!\n\n\n\nR\n\nhyperSpec\n\nChemoSpec\n\nGSOC\n\n\n\nI’m gonna be busy!\n\n\n\n\n\nMay 22, 2021\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatically Searching Github Repos by Topic\n\n\n\nR\n\nGithub\n\nFOSS\n\nhttr\n\n\n\nHow to find packages of interest\n\n\n\n\n\nApr 19, 2021\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Github Actions and drat to Deploy R Packages\n\n\n\nR\n\nGithub Actions\n\ndrat\n\nGSOC\n\nhyperSpec\n\n\n\nAutomating a tedious task\n\n\n\n\n\nApr 11, 2021\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nSpectroscopy Suite Update\n\n\n\nR\n\nChemoSpec\n\nChemoSpec2D\n\nChemoSpecUtils\n\nreadJDX\n\n\n\nUpdates for the new version of R\n\n\n\n\n\nMar 27, 2021\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nInterfacing ChemoSpec to PLS\n\n\n\nR\n\nPLS\n\nChemoSpec\n\n\n\nIt’s easy to connect the two packages\n\n\n\n\n\nFeb 8, 2021\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nGSOC Wrap Up\n\n\n\nR\n\nhyperSpec\n\nGSOC\n\nGuest Post\n\n\n\nGuest post by Erick Oduniyi\n\n\n\n\n\nSep 8, 2020\n\n\nErick Oduniyi\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating Spectroscopic Data Part 1\n\n\n\nR\n\nSimulated Data\n\nSpecHelpers\n\nBaseline\n\n\n\nFaking it…\n\n\n\n\n\nJun 28, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nFortifying hyperSpec: Getting Ready for GSOC\n\n\n\nR\n\nhyperSpec\n\nGSOC\n\n\n\nRe-factoring hyperSpec\n\n\n\n\n\nMay 7, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nChemoSpecUtils Update\n\n\n\nR\n\nChemoSpecUtils\n\n\n\nImprovments to ChemoSpecUtils\n\n\n\n\n\nApr 27, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nSpectral Heatmaps\n\n\n\nR\n\nHeatmap\n\nSeriation\n\nChemoSpec\n\n\n\nInsights into how samples and frequencies affect clustering\n\n\n\n\n\nApr 25, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nData Sharing in the Age of Coronavirus, Part 1\n\n\n\nData Formats\n\nJCAMP-DX\n\nASCII\n\n\n\nASCII & JCAMP-DX formats\n\n\n\n\n\nMar 21, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nAligning 2D NMR Spectra Part 3\n\n\n\nR\n\nChemoSpec2D\n\nAlignment\n\nNMR\n\n\n\nPutting it all together\n\n\n\n\n\nMar 4, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nAligning 2D NMR Spectra Part 2\n\n\n\nR\n\nChemoSpec2D\n\nAlignment\n\nNMR\n\n\n\nImplementing the HATS-PR algorithm\n\n\n\n\n\nMar 2, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nAligning 2D NMR Spectra Part 1\n\n\n\nR\n\nChemoSpec\n\nAlignment\n\nNMR\n\n\n\nQuantifying the mis-alignment\n\n\n\n\n\nFeb 20, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nChemoSpec2D Update\n\n\n\nR\n\nChemoSpec2D\n\n\n\nImprovements to ChemoSpec2D\n\n\n\n\n\nFeb 19, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Github Topics\n\n\n\nR\n\nGithub\n\n\n\nScrape Github for Repos of Interest\n\n\n\n\n\nJan 25, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nChemoSpec Update\n\n\n\nR\n\nChemoSpec\n\n\n\nImprovements to ChemoSpec\n\n\n\n\n\nJan 24, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nFOSS for Spectroscopy Update\n\n\n\nR\n\nPython\n\n\n\nImprovements to FOSS for Spectroscopy\n\n\n\n\n\nJan 22, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nChemoSpecUtils Update\n\n\n\nR\n\nChemoSpecUtils\n\n\n\nNew version of ChemoSpecUtils\n\n\n\n\n\nJan 22, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nreadJDX Overhaul\n\n\n\nR\n\nreadJDX\n\nData Formats\n\n\n\nImprovements to readJDX\n\n\n\n\n\nJan 2, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nFOSS for Spectroscopy\n\n\n\nR\n\nPython\n\n\n\nA Collection of Free and Open Source Spectroscopy Resources\n\n\n\n\n\nJan 1, 2020\n\n\nBryan Hanson\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-01-01-EF-NMR-Build-2/EF-NMR-Build-2.html",
    "href": "posts/2024-01-01-EF-NMR-Build-2/EF-NMR-Build-2.html",
    "title": "Building an EF-NMR Part 2",
    "section": "",
    "text": "Part 1\nWith the polarization coil completed, I decided to take a stab at the software to control the instrument. I felt like I needed to get a feel for how to work with the Arduino so I would understand what kinds of signals I could send to the electronics. In turn that would (ideally) make it easier to understand how the circuits work.\nI started by studying Michal’s software (available here). Michal’s software is designed for use with students in a lab course and includes a Python GUI, the actual Arduino control software, and several utilities. One of the utilities is a separate pulse programming module that produces a file accessed by the GUI. At least that appears to be the big picture. Inspection of the Arduino software made it clear that I had, and have, a lot to learn. Arduino code is written in C++, which encompases the earlier language C, which some have described as “dressed up assembly language”. Oh boy…\nAfter studying some basic Arduino tutorials, I decided the best way to learn was to write my own software, starting with a simple case of an NMR-like interface that would turn Arduino pins on and off to control the various pieces of hardware I will eventually build. Turning pins on and off is really simple on the Arduino, that’s not the challenge. For this instrument, the challenge is that there are several events that occur one after the other on very short time scales. Roughly, one must turn the polarization coil on, then off, then turn on the transmitter and turn it off, and then turn on the receiver and listen. Due to the realities of electronics, there need to be short delays between some of these events so that the electronic signals can “warm up”, or “cool down”. To make this initial version manageable, I decided to not worry about the time scale in detail for now, and focus on building an extensible framework that takes NMR-like inputs to turn things on and off."
  },
  {
    "objectID": "posts/2024-01-01-EF-NMR-Build-2/EF-NMR-Build-2.html#prototype-in-r",
    "href": "posts/2024-01-01-EF-NMR-Build-2/EF-NMR-Build-2.html#prototype-in-r",
    "title": "Building an EF-NMR Part 2",
    "section": "Prototype in R",
    "text": "Prototype in R\nSince R is my computational lingua franca, I decided to think about how I would set up a series of events in R and calculate their on/off times given the duration (or length) of each event. This was quite straightforward; if you know the duration of each event then the on/off times can be computed with a cumulative sum process.\n\n#' \n#' Convert a Named Vector Giving Event Durations to a Data Frame\n#' \n#' @param event_lengths Numeric.  A named numeric vector giving the durations (lengths)\n#'        of a series of events which occur in the given order.\n#' @return A data frame containing the on and off times for each event.\n#'\nevent_length_to_event_on_off &lt;- function(event_lengths) {\n  off &lt;- cumsum(event_lengths)\n  on &lt;- c(0, off[1:(length(off) - 1)])\n  DF &lt;- data.frame(event = names(event_lengths), on = on, off = off)\n  DF\n}\n\nAnd then I needed a function to visualize the result, which is basically a sort of Gantt chart where the events never overlap.\n\n#'\n#' Create a Gantt Chart of NMR Event Timing\n#'\n#' @param my_events Data frame.\n#' @return `ggplot2` object.\n#'\nevents &lt;- function(my_events = NULL) {\n  p &lt;- ggplot(my_events, aes(x = on, xend = off, y = event, yend = event))\n  p &lt;- p + geom_segment(linewidth = 8) + theme_bw()\n  p &lt;- p + labs(title = \"NMR Event Timing\", x = \"time, microseconds\", y = \"\")\n  p &lt;- p + scale_y_discrete(limits = my_events$event)\n  p\n}\n\nFigure 1 shows these functions in action. So far, so good.\n\nf &lt;- 1e6 # conversion factor, seconds to microseconds\nev &lt;- c(10 * f, 5, 1 * f, 5, 5 * f, 10 * f )\nnames(ev) &lt;- c(\"pol_coil\", \"del_pt\", \"transmitter\", \"del_tr\", \"receiver\", \"relax_delay\")\np1 &lt;- events(event_length_to_event_on_off(ev))\np1\n\n\n\n\n\n\n\nFigure 1: Event timings. The short delays are too short to be visible."
  },
  {
    "objectID": "posts/2024-01-01-EF-NMR-Build-2/EF-NMR-Build-2.html#implementation-in-c",
    "href": "posts/2024-01-01-EF-NMR-Build-2/EF-NMR-Build-2.html#implementation-in-c",
    "title": "Building an EF-NMR Part 2",
    "section": "Implementation in C",
    "text": "Implementation in C\nNext, I decided to write something more or less equivalent in C. This meant learning C. Suffice it to say, C provides none of the niceties of R. There are few atomic types in C, and in particular strings and arrays are not native entities. Instead, one must think in terms of pointers to particular memory addresses that hold the strings or arrays. So the entire paradigm is different, and requires thinking about solving problems in new ways. Overall, this has been a good experience. After a lot of struggle, I managed to write functions that carry out the equivalent of the R functions above, except instead of graphical output there is tabular output (there really is no graphical output in the usual sense for Arduino so we need to have other ways of verifying our results). I won’t give details of this work here, as the next section reviews how it was implemented for Arduino."
  },
  {
    "objectID": "posts/2024-01-01-EF-NMR-Build-2/EF-NMR-Build-2.html#implementation-for-arduino",
    "href": "posts/2024-01-01-EF-NMR-Build-2/EF-NMR-Build-2.html#implementation-for-arduino",
    "title": "Building an EF-NMR Part 2",
    "section": "Implementation for Arduino",
    "text": "Implementation for Arduino\nThe version of event timing in C was adapted to the Arduino with relatively minor modifications, mostly related to how results are printed to the console (the C and C++ languages for Arduino are specialized versions of the languages). I also wrote a system to control the starting and stopping of the scans, thinking ahead of how the program is actually going to be used. All user inputs are in a single file, including a simple version of a pulse program (tons of work will be needed in the future on this piece). My overall goal is to write an entire NMR control and acquistion program that runs completely on the Arduino IDE. Well, almost completely: some other entity will have to slurp up the data coming from the Arduino, as there is very little memory on the Arduino. Not sure if this can be done but that’s the goal. The code for this project is stored in a public repo here.\nThe output of a “run” on this “instrument” is shown in Figure 2. The table lists the event name, the Arduino pin that should be activated, and the on/off times for the events. Times are in milliseconds in the example, and are relevant for testing, not an actual NMR scan. A pin value of -1 indicates no pin is active; such an event is just a delay period so the (not yet built) electronics can settle.\n\n\n\n\n\n\nFigure 2: Output of the NMR acquistion program.\n\n\n\nThis program was further tested by wiring the Arduino to a breadboard with a few LEDs and resistors to limit the current to the LEDs appropriately. The video below shows the program in action, doing two scans with the durations as shown in Figure 2. The pins from left represent polarization coil power, transmit, and receive signal (the latter of course should be listening, not powering something). As a proof of concept I’m pretty happy with this result."
  },
  {
    "objectID": "posts/2024-01-01-EF-NMR-Build-2/EF-NMR-Build-2.html#whats-next",
    "href": "posts/2024-01-01-EF-NMR-Build-2/EF-NMR-Build-2.html#whats-next",
    "title": "Building an EF-NMR Part 2",
    "section": "What’s Next?",
    "text": "What’s Next?\nSo much to do, but I’m not in a hurry and can choose to do things in any order that inspires me:\n\nPolarization coil power supply circuit (some work done on this, just needs to be built)\nTransmitter circuit\nReceiver circuit\nDetails of T/R on the Arduino; this will require another round of intense learning I’m certain!"
  },
  {
    "objectID": "posts/2020-01-22-CSU-update/2020-01-22-CSU-update.html",
    "href": "posts/2020-01-22-CSU-update/2020-01-22-CSU-update.html",
    "title": "ChemoSpecUtils Update",
    "section": "",
    "text": "ChemoSpecUtils, a package that supports the common needs of ChemoSpec and ChemoSpec2D, has been updated on CRAN and is coming to a mirror near you. Noteworthy changes:\n\nThere are new color options available in addition to the auto color scheme used during data importing. These should be useful to normal-vision individuals when there are a lot of categories. The auto option remains the default to avoid breaking anyone’s code. All the built-in color schemes are shown below. They can be used in any of the import functions in either package. The code used to make the figure below is in ?colorSymbol. Note: you probably should get the devel version to ChemoSpec in order to see the documentation about how to use the new colors.\nThe function removeFreq in ChemoSpec now accepts a formula for the specification of the frequencies to remove. This brings it in line with the corresponding function in ChemoSpec2D. This should be a lot easier to use.\nThe function sampleDist is now available and replaces sampleDistSpectra. Again the functions in the two overlying packages are essentially as similar as they can be.\nThis version is compatible with the upcoming release of R 4.0.\n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2020,\n  author = {Hanson, Bryan},\n  title = {ChemoSpecUtils {Update}},\n  date = {2020-01-22},\n  url = {http://chemospec.org/posts/2020-01-22-CSU-update/2020-01-22-CSU-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2020. “ChemoSpecUtils Update.” January 22,\n2020. http://chemospec.org/posts/2020-01-22-CSU-update/2020-01-22-CSU-update.html."
  },
  {
    "objectID": "posts/2020-01-25-GH-Topics/2020-01-25-GH-Topics.html",
    "href": "posts/2020-01-25-GH-Topics/2020-01-25-GH-Topics.html",
    "title": "Exploring Github Topics",
    "section": "",
    "text": "As the code driving FOSS for Spectroscopy has matured, I began to think about how to explore Github in a systematic way for additional repositories with tools for spectroscopy. It turns out that a Github repo can have topics assigned to it, and you can use the Github API to search them. Wait, what? I didn’t know one could add topics to a repo, even though there is a little invite right there under the repo name:\n\nNaturally I turned to StackOverflow to find out how to do this, and quickly encountered this question. It was asked when the topics feature was new, so one needs to do things just a bit differently now, but there is a way forward.\nBefore we get to implementation, let’s think about limitations:\n\nThis will only find repositories where topics have been set. I don’t know how broadly people use this feature, I had missed it when it was added.\nGithub topics are essentially tags with a controlled vocabulary, so for the best results you’ll need to manually explore the tags and then use these as your search terms.\nThe Github API only returns 30 results at a time for most types of queries. For our purposes this probably doesn’t matter much. The documentation explains how to iterate to get all possible results.\nThe Github API also limits the number of queries you can make to 60/hr unless you authenticate, in which case the limit goes to 6000/hr.\n\nLet’s get to it! First, create a Github access token on your local machine using the instructions in this gist. Next, load the needed libraries:\n\nset.seed(123)\nlibrary(\"httr\")\nlibrary(\"knitr\")\nlibrary(\"kableExtra\")\n\nSpecify your desired search terms, and create a list structure to hold the results:\n\nsearch_terms &lt;- c(\"NMR\", \"infrared\", \"raman\", \"ultraviolet\", \"visible\", \"XRF\", \"spectroscopy\")\nresults &lt;- list()\n\nCreate the string needed to access the Github API, then GET the results, and stash them in the list we created:\n\nnt &lt;- length(search_terms) # nt = no. of search terms\nfor (i in 1:nt) {\n  search_string &lt;- paste0(\"https://api.github.com/search/repositories?q=topic:\", search_terms[i])\n    request &lt;- GET(search_string, config(token = github_token))\n  stop_for_status(request) # converts http errors to R errors or warnings\n  results[[i]] &lt;- content(request)\n}\nnames(results) &lt;- search_terms\n\nFigure out how many results we have found, set up a data frame and then put the results into the table. The i, j, and k counters required a little experimentation to get right, as content(request) returns a deeply nested list and only certain items are desired.\n\nnr &lt;- 0L # nr = no. of responses\nfor (i in 1:nt) { # compute total number of results/items found\n    nr &lt;- nr + length(results[[i]]$items)\n}\n\nDF &lt;- data.frame(term = rep(NA_character_, nr),\n  repo_name = rep(NA_character_, nr),\n  repo_url = rep(NA_character_, nr),\n  stringsAsFactors = FALSE)\n\nk &lt;- 1L\nfor (i in 1:nt) {\n    ni &lt;- length(results[[i]]$items) # ni = no. of items\n    for (j in 1:ni) {\n        DF$term[k] &lt;- names(results)[[i]]\n        DF$repo_name[k] &lt;- results[[i]]$items[[j]]$name\n        DF$repo_url[k] &lt;- results[[i]]$items[[j]]$html_url\n        k &lt;- k + 1L\n    }\n}\n# remove duplicated repos which result when repos have several of our\n# search terms of interest.\nDF &lt;- DF[-which(duplicated(DF$repo_name)),]\n\nNow put it all in a table we can inspect manually, send to a web page so it’s clickable, or potentially write it out as a csv (If you want this as a csv you should probably write the results out a bit differently). In this case I want the results as a table in web page so I can click the repo links and go straight to them.\n\nnamelink &lt;- paste0(\"[\", DF$repo_name, \"](\", DF$repo_url, \")\")\nDF2 &lt;- data.frame(DF$term, namelink, stringsAsFactors = FALSE)\nnames(DF2) &lt;- c(\"Search Term\", \"Link to Repo\")\n\nWe’ll show just 10 random rows as an example:\nkeep &lt;- sample(1:nrow(DF2), 10)\noptions(knitr.kable.NA = '')\nkable(DF2[keep, ]) %&gt;%\n  kable_styling(c(\"striped\", \"bordered\"))\n\n\n\n\n\n\n\nSearch Term\n\n\nLink to Repo\n\n\n\n\n\n\n31\n\n\ninfrared\n\n\npycroscopy\n\n\n\n\n79\n\n\nultraviolet\n\n\nwoudc-data-registry\n\n\n\n\n51\n\n\ninfrared\n\n\nir-repeater\n\n\n\n\n14\n\n\nNMR\n\n\nspectra-data\n\n\n\n\n67\n\n\nraman\n\n\nRaman-spectra\n\n\n\n\n42\n\n\ninfrared\n\n\nPrecIR\n\n\n\n\n50\n\n\ninfrared\n\n\nesp32-room-control-panel\n\n\n\n\n118\n\n\nspectroscopy\n\n\nLiveViewLegacy\n\n\n\n\n43\n\n\ninfrared\n\n\narduino-primo-tutorials\n\n\n\n\n101\n\n\nXRF\n\n\nweb_geochemistry\n\n\n\n\nObviously, these results must be inspected carefully as terms like “infrared” will pick up projects that deal with infrared remote control of robots and so forth. As far as my case goes, I have a lot of new material to look through…\nA complete .Rmd file that carries out the search described above, and has a few enhancements, can be found at this gist.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2020,\n  author = {Hanson, Bryan},\n  title = {Exploring {Github} {Topics}},\n  date = {2020-01-25},\n  url = {http://chemospec.org/posts/2020-01-25-GH-Topics/2020-01-25-GH-Topics.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2020. “Exploring Github Topics.” January 25,\n2020. http://chemospec.org/posts/2020-01-25-GH-Topics/2020-01-25-GH-Topics.html."
  },
  {
    "objectID": "posts/2022-02-09-Imports-Suggests/2022-02-09-Imports-Suggests.html",
    "href": "posts/2022-02-09-Imports-Suggests/2022-02-09-Imports-Suggests.html",
    "title": "Do You have Stale Imports or Suggests?",
    "section": "",
    "text": "I’ve been developing packages in R for over a decade now. When adding new features to a package, I often import functions from another package, and of course that package goes in the Imports: field of the DESCRIPTION file. Later, I might change my approach entirely and no longer need that package. Do I remember to remove it from DESCRIPTION? Generally not. The same thing happens when writing a new vignette, and it can happen with the Suggests: field as well. It can also happen when one splits a packages into several smaller packages. If one forgets to delete a package from the DESCRIPTION file, the dependencies become bloated, because all the imported and suggested packages have to be available to install the package. This adds overhead to the project, and increases the possibility of a namespace conflict.\nIn fact this just happened to me again! The author of a package I had in Suggests: wrote to me and let me know their package would be archived. It was an easy enough fix for me, as it was a “stale” package in that I was no longer using it. I had added it for a vignette which I later deleted, as I decided a series of blog posts was a better approach.\nSo I decided to write a little function to check for such stale Suggests: and Import: entries. This post is about that function. As far as I can tell there is no built-in function for this purpose, and CRAN does not check for stale entries. So it was worth my time to automate the process.1\nThe first step is to read in the DESCRIPTION file for the package (so we want our working directory to be the top level of the package). There is a built in function for this. We’ll use the DESCRIPTION file from the ChemoSpec package as a demonstration.\n# setwd(\"...\") # set to the top level of the package\ndesc &lt;- read.dcf(\"DESCRIPTION\", all = TRUE)\nThe argument all = TRUE is a bit odd in that it has a particular purpose (see ?read.dcf) which isn’t really important here, but has the side effect of returning a data frame, which makes our job simpler. Let’s look at what is returned.\nstr(desc)\n\n'data.frame':   1 obs. of  18 variables:\n $ Package         : chr \"ChemoSpec\"\n $ Type            : chr \"Package\"\n $ Title           : chr \"Exploratory Chemometrics for Spectroscopy\"\n $ Version         : chr \"6.1.2\"\n $ Date            : chr \"2022-02-08\"\n $ Authors@R       : chr \"c(\\nperson(\\\"Bryan A.\\\", \\\"Hanson\\\",\\nrole = c(\\\"aut\\\", \\\"cre\\\"), email =\\n\\\"hanson@depauw.edu\\\",\\ncomment = c(\"| __truncated__\n $ Description     : chr \"A collection of functions for top-down exploratory data analysis\\nof spectral data including nuclear magnetic r\"| __truncated__\n $ License         : chr \"GPL-3\"\n $ Depends         : chr \"R (&gt;= 3.5),\\nChemoSpecUtils (&gt;= 1.0)\"\n $ Imports         : chr \"plyr,\\nstats,\\nutils,\\ngrDevices,\\nreshape2,\\nreadJDX (&gt;= 0.6),\\npatchwork,\\nggplot2,\\nplotly,\\nmagrittr\"\n $ Suggests        : chr \"IDPmisc,\\nknitr,\\njs,\\nNbClust,\\nlattice,\\nbaseline,\\nmclust,\\npls,\\nclusterCrit,\\nR.utils,\\nRColorBrewer,\\nser\"| __truncated__\n $ URL             : chr \"https://bryanhanson.github.io/ChemoSpec/\"\n $ BugReports      : chr \"https://github.com/bryanhanson/ChemoSpec/issues\"\n $ ByteCompile     : chr \"TRUE\"\n $ VignetteBuilder : chr \"knitr\"\n $ Encoding        : chr \"UTF-8\"\n $ RoxygenNote     : chr \"7.1.2\"\n $ NeedsCompilation: chr \"no\"\nWe are interested in the Imports and Suggests elements. Let’s look more closely.\nhead(desc$Imports)\n\n[1] \"plyr,\\nstats,\\nutils,\\ngrDevices,\\nreshape2,\\nreadJDX (&gt;= 0.6),\\npatchwork,\\nggplot2,\\nplotly,\\nmagrittr\"\nYou can see there are a bunch of newlines in there (\\n), along with some version specifications, in parentheses. We need to clean this up so we have a simple list of the packages as a vector. For clean up we’ll use the following helper function.\nclean_up &lt;- function(string) {\n  string &lt;- gsub(\"\\n\", \"\", string) # remove newlines\n  string &lt;- gsub(\"\\\\(.+\\\\)\", \"\", string) # remove parens & anything within them\n  string &lt;- unlist(strsplit(string, \",\")) # split the long string into pieces\n  string &lt;- trimws(string) # remove any white space around words\n}\nAfter we apply this to the raw results, we have what we are after, a clean list of imported packages.\nimp &lt;- clean_up(desc$Imports)\nimp\n\n [1] \"plyr\"      \"stats\"     \"utils\"     \"grDevices\" \"reshape2\"  \"readJDX\"  \n [7] \"patchwork\" \"ggplot2\"   \"plotly\"    \"magrittr\"\nNext, we can search the entire package looking for these package names to see if they are used in the package. They might appear in import statements, vignettes, code and so forth, so it’s not sufficient to just look at code. This is a job for grep, but we’ll call grep from within R so that we don’t have to use the command line and transfer the results to R, that gets messy and is error-prone.\nif (length(imp) &gt;= 1) { # Note 1\n  imp_res &lt;- rep(\"FALSE\", length(imp)) # Boolean to keep track of whether we found a package or not\n  for (i in 1:length(imp)) {\n    args &lt;- paste(\"-r -e '\", imp[i], \"' *\", sep = \"\") # assemble arguments for grep\n    g_imp &lt;- system2(\"grep\", args, stdout = TRUE)\n    if (length(g_imp) &gt; 1L) imp_res[i] &lt;- TRUE # Note 2\n  }\n}\nWe can do the same process for the Suggests: field of DESCRIPTION. And then it would be nice to present the results in a more useable form. At this point we can put it all togther in an easy-to-use function.2\n# run from the package top level\ncheck_stale_imports_suggests &lt;- function() {\n\n  # helper function: removes extra characters\n  # from strings read by read.dcf\n  clean_up &lt;- function(string) {\n    string &lt;- gsub(\"\\n\", \"\", string)\n    string &lt;- gsub(\"\\\\(.+\\\\)\", \"\", string)\n    string &lt;- unlist(strsplit(string, \",\"))\n    string &lt;- trimws(string)\n  }\n\n  desc &lt;- read.dcf(\"DESCRIPTION\", all = TRUE)\n\n  # look for use of imported packages\n  imp &lt;- clean_up(desc$Imports)\n  if (length(imp) == 0L) message(\"No Imports: entries found\")\n  if (length(imp) &gt;= 1) {\n    imp_res &lt;- rep(\"FALSE\", length(imp))\n    for (i in 1:length(imp)) {\n      args &lt;- paste(\"-r -e '\", imp[i], \"' *\", sep = \"\")\n      g_imp &lt;- system2(\"grep\", args, stdout = TRUE)\n      # always found once in DESCRIPTION, hence &gt; 1\n      if (length(g_imp) &gt; 1L) imp_res[i] &lt;- TRUE\n    }\n  }\n\n  # look for use of suggested packages\n  sug &lt;- clean_up(desc$Suggests)\n  if (length(sug) == 0L) message(\"No Suggests: entries found\")\n  if (length(sug) &gt;= 1) {\n    sug_res &lt;- rep(\"FALSE\", length(sug))\n    for (i in 1:length(sug)) {\n      args &lt;- paste(\"-r -e '\", sug[i], \"' *\", sep = \"\")\n      g_sug &lt;- system2(\"grep\", args, stdout = TRUE)\n      # always found once in DESCRIPTION, hence &gt; 1\n      if (length(g_sug) &gt; 1L) sug_res[i] &lt;- TRUE\n    }\n  }\n\n  # arrange output in easy to read format\n  role &lt;- c(rep(\"Imports\", length(imp)), rep(\"Suggests\", length(sug)))\n\n  return(data.frame(\n    pkg = c(imp, sug),\n    role = role,\n    found = c(imp_res, sug_res)))\n}\nApplying this function to my ChemoSpec2D package (as of the date of this post), we see the following output. You can see a bunch of packages are imported but never used, so I have some work to do. This was the result of copying the DESCRIPTION file from ChemoSpec when I started ChemoSpec2D and obviously I never went back and cleaned things up."
  },
  {
    "objectID": "posts/2022-02-09-Imports-Suggests/2022-02-09-Imports-Suggests.html#footnotes",
    "href": "posts/2022-02-09-Imports-Suggests/2022-02-09-Imports-Suggests.html#footnotes",
    "title": "Do You have Stale Imports or Suggests?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs you will see in a moment, during testing I found a bunch of stale entries I need to remove from several packages!↩︎\nIn easy to use form as a Gist.↩︎"
  },
  {
    "objectID": "posts/2022-11-07-Announce-Subscribe/Announce-Subscribe.html",
    "href": "posts/2022-11-07-Announce-Subscribe/Announce-Subscribe.html",
    "title": "You Can Now Subscribe",
    "section": "",
    "text": "Just a short post to let readers know that you can now subscribe to this blog. Of course, you have always been able to get the RSS feed via the buttons in the navbar, but now you can submit your e-mail and my cheap-ass free level MailChimp account will let you know when there is a new post. You might find this useful if you are leaving Twitter or Twitter collapses completely!\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2022,\n  author = {Hanson, Bryan},\n  title = {You {Can} {Now} {Subscribe}},\n  date = {2022-11-07},\n  url = {http://chemospec.org/posts/2022-11-07-Announce-Subscribe/Announce-Subscribe.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2022. “You Can Now Subscribe.” November 7,\n2022. http://chemospec.org/posts/2022-11-07-Announce-Subscribe/Announce-Subscribe.html."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "",
    "text": "Last summer, a GSOC project was approved for work on the hyperSpec package which had grown quite large and hard to maintain.1 The essence of the project was to break the original hyperSpec package into smaller packages.2 As part of that project, we needed to be able to:\nIn this post I’ll describe how we used Dirk Eddelbuettel’s drat package and Github Actions to automate the deployment of packages between repositories."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#what-is-drat",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#what-is-drat",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "What is drat?",
    "text": "What is drat?\ndrat is a package that simplifies the creation and modification of CRAN-like repositories. The structure of a CRAN-like repository is officially described briefly here.3 Basically, there is required set of subdirectories, required files containing package metadata, and source packages that are the result of the usual build and check process. One can also have platform-specific binary packages. drat will create the directories and metadata for you, and provides utilities that will move packages to the correct location and update the corresponding metadata.4 The link above provides access to all sorts of documentation. My advice is to not overthink the concept. A repository is simply a directory structure and a couple of required metadata files, which must be kept in sync with the packages present. drat does the heavy-lifting for you."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#what-are-github-actions",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#what-are-github-actions",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "What are Github Actions?",
    "text": "What are Github Actions?\nGithub Actions are basically a series of tasks that one can have Github run when there is an “event” on a repo, like a push or pull. Github Actions are used extensively for continuous integrations tasks, but they are not limited to such use. Github Actions are written in a simply yaml-like script that is rather easy to follow even if the details are not familiar. Github Actions uses shell commands, but much of the time the shell simply calls Rscript to run native R functions. One can run tasks on various hardware and OS versions."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#the-package-repo",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#the-package-repo",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "The Package Repo",
    "text": "The Package Repo\nThe deployed packages reside on the gh-pages branch of r-hyperspec/pkg-repo in the form of the usual .tar.gz source archives, ready for users to install. One of the important features of this repo is the table of hosted packages displayed in the README. The table portion of README.md file is generated automatically whenever someone, or something, pushes to this repo. I include the notion that something might push because as you will see next, the deploy process will automatically push archives to this repo from the repo where they are created. The details of how this README.md is generated are in drat--update-readme.yaml. If you take a look, you’ll see that we use some shell-scripting to find any .tar.gz archives and create a markdown-ready table structure, which Github then automatically displays (as it does with all README.md files at the top level of a repo). The yaml file also contains a little drat action that will refresh the repo in case that someone manually removes an archive file by git operations. Currently we do not host binary packages at this repo, but that is certainly possible by extension of the methods used for the source packages."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#the-automatic-deploy-process",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#the-automatic-deploy-process",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "The Automatic Deploy Process",
    "text": "The Automatic Deploy Process\nThe automatic deploy process is used in several r-hyperSpec repos. I’ll use the chondro repo to illustrate the process. chondro is a simple package containing a &gt; 2 Mb data set. If the package is updated, the package is built and checked and then deployed automatically to r-hyperSpec/pkg-repo (described above). The magic is in drat--insert-package.yaml. The first part of this file does the standard build and check process.5 The second part takes care of deploying to r-hyperspec/pkg-repo. The basic steps are given next (study the file for the details). It is essential to keep in mind that each task in Github Actions starts from the same top level directory.6 Tasks are set off by the syntax - name: task description.\n\nConfigure access to Github. Note that we employ a Github user name and e-mail that will uniquely identify the repo that is pushing to r-hyperSpec/pkg-repo. This is helpful for troubleshooting.\nClone r-hyperSpec/pkg-repo into a temporary directory and checkout the gh-pages branch.\nSearch for any .tar.gz files in the check folder, which is where we directed Github Actions to carry out the build and check process (the first half of this workflow).7 Note that the argument full.names = TRUE is essential to getting the correct path. Use drat to insert the .tar.gz files into the cloned r-hyperSpec/pkg-repo temporary directory.\nMove to the temporary directory, then use git commands to send the updated r-hyperspec/pkg-repo branch back to its home, now with the new .tar.gz files included. Use a git commit message that will show where the new tar ball came from.\n\nThanks for reading. Let me know if you have any questions, via the comments, e-mail, etc."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#acknowledgements",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#acknowledgements",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis portion of the hyperSpec GSOC 2020 project was primarily the work of hyperSpec team members Erick Oduniyi, Bryan Hanson and Vilmantas Gegzna. Erick was supported by GSOC in summer 2020."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#footnotes",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#footnotes",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe work continues this summer, hopefully again with the support of GSOC.↩︎\nProject background and results.↩︎\nA more loquacious description that may be slightly dated is here.↩︎\ndrat is using existing R functions, mainly from the tools package. They are just organized and presented from the perspective of a user who wants to create a repo.↩︎\nModified from the recipes here.↩︎\nThe toughest part of writing this workflow was knowing where one was in the directory tree of the Github Actions workspace. We made liberal use of getwd(), list.files() and related functions during troubleshooting. All of these “helps” have been removed from the mature version of the workflow. As noted in the workflow, the top directory is /home/runner/work/${{ REPOSITORY_NAME }}/${{ REPOSITORY_NAME }}.↩︎\nIt’s helpful to understand in a general way what happens during the build and check process (e.g. the directories and files created).↩︎"
  },
  {
    "objectID": "posts/2022-05-01-Protocol-Pt3/2022-05-01-Protocol-Pt3.html",
    "href": "posts/2022-05-01-Protocol-Pt3/2022-05-01-Protocol-Pt3.html",
    "title": "Metabolic Phenotyping Protocol Part 3",
    "section": "",
    "text": "Part 1 of this series is here.\nPart 2 of this series is here.\nIf you aren’t familiar with ChemoSpec, you might wish to look at the introductory vignette first.\nIn this series of posts we are following the protocol as described in the printed publication closely (Blaise et al. 2021). The authors have also provided a Jupyter notebook. This is well worth your time, even if Python is not your preferred language, as there are additional examples and discussion for study."
  },
  {
    "objectID": "posts/2022-05-01-Protocol-Pt3/2022-05-01-Protocol-Pt3.html#supervised-analysis-with-pls-da",
    "href": "posts/2022-05-01-Protocol-Pt3/2022-05-01-Protocol-Pt3.html#supervised-analysis-with-pls-da",
    "title": "Metabolic Phenotyping Protocol Part 3",
    "section": "Supervised Analysis with PLS-DA",
    "text": "Supervised Analysis with PLS-DA\nChemoSpec carries out exploratory data analysis, which is an unsupervised process. The next step in the protocol is PLS-DA (partial least squares - discriminant analysis). I have written about ChemoSpec + PLS here if you would like more background on plain PLS. However, PLS-DA is a technique that combines data reduction/variable selection along with classification. We’ll need the mixOmics package (F et al. (2017)) package for this analysis; note that loading it replaces the plotLoadings function from ChemoSpec.\n\nlibrary(\"mixOmics\")\n\nLoading required package: MASS\n\n\nLoading required package: lattice\n\n\n\nLoaded mixOmics 6.20.0\nThank you for using mixOmics!\nTutorials: http://mixomics.org\nBookdown vignette: https://mixomicsteam.github.io/Bookdown\nQuestions, issues: Follow the prompts at http://mixomics.org/contact-us\nCite us:  citation('mixOmics')\n\n\n\nAttaching package: 'mixOmics'\n\n\nThe following object is masked from 'package:ChemoSpec':\n\n    plotLoadings\n\n\nFigure 6 shows the score plot; the results suggest that classification and modeling may be successful. The splsda function carries out a single sparse computation. One computation should not be considered the ideal answer; a better approach is to use cross-validation, for instance the bootsPLS function in the bootsPLS package (Rohart, Le Cao, and Wells (2018) which uses splsda under the hood). However, that computation is too time-consuming to demonstrate here.\n\nX &lt;- Worms2$data\nY &lt;- Worms2$groups\nsplsda &lt;- splsda(X, Y, ncomp = 8)\n\n\nplotIndiv(splsda,\n  col.per.group = c(\"#FB0D16FF\", \"#FFC0CBFF\", \"#511CFCFF\", \"#2E94E9FF\"),\n  title = \"sPLS-DA Score Plot\", legend = TRUE, ellipse = TRUE)\n\n\n\n\n\n\n\nFigure 6: sPLS-DA plot showing classification.\n\n\n\n\n\nTo estimate the number of components needed, the perf function can be used. The results are in Figure 7 and suggest that five components are sufficient to describe the data.\n\nperf.splsda &lt;- perf(splsda, folds = 5, nrepeat = 5)\nplot(perf.splsda)\n\n\n\n\n\n\n\nFigure 7: Evaluation of the PLS-DA performance.\n\n\n\n\n\nAt this point, we have several ideas of how to proceed. Going forward, one might choose to focus on accurate classification, or on determining which frequencies should be included in a predictive model. Any model will need to refined and more details extracted. The reader is referred to the case study from the mixOmics folks which covers these tasks and explains the process.\n\nThis post was created using ChemoSpec version 6.1.3 and ChemoSpecUtils version 1.0.0."
  },
  {
    "objectID": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html",
    "href": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html",
    "title": "Metabolic Phenotyping Protocol Part 2",
    "section": "",
    "text": "Part 1 of this series is here.\nIf you aren’t familiar with ChemoSpec, you might wish to look at the introductory vignette first.\nIn this series of posts we are following the protocol as described in the printed publication closely (Blaise et al. 2021). The authors have also provided a Jupyter notebook. This is well worth your time, even if Python is not your preferred lanaguage, as there are additional examples and discussion for study."
  },
  {
    "objectID": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html#normalization-scaling",
    "href": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html#normalization-scaling",
    "title": "Metabolic Phenotyping Protocol Part 2",
    "section": "Normalization & Scaling",
    "text": "Normalization & Scaling\nApply PQN normalization; scaling in ChemoSpec is applied at the PCA stage (next).\n\nWorms &lt;- normSpectra(Worms)  # PQN is the default"
  },
  {
    "objectID": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html#pca",
    "href": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html#pca",
    "title": "Metabolic Phenotyping Protocol Part 2",
    "section": "PCA",
    "text": "PCA\nConduct classical PCA using autoscaling.1 Note that ChemoSpec includes several different variants of PCA, each with scaling options. See the introductory vignette for more details. For more about what PCA is and how it works, please see the LearnPCA package.\n\nc_pca &lt;- c_pcaSpectra(Worms, choice = \"autoscale\")  # no scaling is the default\n\n\nComponents to Retain\nA key question at this stage is how many components are needed to describe the data set. Keep in mind that this depends on the choice of scaling. Figure 1 and Figure 2 are two different types of scree plots, which show the residual variance. This is the R2x value in the protocol (see protocol Figure 7a). Another approach to answering this question is to do a cross-validated PCA.2 The results are shown in Figure 3. These are the Q2x values in protocol Figure 7a. All of these ways of looking at the variance explained suggest that retaining three or possibly four PCs is adequate.\n\nplotScree(c_pca)\n\n\n\n\n\n\n\nFigure 1: Scree plot (recommended style).\n\n\n\n\n\n\nplotScree(c_pca, style = \"trad\")\n\n\n\n\n\n\n\nFigure 2: Scree plot (traditional style).\n\n\n\n\n\n\ncv_pcaSpectra(Worms, choice = \"autoscale\", pcs = 10)\n\n\n\n\n\n\n\nFigure 3: Scree plot using cross validation.\n\n\n\n\n\n\n\nScore Plots\nNext, examine the score plots (Figure 4, Figure 5). In these plots, each data point is colored by its group membership (keep in mind this is completely independent of the PCA calculation). In addition, robust confidence ellipses are shown for each group. Inspection of these plots is one way to identify potential outliers. The other use is of course to see if the sample classes separate, and by how much.\nExamination of these plots shows that separation by classes has not really been achieved using autoscaling. In Figure 4 we see four clear outlier candidates (samples 37, 101, 107, and 118). In Figure 5 we see some of these samples and should probably add sample 114 for a total of five candidates.\n\np &lt;- plotScores(Worms, c_pca, pcs = 1:2, ellipse = \"rob\", tol = 0.02)\np\n\n\n\n\n\n\n\nFigure 4: Score plot for PCs 1 and 2. Compare to protocol figure 7a.\n\n\n\n\n\n\np &lt;- plotScores(Worms, c_pca, pcs = 2:3, ellipse = \"rob\", leg.loc = \"topright\", tol = 0.02)\np\n\n\n\n\n\n\n\nFigure 5: Score plot for PCS 2 and 3.\n\n\n\n\n\nTo label more sample points, you can increase the value of the argument tol.\n\n\nOutliers\nThe protocol recommends plotting Hotelling’s T2 ellipse for the entire data set; this is not implemented in ChemoSpec but we can easily do it if we are using ggplot2 plots (which is the default in ChemoSpec). We need the ellipseCoord function from the HotellingsEllipse package.3\n\nsource(\"ellipseCoord.R\")\nxy_coord &lt;- ellipseCoord(as.data.frame(c_pca$x), pcx = 1, pcy = 2, conf.limit = 0.95,\n    pts = 500)\np &lt;- plotScores(Worms, c_pca, which = 1:2, ellipse = \"none\", tol = 0.02)\np &lt;- p + geom_path(data = xy_coord, aes(x = x, y = y)) + scale_color_manual(values = \"black\")\np\n\n\n\n\n\n\n\nFigure 6: Score plot for PCs 1 and 2 with Hotelling’s T2 ellipse. Compare to protocol figure 7a.\n\n\n\n\n\nWe can see many of the same outliers by this approach as we saw in Figure 4 and Figure 5.\nAnother way to identify outliers is to use the approach described in Varmuza and Filzmoser (2009) section 3.7.3. Figure 7 and Figure 8 give the plots. Please see Filzmoser for the details, but any samples that are above the plotted threshold line are candidate outliers, and any samples above the threshold in both plots should be looked at very carefully. Though we are using classical PCA, Filzmoser recommends using these plots with robust PCA. These plots are a better approach than “eye balling it” on the score plots.\n\np &lt;- pcaDiag(Worms, c_pca, plot = \"OD\")\np\n\n\n\n\n\n\n\nFigure 7: Orthogonal distance plot based on the first three PCs.\n\n\n\n\n\n\np &lt;- pcaDiag(Worms, c_pca, plot = \"SD\")\np\n\n\n\n\n\n\n\nFigure 8: Score distance plot based on the first three PCs.\n\n\n\n\n\nComparison of these plots suggest that samples 37, 101, 107, 114 and 118 are likely outliers. These spectra should be examined to see if the reason for their outlyingness can be deduced. If good reason can be found, they can be removed as follows.4\n\nWorms2 &lt;- removeSample(Worms, rem.sam = c(\"37_\", \"101_\", \"107_\", \"114_\", \"118_\"))\n\nAt this point one should repeat the PCA, score plots and diagnostic plots to get a good look at how removing these samples affected the results. Those tasks are left to the reader.\n\nWe will continue in the next post with a discussion of loadings.\n\nThis post was created using ChemoSpec version 6.1.3 and ChemoSpecUtils version 1.0.0."
  },
  {
    "objectID": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html#footnotes",
    "href": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html#footnotes",
    "title": "Metabolic Phenotyping Protocol Part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWithout scaling, the largest peaks will drive the separation in the scores plot.↩︎\nBe sure you have ChemoSpec 6.1.3 or higher, as cv_pcaSpectra had a bug in it! One benefit of writing these posts is finding lame bugs…↩︎\nWe are sourcing in a corrected version of the function, as the CRAN version has a small error in it.↩︎\nThe Jupyter notebook has details about this.↩︎"
  },
  {
    "objectID": "posts/2024-04-17-EF-NMR-Build-3/EF-NMR-Build-3.html",
    "href": "posts/2024-04-17-EF-NMR-Build-3/EF-NMR-Build-3.html",
    "title": "EF-NMR Part 3: Receiver Software",
    "section": "",
    "text": "Before We Start…\n\n\n\nNMRduino is maturing rapidly! If what I’m doing is at all interesting to you, and you don’t know about the NMRduino Project and their recent publication, be sure to check it out. It’s much more sophisticated than what’s going on here, and will be available soon.\nPart 1 Part 2"
  },
  {
    "objectID": "posts/2024-04-17-EF-NMR-Build-3/EF-NMR-Build-3.html#capturing-an-fid",
    "href": "posts/2024-04-17-EF-NMR-Build-3/EF-NMR-Build-3.html#capturing-an-fid",
    "title": "EF-NMR Part 3: Receiver Software",
    "section": "Capturing an “FID”",
    "text": "Capturing an “FID”\nIn the previous post on biwise operators in C I detailed some of the machinations needed to control the ADC on an Arduino. After considerable work, that knowledge has been put to use to develop a working receiver system (though more work will be needed to perfect it). In the process, I have fine-tuned the code needed to control the instrument and collect the data in a useable form.\nThe Bnmr software is available on Github. The hardware used for testing and development is shown in Figure 1.\n\n\n\n\n\n\nFigure 1: The hardware used for testing. The blue box on the left is the PicoScope, which is generating a sine wave simulating an FID. The Arduino is in the middle. The bread board on the right has an Adafruit micro SD card breakout board mounted at the bottom (at the top is a voltage divider which I use to generate a constant voltage for quicker testing; it’s not hooked up when using the PicoScope). Once data is collected, the micro SD card is removed from the breakout board and put in the dongle to move the data to the laptop.\n\n\n\nOnce the proper bits were set so the ADC would collect data, I first used a simple voltage divider to generate a constant ADC signal, adjustable via a potentiometer (if one doesn’t provide some kind of input, the ADC output drifts around). With a signal available, there were many rounds of code revision so that a specified number of data points could be collected and stored somewhere.1\nIn terms of storage, there were issues. The Arduino has very little actual memory, so the amount of data that can be “stored” is very small. As a result, this data has to be quickly moved somewhere else with significant memory. The solution to the transient data storage is a ring buffer. I was able to implement the code found on Wikipedia in C without too much trouble. The idea behind a ring or circular buffer is that data is stored in a fixed size buffer, and added and removed in a coordinated manner via indices. However, in the big picture data must be removed from the ring buffer as fast or faster than it is put in, otherwise data is overwritten. And, it turns out that the Arduino ADC can really pump out data. In order to keep the ring buffer from filling and overwriting (which is treated as an error), I had to collect data from the ADC at a lower rate that it can produce numbers, for instance every 10th reading.2\nThe second problem was what to do with the data that was emptied out of the ring buffer. I spent a lot of time trying to send it to the serial port, so I could capture it from there. However, Bnmr also sends a lot of messages about various events to the serial port. These messages inform the user about what is happening and also provide troubleshooting guidance. Ultimately, it was not possible to capture the data this way – the messages invariably introduced problems with the formatting of the data. The solution was to add a micro SD card breakout board to store the data on the fly, effectively separating the message stream from the data stream. Before I settled on that approach, I also tried to use R to both send messages and capture the data.3 In addition, I also tried using a shell script and a terminal emulator to do the same. Neither was completely successful when messages and data were mixed. However, the shell script experience proved helpful in developing the final, successful approach. Another problem with having both messages and data in the same serial stream was that MacOS has a nasty habit of reseting high baud rates desirable for data collection back to lower rates. This is discussed in various forums and workarounds exist, but I could not get the overall process to be reliable and robust."
  },
  {
    "objectID": "posts/2024-04-17-EF-NMR-Build-3/EF-NMR-Build-3.html#results",
    "href": "posts/2024-04-17-EF-NMR-Build-3/EF-NMR-Build-3.html#results",
    "title": "EF-NMR Part 3: Receiver Software",
    "section": "Results",
    "text": "Results\nWith functioning software and a method to control the overall acquistion process in hand, I used the PicoScope to generate a sine wave (Figure 2). Bnmr was compiled and uploaded via a shell script calling the arduino-cli (included in the repo, see Listing 1). Control was then transferred to picocom which is a terminal emulation program, and the start signal sent to the Arduino. Once the scans completed, the micro SD card was moved from the Arduino to a dongle connected to the laptop, and analyzed using R as shown later.\n\n\n\n\nListing 1: Shell Script to Send/Receive Messages & Data.\n\n\n#!/bin/bash\narduino-cli compile -b arduino:avr:uno $1\narduino-cli upload -p $2 -b arduino:avr:uno $1\npicocom $2 -b $3 -g $4\n\n# Typical Usage:\n# Set working directory to .../Bnmr\nls -1 /dev/cu.* # With Arduino plugged in, get the name of the port\n./go.sh Bnmr.ino /dev/cu.usbmodem101 9600 outfile.txt\n# $1 is the name of the Arduino code file\n# $2 is the port name\n# $3 is the baud rate for messages\n# $4 is the filename for messages received\n# ADC data is stored on a micro SD card with a filename given in user_input.h\n# cntrl-A central-X to quit the Picocom window\n\n\n\n\n\n\n\n\n\n\nFigure 2: The sine wave generated by the PicoScope which was used to represent an NMR FID signal. Peak-to-peak voltage was 500 mV, with a 1 V offset as the Arduino ADC can only process positive voltage readings. Frequency is 50 Hz.\n\n\n\n\nMessage Log File\nWhatever is typed in the picocom terminal/window is sent to the serial port and then to the Arduino. All messages sent by the Arduino are echoed in the picocom window and saved to a message log file. A typical output is in Listing 2.\n\n\n\n\nListing 2: Messages Captured by Picocom.\n\n\nLooking for SD card...\nSD card found & working\nSD card directory:\nSPOTLI~1/\nFID_CSV     0\nFSEVEN~5/\nTRASHE~9/\n \nBnmr listening...\nEnter g or s at any time # note: g is typed (but not echoed)\n                         # and sent to the Arduino, which starts the program\n=====================\nLoading experiment...\n\nStarting scans...\n    Scan no: 1\n    Scan no: 2\n    Scan no: 3\n    Scan no: 4\n    Scan no: 5\nScans complete!\nExperiment complete, stop\n\n\n\n\n\n\nData Log File\nThe data log file is a comma-separated file with an entire FID/scan on one long line. There is a blank line between each data line. This is stored on the micro SD card in a file whose name is provided by the user in user_input.h (this is where all user modifiable parameters are given). We can read in the first two scans and plot the early points as follows (Figure 3).\n\ndat &lt;- readLines(\"FID_CSV\")\nres1 &lt;- as.numeric(unlist(strsplit(dat[1], \", \")))\n# skipping dat[2] as it is a blank line\nres2 &lt;- as.numeric(unlist(strsplit(dat[3], \", \")))\n\nplot(x = 1:length(res1), y = res1, type = \"b\", xlim = c(1, 25),\nxlab = \"Index\", ylab = \"ADC Reading\")\nlines(x = 1:length(res1), y = res2, col = \"red\")\n\n\n\n\n\n\n\nFigure 3: Two typical “scans”.\n\n\n\n\n\nSince there is no coordination (i.e. no common time base) between the generated signal and the ADC data collection, the two sample scans are offset slightly. A common time base is very important for an NMR, so this will be one of the next items for focus."
  },
  {
    "objectID": "posts/2024-04-17-EF-NMR-Build-3/EF-NMR-Build-3.html#footnotes",
    "href": "posts/2024-04-17-EF-NMR-Build-3/EF-NMR-Build-3.html#footnotes",
    "title": "EF-NMR Part 3: Receiver Software",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRight now, a fixed number of data points are collected in whatever time it takes. This needs to be modified so that the data points are collected over a fixed amount of time, specified by the user.↩︎\nThere’s a potential problem here, and that is one must collect enough points to satisfy Nyquist’s criterion in order to faithfully represent a sine wave. Preliminary experiments suggest that there are plenty of data points because the ADC is extremely fast.↩︎\nI spent considerable time writing an R package which I named UtiliDuino for this purpose, but ultimately it was not the best solution.↩︎"
  },
  {
    "objectID": "posts/2025-02-06-Multiplets/Multiplets.html",
    "href": "posts/2025-02-06-Multiplets/Multiplets.html",
    "title": "Mastering Tough Multiplets",
    "section": "",
    "text": "Alright, time for a little confession. I have been a chemist for nearly 40 years, and used NMR spectroscopy extensively that entire time. Somehow I have never come across the method I am about to discuss, though it has been known since the early 1990’s. Perhaps that’s because the compounds I encountered did not have peaks requiring this approach. Perhaps it is because I could figure out the structures easily enough without worrying about “nasty” multiplet details. Perhaps it is because so often we are confirming an expected structure rather than working with a complete unknown.\nI’m talking about those multiplets we encounter that have some decent, symmetrical structure to them, but are too hard to work out exactly what is going on. Those “it almost looks like a quartet of doublets but not quite” peaks. Those “it should be a doublet of triplets but there are not enough peaks” cases.\nThe other day I saw an announcement about a 2nd edition of a book by the visually adept Roman A. Valiulin called NMR Multiplet Interpretation. With a catnip-title like that I ordered it immediately. I have not been disappointed. It’s taught me to look at those multiplet beasts completely differently.\nThis book teaches an interesting method, originated by Hoye’s lab at Minnesota, that allows one to work out the values of the coupling constants in a complex, first order multiplet without thinking about structure at all.1 Once you have these values, one can determine which type of multiplet is at hand. For instance, is it a doublet of triplets (dt) or a triplet of doublets (td)? Sure, if all the coupling constants are different and the resolution of the instrument is good enough, one can do this by inspection. But if the coupling constants are degenerate or almost so, or we are working with quartets and quintets, then one can end up with a rather complex pattern whose origin is not at all clear, which is when this method shines. With this information, one can then turn back to issues of structure.\nI am thinking of this as a “coupling constant forward”2 method, a way to work out those multiplets that in the past one might have been tempted to set aside as too complex to deal with.\nThis is clearly not a beginner topic in NMR. When we teach NMR to novices, we focus on using the three big pieces of information: chemical shift, area, and splitting, but of course the splitting is almost always very simple. Sometimes students get a taste of the real world when a sample has a doublet of doublets but the two central peaks come close to overlapping; they can often appreciate that with small changes in coupling constants this might look like a triplet. But at the beginning we may not even really talk much about coupling constants, and instead just talk about peak spacing and symmetry.\nOne interesting aspect of Valiulin’s presentation is he makes very clear how every multiplet is a “doublet of doublets of … of doublets” in this approach. In a J-tree diagram every splitting is a doublet until you’ve got all the coupling constants and have determined that some of them are the same, leading to overlaps in the final pattern of peaks. Using the example from the previous paragraph, a doublet of doublets becomes a triplet once the two coupling constants are the same. This sort of collapsing of peaks is surely appreciated by anyone beyond the novice level, but Valiulin provides some nice demonstrations of how this works when many more spins are involved.\nAny of you multiplet junkies (or more so, multiplet avoiders) that haven’t seen this book should definitely get a hold of it."
  },
  {
    "objectID": "posts/2025-02-06-Multiplets/Multiplets.html#footnotes",
    "href": "posts/2025-02-06-Multiplets/Multiplets.html#footnotes",
    "title": "Mastering Tough Multiplets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is no n + 1 rule in play in this approach.↩︎\nI’m coining this term here, I think it’s the right way to think about it, feel free to comment.↩︎"
  },
  {
    "objectID": "posts/2020-01-01-Intro-F4S/2021-01-01-Intro-F4S.html",
    "href": "posts/2020-01-01-Intro-F4S/2021-01-01-Intro-F4S.html",
    "title": "FOSS for Spectroscopy",
    "section": "",
    "text": "For this inaugural blog post, I’m pleased to share a project I have been working on recently: FOSS4Spectroscopy is an attempt to catalog FOSS spectroscopy software. The page is designed to be updated easily with new or edited entries – see the page for details, and please contribute!\nFor this initial version, I searched the CRAN ecosystem via packagefinder and the Python world via PyPi.org using spectroscopy-related keywords. My expertise is in R so I’m pretty confident I have most of the R packages included. I’m not so confident about coverage of the Python packages (where else should I look?).\nCurrently, the “status” column in the main table is empty. I am working on a version which will update the status with a date giving some indication of the age and activity of the repository. Right now the page is updated when I push to Github, but in the long run I hope to get Travis-CI to run it as a weekly cron job.\nI welcome your feedback in any form!\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2020,\n  author = {Hanson, Bryan},\n  title = {FOSS for {Spectroscopy}},\n  date = {2020-01-01},\n  url = {http://chemospec.org/posts/2020-01-01-Intro-F4S/2021-01-01-Intro-F4S.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2020. “FOSS for Spectroscopy.” January 1,\n2020. http://chemospec.org/posts/2020-01-01-Intro-F4S/2021-01-01-Intro-F4S.html."
  },
  {
    "objectID": "posts/2022-09-10-Linear-Alg-Notes-Pt3/Linear-Alg-Notes-Pt3.html",
    "href": "posts/2022-09-10-Linear-Alg-Notes-Pt3/Linear-Alg-Notes-Pt3.html",
    "title": "Notes on Linear Algebra Part 3",
    "section": "",
    "text": "Series: Part 1 Part 2\nUpdate 19 September 2022: in “Use of outer() for Matrix Multiplication”, corrected use of “cross” to be “outer” and added example in R. Also added links to work by Hiranabe.\nThis post is a survey of the linear algebra-related functions from base R. Some of these I’ve disccused in other posts and some I may discuss in the future, but this post is primarily an inventory: these are the key tools we have available. “Notes” in the table are taken from the help files.\nMatrices, including row and column vectors, will be shown in bold e.g. \\(\\mathbf{A}\\) or \\(\\mathbf{a}\\) while scalars and variables will be shown in script, e.g. \\(n\\). R code will appear like x &lt;- y.\nIn the table, \\(\\mathbf{R}\\) or \\(\\mathbf{U}\\) is an upper/right triangular matrix. \\(\\mathbf{L}\\) is a lower/left triangular matrix (triangular matrices are square). \\(\\mathbf{A}\\) is a generic matrix of dimensions \\(m \\times n\\). \\(\\mathbf{M}\\) is a square matrix of dimensions \\(n \\times n\\).\nOne thing to notice is that there is no LU decomposition in base R. It is apparently used “under the hood” in solve() and there are versions available in contributed packages.2"
  },
  {
    "objectID": "posts/2022-09-10-Linear-Alg-Notes-Pt3/Linear-Alg-Notes-Pt3.html#footnotes",
    "href": "posts/2022-09-10-Linear-Alg-Notes-Pt3/Linear-Alg-Notes-Pt3.html#footnotes",
    "title": "Notes on Linear Algebra Part 3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor details see the discussion in Part 1.↩︎\nDiscussed in this Stackoverflow question, which also has an implementation.↩︎\nIn fact, for the default outer(), FUN = \"*\", outer() actually calls tcrossprod().↩︎"
  },
  {
    "objectID": "posts/2020-04-25-Heatmaps/2020-04-25-Heatmaps.html",
    "href": "posts/2020-04-25-Heatmaps/2020-04-25-Heatmaps.html",
    "title": "Spectral Heatmaps",
    "section": "",
    "text": "Most everyone is familiar with heatmaps in a general way. It’s hard not to run into them. Let’s consider some variations:\nThese three types of plots are conceptually unified in that they require a 3D data set. In the case of the heatmap and the image, the underlying data are on a regular x, y grid of values; mathematically, a matrix. The row and column indices are mapped to the x, y values, and the matrix entries are the z values. A chloropleth can be thought of as a very warped matrix where the cells are not on a regular grid but instead a series of arbitrary connected paths, namely the geographic boundaries. There is a value inside each connected path (the z value), but naturally the specification of the paths requires a completely different data structure. An intermediate type would be the cartogram heatmap described by Wilke."
  },
  {
    "objectID": "posts/2020-04-25-Heatmaps/2020-04-25-Heatmaps.html#footnotes",
    "href": "posts/2020-04-25-Heatmaps/2020-04-25-Heatmaps.html#footnotes",
    "title": "Spectral Heatmaps",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOther functions in ChemoSpec that can help you explore which frequencies are important are plotLoadings, plot2Loadings and sPlotSpectra.↩︎"
  },
  {
    "objectID": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html",
    "href": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html",
    "title": "Earth’s Field NMR",
    "section": "",
    "text": "In EF-NMR the line widths are extremely narrow but there is no chemical shift dispersion.\nWe can observe heteronuclear couplings in EF-NMR as they are field-invariant.\nIn EF-NMR the population of the two energy states is essentially equal, eliminating any signal. We can get around this with pre-polarization.\nThe resonance frequency of \\(\\ce{^{1}H}\\) NMR in EF is in the audio range, greatly simplifying the electronics.\n\n\nLet’s take a closer look from first principles what kinds of information one can glean from EF-NMR. We’ll restrict our discussion to spin \\(\\frac{1}{2}\\) nuclei with ~100% abundance, like \\(\\ce{^{1}H}\\), \\(\\ce{^{31}P}\\) or \\(\\ce{^{19}F}\\) – you’ll see why soon enough. Table 1 gives some relevant physical parameters for these nuclei.\n\n\n\nTable 1: Important NMR parameters for ~100% abundant spin \\(\\frac{1}{2}\\) nuclei. The units of \\(\\gamma\\) are \\(10^7\\) rad \\(T^{-1}\\) \\(s^{-1}\\). Larmor frequency is relative in MHz.\n\n\n\n\n\nNuclei\nGyromagnetic ratio \\(\\gamma\\)\nLarmor Freq.\n\n\n\n\n\\(\\ce{^{1}H}\\)\n26.7522\n100\n\n\n\\(\\ce{^{19}F}\\)\n25.1815\n94\n\n\n\\(\\ce{^{31}P}\\)\n10.8394\n40.5\n\n\n\n\n\n\nExcellent general references on NMR theory are Friebolin (Friebolin 2011) and Claridge (Claridge 2016)."
  },
  {
    "objectID": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#tldr",
    "href": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#tldr",
    "title": "Earth’s Field NMR",
    "section": "",
    "text": "In EF-NMR the line widths are extremely narrow but there is no chemical shift dispersion.\nWe can observe heteronuclear couplings in EF-NMR as they are field-invariant.\nIn EF-NMR the population of the two energy states is essentially equal, eliminating any signal. We can get around this with pre-polarization.\nThe resonance frequency of \\(\\ce{^{1}H}\\) NMR in EF is in the audio range, greatly simplifying the electronics.\n\n\nLet’s take a closer look from first principles what kinds of information one can glean from EF-NMR. We’ll restrict our discussion to spin \\(\\frac{1}{2}\\) nuclei with ~100% abundance, like \\(\\ce{^{1}H}\\), \\(\\ce{^{31}P}\\) or \\(\\ce{^{19}F}\\) – you’ll see why soon enough. Table 1 gives some relevant physical parameters for these nuclei.\n\n\n\nTable 1: Important NMR parameters for ~100% abundant spin \\(\\frac{1}{2}\\) nuclei. The units of \\(\\gamma\\) are \\(10^7\\) rad \\(T^{-1}\\) \\(s^{-1}\\). Larmor frequency is relative in MHz.\n\n\n\n\n\nNuclei\nGyromagnetic ratio \\(\\gamma\\)\nLarmor Freq.\n\n\n\n\n\\(\\ce{^{1}H}\\)\n26.7522\n100\n\n\n\\(\\ce{^{19}F}\\)\n25.1815\n94\n\n\n\\(\\ce{^{31}P}\\)\n10.8394\n40.5\n\n\n\n\n\n\nExcellent general references on NMR theory are Friebolin (Friebolin 2011) and Claridge (Claridge 2016)."
  },
  {
    "objectID": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#line-widths-are-very-narrow",
    "href": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#line-widths-are-very-narrow",
    "title": "Earth’s Field NMR",
    "section": "Line Widths are Very Narrow",
    "text": "Line Widths are Very Narrow\nThe line width of an NMR signal is primarily dependent on the homogeneity of the \\(B_o\\) field, which in the case of earth’s field is very good. Appelt et al. (2006) state that when observations are made &gt;100 meters from buildings and ferrous structures1 the homogeneity of the earth’s magnetic field for small sample volumes is in the range of \\(\\Delta B/B &lt; 10^{-6}\\). They further state that when \\(T_1 \\sim T_2 &gt; 3\\) seconds line widths will be less than 0.1 Hz.2 This all sounds very promising: narrow lines imply good separation between peaks."
  },
  {
    "objectID": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#no-chemical-shift-information",
    "href": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#no-chemical-shift-information",
    "title": "Earth’s Field NMR",
    "section": "No Chemical Shift Information",
    "text": "No Chemical Shift Information\nOne of the characteristics of high-field NMR which makes it so useful is the dispersion of chemical shifts as a function of structure. Unfortunately, EF-NMR has effectively zero chemical shift dispersion. The equation for computing chemical shift, \\(\\delta\\), is:\n\\[\n\\delta = \\frac{\\nu_{sample} - \\nu_{reference}}{\\nu_{B_o}}\n\\]\nwhere the units are:\n\\[\nppm = \\frac{Hz}{MHz}\n\\]\nsince \\(\\delta\\) is a field strength independent quantity. Taking \\(\\nu_{reference}\\) to be zero, e.g. TMS added to the sample, we can rearrange the equation to get \\(\\nu_{sample}\\). Consider the compound \\(\\ce{CH3Br}\\) whose methyl group has a chemical shift of 2.63 ppm. Using an earth’s field \\(\\ce{^{1}H}\\) Larmor frequency of 19.1 KHz, we can compute the shift of \\(\\ce{CH3Br}\\) in Hz as 0.0191 Hz. This is an extremely small value, smaller than the typical line width in earth’s field (so the promise of narrow line widths is not going to save us).\nFor further comparison, we can do the same calculation for \\(\\ce{CH2Br2}\\) which has a shift of 4.90 ppm. The result is exactly the same, 0.0191 Hz. We can see that these two compounds with differing numbers of halogens, which would be trivial to distinguish with a low field bench-top instrument operating at 80 MHz, are indistinguishable in earth’s field. This is due to the very small value of earth’s magnetic field."
  },
  {
    "objectID": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#heteronuclear-couplings",
    "href": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#heteronuclear-couplings",
    "title": "Earth’s Field NMR",
    "section": "Heteronuclear Couplings",
    "text": "Heteronuclear Couplings\nWhile the chemical shift dispersion in earth’s field is clearly nil, heteronuclear J couplings are readily observed due to their greater magnitude, up to about 200 Hz. Appelt et al. (2006) gives a number of interesting examples involving \\(\\ce{^{1}H}\\), \\(\\ce{^{19}F}\\) and \\(\\ce{^{29}Si}\\) containing compounds."
  },
  {
    "objectID": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#populations-of-quantum-states",
    "href": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#populations-of-quantum-states",
    "title": "Earth’s Field NMR",
    "section": "Populations of Quantum States",
    "text": "Populations of Quantum States\nBasic NMR theory tells us that the energy difference between the two quantum states for a spin \\(\\frac{1}{2}\\) nucleus is proportional to the field strength \\(B_o\\):\n\\[\n\\Delta E = \\gamma \\hbar B_o\n\\]\nwhere \\(\\hbar\\) is \\(\\frac{h}{2\\pi}\\). A plot for \\(\\ce{^{1}H}\\) is shown in Figure 1; the right-most point corresponds to a 1,000 MHz instrument. Clearly as \\(B_o\\) goes to zero the \\(\\Delta E\\) goes to zero in a simple linear fashion.\n\n\n\n\n\n\n\n\nFigure 1: \\(\\Delta\\)E as a function of field strength \\(B_o\\)\n\n\n\n\n\nWe can then relate the number of nuclei in the upper energy state, \\(N_{\\beta}\\), to that in the lower energy state, \\(N_{\\alpha}\\), at thermal equilibrium as:\n\\[\n\\frac{N_{\\beta}}{N_{\\alpha}} = e^{- \\Delta E/kT} \\approx 1 - \\frac{\\Delta E}{kT} = 1- \\frac{\\gamma \\hbar B_o}{kT}\n\\]\nwhere \\(k\\) is the Boltzman constant and \\(T\\) is the temperature in Kelvin. The ratio of population states is nearly equal for any value of \\(B_o\\) but of course gets even worse as \\(B_o\\) decreases. This is the reason for the low overall sensitivity of NMR as an analytical technique. We can compute the ratio for \\(\\ce{^{1}H}\\) at room temperature; we’ll compare the value for earth’s field to those of a 100 and 1,000 MHz instruments:\n\n\n      45 uT (Earth)    2.35 T (100 MHz) 23.49 T (1,000 MHz) \n     1.000000000000      0.999999999998      0.999999999982 \n\n\nAs you can see, in earth’s field there is basically no difference in the two population states, meaning there is no signal to observe. Clearly a problem!\nIf all the nuclei were in \\(N_{\\alpha}\\) we could measure the energy required to bump them up to \\(N_{\\beta}\\), or more commonly, bump them up and then watch the energy given off as equilibrium returns. Unfortunately, the signal produced is proportional to \\(N_{\\alpha} - N_{\\beta}\\), which is effectively zero in earth’s field. At the same time however, the more spins we have, the higher the signal will be. More spins total in the detection coil sweet spot will be helpful, but there are other factors mitigating against making large coils to accommodate large samples. One way around this is to use signal averaging."
  },
  {
    "objectID": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#pre-polarization",
    "href": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#pre-polarization",
    "title": "Earth’s Field NMR",
    "section": "Pre-Polarization",
    "text": "Pre-Polarization\nIn the case of earth’s field NMR, the usual way around this problem of very limited signal is to pre-polarize the sample.3 This basically involves subjecting the sample to a fairly high magnetic field for a brief period before measuring the any signals. This pre-polarization field forces more of the \\(N_{\\beta}\\) nuclei to assume the lower energy \\(N_{\\alpha}\\) state, thus increasing \\(N_{\\alpha} - N_{\\beta}\\) which means there is a signal to be observed. Mohorič has an excellent but technical discussion of the details of this process (Mohorič and Stepišnick 2009)."
  },
  {
    "objectID": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#ef-nmr-signals-are-in-the-audio-range",
    "href": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#ef-nmr-signals-are-in-the-audio-range",
    "title": "Earth’s Field NMR",
    "section": "EF-NMR Signals are in the Audio Range",
    "text": "EF-NMR Signals are in the Audio Range\nWhat is the Larmor (resonance) frequency in earth’s field? Earth’s magnetic field varies from about 25 to 65 \\(\\mu\\)T; we’ll use an intermediate value of 45 \\(\\mu\\)T for our calculations. The Larmor frequency is given by the equation:\n\\[\n\\nu_{L} = \\lvert\\frac{\\gamma}{2\\pi}\\rvert B_o\n\\]\nNotice there is a simple linear relation between \\(\\nu_{L}\\) and \\(B_o\\).4 If we plug in values for our nuclei we get the following values in Hz:\n\n\n       1H       19F       31P \n19159.852 18034.921  7763.148 \n\n\nWhat we have shown here is that for EF-NMR, resonance frequencies are in the audio (20 - 20,000 Hz) and lower radio (20,000 Hz +) frequency range. Why is this important? It greatly simplifies signal detection because audio receivers are essentially radios, and the electronics for working in this frequency range are extremely well worked out, and not expensive to buy or build."
  },
  {
    "objectID": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#historical-note",
    "href": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#historical-note",
    "title": "Earth’s Field NMR",
    "section": "Historical Note",
    "text": "Historical Note\nThe first earth’s field NMR experiment was apparently conducted by Martin Packard and Russell Varian while at Varian Associates (Packard and Varian 1954). Varian Associates was of course a major instrument player, including NMR, and for a long time marketed their instruments largely toward colleges. 5"
  },
  {
    "objectID": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#footnotes",
    "href": "posts/2023-07-19-EF-NMR-1/EF-NMR-1.html#footnotes",
    "title": "Earth’s Field NMR",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKeep in mind that buried utilities made of iron or carrying electrical current can interfere.↩︎\n\\(T_1\\) is the relaxation time for magnetization aligned with the \\(B_o\\) axis, which corresponds to the \\(z\\) axis. This is the relaxation time that affects the ability to pulse quickly. It’s also called the spin-lattice relaxation time. \\(T_2\\) is the relaxation time corresponding to magnetization in the \\(x,y\\) plane, and is also known as the spin-spin relaxation time. \\(T_2\\) is largely determined by magnetic field inhomogeneity and the line width at half peak height is \\(\\Delta \\nu_{1/2} = \\gamma \\Delta B_o /2\\). \\(T_1 \\ge T_2\\). See Friebolin chapter 7 for a detailed discussion.↩︎\nIn fact pre-polarizing or polarizing the sample is now en-vogue for higher field instruments as well, in the form of DNP, SABRE etc.↩︎\nThe gyromagnetic ratio can be negative, hence the absolute value is taken here.↩︎\nMartin Packard is apparently unrelated to David Packard, one of the founders of HP.↩︎"
  },
  {
    "objectID": "posts/2020-02-20-NMR-Align-Pt1/2020-02-20-NMR-Align-Pt1.html",
    "href": "posts/2020-02-20-NMR-Align-Pt1/2020-02-20-NMR-Align-Pt1.html",
    "title": "Aligning 2D NMR Spectra Part 1",
    "section": "",
    "text": "In this series of posts, I’ll discuss the alignment process for the case of 2D NMR, as implemented in the package ChemoSpec2D. This is Part 1. Part 2. Part 3.\n\nIn one-dimensional \\(^1\\)H NMR spectroscopy, particularly biomolecular NMR, it is frequently necessary to align spectra before chemometric or metabolomics analysis. Poor alignment arises largely from pH and ionic strength induced shifts in aqueous samples. There are a number of published alignment algorithms for the one-dimensional case. The same issue presumably exists for 2D NMR spectra, but alignment options are limited. Instead, for 2D NMR people often work with tables of peaks. Creating these tables is an extra step and decisions about what to include may leave useful information behind.\nNo doubt you’ve compared two spectra by overlaying them on the screen, or printing them out, placing them on top of each other, and holding them up to the light. Conceptually, one can “align” spectra by a similar method: just slide one of the pieces of paper up/down and left/right until the spectra are optimally aligned. But how would one do this algorithmically? A literature searched turned up only a few publications on this topic. Among these, there was only one that I felt I could implement using the description in the paper: the HATS-PR algorithm of Robinette et al. (Robinette et al. 2011).\nWe’ll discuss the HATS algorithm in a future post. As a first step however, we need to consider how we know when two spectra are properly aligned. Visual inspection won’t work, as we will encounter cases where peaks in one region align, but only at the expense of peaks in another region. How would we rank such cases? To automate this process, we need to use an objective function, basically some kind of equation, that we evaluate as we explore the alignment space. A simple but effective option is to compute the distance between the two spectra. This is done by concatenating each row of the 2D spectra to give a long vector of intensities. The distance between these vectors can then be computed using any of the standard distance definitions. Let’s illustrate, starting by taking a look at some mis-aligned data. ChemoSpec2D contains a mis-aligned data set, MUD2, for just this purpose. Here are two spectra from MUD2; note we are using the new convenience functions LofC and LofL to make it easy to overlay the spectra.\n\nlibrary(\"ChemoSpec2D\")\n\nLoading required package: ChemoSpecUtils\n\n\n\nAs of version 6, ChemoSpec2D offers new graphics output options\n\n\n\nFunctions plotScores and plotScree will work with the new options\n\n\nFor details, please see ?GraphicsOptions\n\n\n\nThe ChemoSpec graphics option is set to 'ggplot2'\n\n\nTo change it, do\n    options(ChemoSpecGraphics = 'option'),\n    where 'option' is one of 'base' or 'ggplot2' or'plotly'.\n\ndata(MUD2)\nmylvls &lt;- seq(0, 30, length.out = 10)\nplotSpectra2D(MUD2, which = c(1, 6), showGrid = TRUE,\n  lvls = LofL(mylvls, 2),\n  cols = LofC(c(\"red\", \"black\"), 2, length(mylvls), 2),\n  main = \"MUD2 Spectra 1 & 6\")\n\n\n\n\n\n\n\n\nThe function sampleDist allows us to compute the distance between every pair of spectra in the MUD2 data set, and present the results as a heat map. Here are the results using cosine as the distance measure.\n\ncos_dist &lt;- sampleDist(MUD2, method = \"cosine\",\n  main = \"Cosine Distance\")\n\n\n\n\n\n\n\n\nThe actual numerical values are in cos_dist, a matrix. Looking at the heatmap, there are some modest differences among the spectra. However, if one looks at the scale, cosine distances are only defined on [-1 … 1]. While the cosine distance is popular in many spectroscopic contexts, it’s not the best objective function for our purpose because there is little absolute difference between -1 and 1 (and for MUD2 the absolute differences are even smaller, as the range is only 0, 0.99). This limited range affects the alignment process in a subtle way that we won’t cover here (alignment is still successful, however).\nLet’s consider instead the Euclidean distance.\n\neu_dist &lt;- sampleDist(MUD2, method = \"euclidean\",\n  main = \"Euclidean Distance\")\n\n\n\n\n\n\n\n\nIt turns of that the Euclidean distance gives a wider span of distances, which will serve us well in the next steps. Here, the range is roughly 80, 150. Note that in this plot the distance between identical spectra is zero, plotted as a white squares along the diagonal. When we used cosine as the distance, identical spectra were perfectly correlated and hence the diagonal in that plot was red.\nIn the next post I’ll discuss the general flow of the HATS algorithm, and how to carry it out using ChemoSpec2D.\n\n\n\n\nReferences\n\nRobinette, Steven L., Ramadan Ajredini, Hasan Rasheed, Abdulrahman Zeinomar, Frank C. Schroeder, Aaron T. Dossey, and Arthur S. Edison. 2011. “Hierarchical Alignment and Full Resolution Pattern Recognition of 2D NMR Spectra: Application to Nematode Chemical Ecology.” Analytical Chemistry 83 (5): 1649–57. https://doi.org/10.1021/ac102724x.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2020,\n  author = {Hanson, Bryan},\n  title = {Aligning {2D} {NMR} {Spectra} {Part} 1},\n  date = {2020-02-20},\n  url = {http://chemospec.org/posts/2020-02-20-NMR-Align-Pt1/2020-02-20-NMR-Align-Pt1.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2020. “Aligning 2D NMR Spectra Part 1.”\nFebruary 20, 2020. http://chemospec.org/posts/2020-02-20-NMR-Align-Pt1/2020-02-20-NMR-Align-Pt1.html."
  },
  {
    "objectID": "posts/2020-04-27-CSU-update/2020-04-27-CSU-update.html",
    "href": "posts/2020-04-27-CSU-update/2020-04-27-CSU-update.html",
    "title": "ChemoSpecUtils Update",
    "section": "",
    "text": "ChemoSpecUtils, a package that supports the common needs of ChemoSpec and ChemoSpec2D, has been updated to fix an unfortunate distance calculation error in version 0.4.38, released in January of this year. From the NEWS file for version 0.4.51:\n\nFunction rowDist, which supports a number of functions, was overhauled to address confusion in the documentation, and in my head, about distances vs. similarities. Also, different definitions found in the literature were documented more clearly. The Minkowski distance option was removed (ask if you want it back), code was cleaned up, documentation greatly improved, an example was added and unit tests were added. Plot scales were also corrected as necessary. Depending upon which distance option is chosen, this change affects hcaSpectra, plotSpectraDist, sampleDist and hcaScores in package ChemoSpec as well as hats_alignSpectra2D and hcaScores in package ChemoSpec2D.\n\nThis brings to mind a Karl Broman quote I think about frequently:\n\n“Open source means everyone can see my stupid mistakes. Version control means everyone can see every stupid mistake I’ve ever made.”\n– Karl Broman\n\nKarl Broman quote source\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2020,\n  author = {Hanson, Bryan},\n  title = {ChemoSpecUtils {Update}},\n  date = {2020-04-27},\n  url = {http://chemospec.org/posts/2020-04-27-CSU-update/2020-04-27-CSU-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2020. “ChemoSpecUtils Update.” April 27,\n2020. http://chemospec.org/posts/2020-04-27-CSU-update/2020-04-27-CSU-update.html."
  },
  {
    "objectID": "posts/2022-02-18-Key-References/2022-02-18-Key-References.html",
    "href": "posts/2022-02-18-Key-References/2022-02-18-Key-References.html",
    "title": "Chemometrics in Spectroscopy: Key References",
    "section": "",
    "text": "Jerome Workman Jr. and Howard Mark have published a very useful series of articles in Spectroscopy Online, summarizing common chemometric methods in spectroscopy, and giving the key publications on each. These are a great resource if one is learning about a technique, or if one wants to check out the fundamentals of a method you think you already know. Posting here for convenience!\nProtip: These pages load slowly in some browsers. I had the best luck with Chrome. Try the reader view for a user-friendly version that prints well (if you are in to that).\n\nSurvey of Chemometric Methods in Spectroscopy\nKey References Part 1\nKey References Part 2\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2022,\n  author = {Hanson, Bryan},\n  title = {Chemometrics in {Spectroscopy:} {Key} {References}},\n  date = {2022-02-18},\n  url = {http://chemospec.org/posts/2022-02-18-Key-References/2022-02-18-Key-References.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2022. “Chemometrics in Spectroscopy: Key\nReferences.” February 18, 2022. http://chemospec.org/posts/2022-02-18-Key-References/2022-02-18-Key-References.html."
  },
  {
    "objectID": "posts/2022-05-03-LearnPCA-Intro/2022-05-03-LearnPCA-Intro.html",
    "href": "posts/2022-05-03-LearnPCA-Intro/2022-05-03-LearnPCA-Intro.html",
    "title": "Introducing LearnPCA",
    "section": "",
    "text": "PCA, or principal components analysis, is one of the most wide-spread statistical methods in use. It shows up in many disciplines, from political science and psychology, to chemistry and biology. PCA is also really challenging to understand.\nI’m pleased to announce that my colleague David Harvey and I have recently released LearnPCA, an R package to help people with understanding PCA. In LearnPCA we’ve tried to integrate our years of experience teaching the topic, along with the best insights we can find in books, tutorials and the nooks and crannies of the internet. Though our experience is in a chemometrics context, we use examples from different disciplines so that the package will be broadly helpful.\nThe package contains seven vignettes that proceed from the conceptual basics to advanced topics. As of version 0.2.0, there is also a Shiny app to help visualize the process of finding the principal component axes. The current vignettes are:\n\nA Guide to the LearnPCA Package\nA Conceptual Introduction to PCA\nStep-by-Step PCA\nUnderstanding Scores and Loadings\nVisualizing PCA in 3D\nThe Math Behind PCA\nA Comparison of Functions for PCA\n\nYou can access the vignettes at the Github Site, you don’t even have to install the package. For the Shiny app, do the following:\n\ninstall.packages(\"LearnPCA\") # you'll need version 0.2.0\nlibrary(\"LearnPCA\")\nPCsearch()\n\nWe would really appreciate your feedback on this package. You can do so in the comments below, or open an issue. \n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2022,\n  author = {Hanson, Bryan},\n  title = {Introducing {LearnPCA}},\n  date = {2022-05-03},\n  url = {http://chemospec.org/posts/2022-05-03-LearnPCA-Intro/2022-05-03-LearnPCA-Intro.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2022. “Introducing LearnPCA.” May 3, 2022.\nhttp://chemospec.org/posts/2022-05-03-LearnPCA-Intro/2022-05-03-LearnPCA-Intro.html."
  },
  {
    "objectID": "posts/2020-02-19-CS2D-update/2020-02-19-CS2D-update.html",
    "href": "posts/2020-02-19-CS2D-update/2020-02-19-CS2D-update.html",
    "title": "ChemoSpec2D Update",
    "section": "",
    "text": "I’m pleased to announce that ChemoSpec2D, a package for exploratory data analysis of 2D NMR spectra, has been updated on CRAN and is coming to a mirror near you. Barring user reports to the contrary, I feel like the package has pretty much stabilized and is pretty robust. The main area for future expansion is to add additional data import routines. Please feel free to ask about your specific use case!\nThe most noteworthy user-facing improvements are:\n\nFunction import2DSpectra can now handle JCAMP-DX files, Bruker files exported via the TopSpin “totxt” command, and JEOL spectra exported as “generic ascii”. The design allows additional formats to be added if I have test files to play with (hint hint).\nfiles2Spectra2DObject gains a new argument allowSloppy. This experimental feature will allow one to import data sets that do not have the same dimensions. The intent here is to deal with data sets where the number of points in each dimension is similar but not identical. Additional functions will be needed to handle this kind of data. See the documentation for details.\nfiles2Spectra2DObject has also been modified to allow arguments to be passed to list.files and readJDX. This means for instance you can specify a path other than the current working directory, and have the function recurse through sub-directories. This brings files2Spectra2DObject into line with ChemoSpec::files2SpectraObject.\nFunction hats_alignSpectra2D gains new arguments dist_method and maximize which allows the user to pass their choice of distance measure through to the objective function used to evaluate the overlap of the spectra. This greatly improves the quality of the alignment.\nPlotting is simplified with the addition of two new functions to create Lists of Colors, LofC and Lists of Levels, LofL.\nThe basic color scheme for contours was updated to use a perceptually consistent low/blue -&gt; high/red scheme, based on the principles in the colorspace package. The color-handling infrastructure was also changed to allow easy introduction of different color schemes in the future, though the user cannot yet make changes on the fly.\n\nIn addition, a number of small bugs and annoyances were taken care of, arguments tweaked and documentation improved and expanded. Several functions were rebuilt to make them more robust.\nPlease see the package website for the full changelog and all documentation.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2020,\n  author = {Hanson, Bryan},\n  title = {ChemoSpec2D {Update}},\n  date = {2020-02-19},\n  url = {http://chemospec.org/posts/2020-02-19-CS2D-update/2020-02-19-CS2D-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2020. “ChemoSpec2D Update.” February 19,\n2020. http://chemospec.org/posts/2020-02-19-CS2D-update/2020-02-19-CS2D-update.html."
  },
  {
    "objectID": "posts/2023-07-16-Photometer/Photometer.html",
    "href": "posts/2023-07-16-Photometer/Photometer.html",
    "title": "Home Built Photometer",
    "section": "",
    "text": "Way back in 2014, I ordered the parts and started to build a photometer according to the plans laid out by McClain (2014). I didn’t get very far, it was a busy time. Well, I have finally completed the project!\nA number of simple designs for photometers and spectrometers have been published. What drew me to McClain’s approach is that his goal is to teach some basic electronics relevant to instrument design, which is something I have wanted to learn for sometime (apparently since 2014, though actually I think this goes back to watching my father build a Heath Kit stereo receiver which used tubes). Further, McClain starts with a very simple design, and then adds circuit modules to improve the design. Everything is laid out logically and is easy to follow. At each step there is an opportunity to go further to understand how the circuit actually works in detail.\nIn this post I’ll describe the project at various stages. All the electronics are McClain’s design, but instead of McClain’s cuvette holder I used the design of Kvittingen (Kvittingen et al. (2017)) which uses LEGO bricks as a sample holder and can accommodate an additional detector for fluorescence measurements.\nThis design is a photometer, and not a spectrophotometer, because only one wavelength at a time can be measured. The source LED must have an emission spectrum overlapping with the \\(\\lambda_{max}\\) of the compound to be measured; LEDs are available which cover pieces of the whole visible spectrum so it’s pretty easy to swap for a different wavelength range. The detector photodiode (a type of LED, working in reverse) responds over a broad wavelength range, though with greatly varying efficiency. If one wants to measure fluorescence, the photodiode is moved to the 90\\(^{\\circ}\\) position.1\nA couple of important notes:"
  },
  {
    "objectID": "posts/2023-07-16-Photometer/Photometer.html#version-1-dc-power-supply-for-the-led",
    "href": "posts/2023-07-16-Photometer/Photometer.html#version-1-dc-power-supply-for-the-led",
    "title": "Home Built Photometer",
    "section": "Version 1: DC Power Supply for the LED",
    "text": "Version 1: DC Power Supply for the LED\nIn this version a standard “green” LED (maximum emission at 523 nm) is used as the light source and has the simplest possible power supply. As built, the system provides a current of about 26 mA to the LED. The data sheet recommends 30 mA max.\nThe detector in this version is a photodiode linked to a TIA, a transimpedance amplifier. This is an current to voltage (I to V) converter, and something similar can be used in any instrument where a detector generates a current. Figure 1 shows the circuit.\nThe main deviation from McClain’s design is that R2 needed to be set to 3M \\(\\ohm\\) in order to reach about 1V on the output. McClain gives a range of 100K to 1M. As the value of this resistor goes up, the output voltage goes up due to increasing amplification. This change is likely necessary as the photodiode in use here is a bit different than McClain specified. After some experimentation, the current on I1 (which replicates the current produced by the photodiode in the simulation) was set to 1/10,000 of the value of the current of D1, based upon currents observed when isolating D2 from the rest of the circuit.\nMonitoring the current and voltage across D2 as built and warmed up, the values were about 0.3 \\(\\mu\\)A and 0.23 V; if the LEGO holding D1 was moved immediately adjacent to that holding D2 these numbers were 0.7 \\(\\mu\\)A and 0.26 V. These readings support the discussion above that the photodiode was generating a relatively small response.\n\n\n\n\n\n\nFigure 1: Version 1 with simple LED power supply and a transimpedance amplifier as the detector.\n\n\n\nFigure 2 and Figure 3 show the project from each side.\n\n\n\n\n\n\nFigure 2: View of project. The simplicity of the supply to the LED is apparent. The top rail is the negative supply, the lower rail is the positive supply, and the 2nd-from-bottom rail is the ground.\n\n\n\n\n\n\n\n\n\nFigure 3: View of project. The op amp for the detector is in the foreground."
  },
  {
    "objectID": "posts/2023-07-16-Photometer/Photometer.html#version-2-relaxation-oscillator-as-the-led-power-supply",
    "href": "posts/2023-07-16-Photometer/Photometer.html#version-2-relaxation-oscillator-as-the-led-power-supply",
    "title": "Home Built Photometer",
    "section": "Version 2: Relaxation Oscillator as the LED Power Supply",
    "text": "Version 2: Relaxation Oscillator as the LED Power Supply\nThe next step in McClain’s scheme is to change the basic power supply to a more sophisticated “relaxation oscillator” which produces a square wave output with a certain frequency. The idea here is to eliminate stray room light from affecting the output by using a specific AC-like frequency as the source and then modify the detector to only see this frequency. Stray room light may consist of random light causing DC offsets in the circuit, or something more determinant like 60 Hz flicker from light fixtures.\n\nSimulation\nThe relaxation oscillator circuit was modeled in CircuitLab before building the circuit. The circuit is in Figure 4 and the simulation results are shown in Figure 5.\n\n\n\n\n\n\nFigure 4: The relaxation oscillator circuit.\n\n\n\n\n\n\n\n\n\nFigure 5: The relaxation oscillator simulation output. The gold/orange line indicates the charge on C2 building and decaying. When it reaches an extreme positive or negative value, the phase of the output square wave changes. The lower plot is the very small current produced at the output of the op amp.\n\n\n\n\n\nAs Built\nCapacitor C2 controls the frequency of the square wave produced by the relaxation oscillator. Figure 6 shows the oscilloscope traces with C2 set to 1\\(\\mu\\)F which gives a frequency of about 8 Hz, as seen in the video below. This serves as visual “proof of concept”. Figure 7 shows the oscilloscope traces for a value of 4700 pF for C2 which generates a square wave with frequency 1,500 Hz. This is higher than the frequency of any room light flickering and thus will serve as a “carrier” of the absorbance value unaltered by any stray room light, once we add the other modules to the detection side.\nNote that all oscilloscope traces have two vertical scales, one on the left and one on the right, color coordinated with the trace.\n\n\n\n\n\n\n\nFigure 6: Relaxation oscillator with a 1\\(\\mu\\)F capcitor for C2. The blue curve is the output of the op amp, the red line is the charging and discharging of C2. Note the box at the bottom which reports the period of the square wave, which corresponds to a frequency of about 8 Hz.\n\n\n\n\n\n\n\n\n\nFigure 7: Relaxation oscillator with C2 at 4700 pF. Note the horizontal scale range is much smaller than in the previous figure, as the frequency is much higher.\n\n\n\nThe built version of the relaxation oscillator corresponds well with the simulation."
  },
  {
    "objectID": "posts/2023-07-16-Photometer/Photometer.html#version-3-almost-all-the-bells-and-whistles",
    "href": "posts/2023-07-16-Photometer/Photometer.html#version-3-almost-all-the-bells-and-whistles",
    "title": "Home Built Photometer",
    "section": "Version 3: Almost All the Bells and Whistles",
    "text": "Version 3: Almost All the Bells and Whistles\nThis final version contains all the circuits as described by McClain. I decided to measure voltages directly at the output rather than use an Arduino and display to provide an absorbance value.\nFigure 8 shows the final circuit. Note that several test points are labeled and referred to in the discussion below.\n\n\n\n\n\n\nFigure 8: Completed project with key modules labeled. Click for full size.\n\n\n\n\nRelaxation Oscillator\nThe details of the relaxation oscillator are exactly as described above.\n\n\nCurrent Amplifier\nAs the simulation of the relaxation oscillator shows, the current output of the op amp is very small. Consequently a simple transistor is used to bump up the current driving the LED source to an appropriate value.\n\n\nI to V Converter\nThe I to V converter circuit is the same as described earlier.\n\n\nHigh Pass Filter\nA high pass filter takes a signal that is time-varying, in our case a square wave, and filters it so that only high frequency components are kept. This is a key part of the detector design, since we create an approximately 1,500 Hz square wave and any other component, like 60 Hz flicker from room lights, should be eliminated. Figure 9 shows an isolated version of our high pass filter, and Figure 10 shows the frequency dependency filtering.\n\n\n\n\n\n\nFigure 9: Isolated high pass filter circuit.\n\n\n\n\n\n\n\n\n\nFigure 10: Frequency dependence of the high pass filter. Lower values on the vertical axis means greater attenuation.\n\n\n\n\n\nHalf Wave Rectifier\nA half wave rectifier converts an alternating current, alternating between positive and negative values, into a positive only form. Essentially, the negative portion of the signal is converted to positive values, and the positive portion is set to zero. Figure 11 shows the action of the rectifier.\n\n\n\n\n\n\nFigure 11: Action of the half wave rectifier. The red trace is observed at test point D, and fluctuates positive and negative. The blue trace is the rectified wave observed at test point E. Notice that its voltage is always positive.\n\n\n\n\n\nActive Low Pass Filter\nThe final step is an active low pass filter which only passes signals below a certain frequency and amplifies them (that’s the active part). Importantly, in addition to amplifying the signal, the op amp emits a steady DC voltage which is ultimately proportional to the current hitting the photodiode. This is the value we are after when making absorbance measurements. Figure 12 shows the actual output.\n\n\n\n\n\n\nFigure 12: Effect of the low pass filter. The red curve is the same as in the previous figure, namely test poiont D. The blue line is the final DC output at test point F. This is where the final voltages are measured.\n\n\n\nIf we isolate the low pass filter circuit we can try to understand its operation in greater detail. Figure 13 shows the isolated circuit with simulation inputs configured to match the measured inputs.\n\n\n\n\n\n\nFigure 13: Isolated active low pass filter circuit.\n\n\n\nIf we look at the frequency dependence of this circuit, we see that low frequencies are passed relatively unattenuated (Figure 14), as expected. The combination of the earlier high pass filter and this low pass filter amounts to a band pass filter. This suggests a potential follow up design which uses a band pass filter followed by rectification and conversion to DC by some combination of op amps.\n\n\n\n\n\n\nFigure 14: Attenuation of high frequencies by the low pass filter.\n\n\n\nIn addition to the filtering behavior, we know that the circuit produces a steady DC current from the approximately square wave input. Let’s check this using the simulator again, but this time looking at output voltages. Figure 15 shows the results, which should ideally be close to those in Figure 12.\n\n\n\n\n\n\nFigure 15: Voltages produce by the active low pass filer. The square wave is the simulated input. The gold/orange line is the output DC voltage, which is higher than observed, probably due to an imperfect simulation configuration. The key point is that a steady DC voltage is produced.\n\n\n\n\n\nCalibration Curve\nA calibration curve was prepared using a 10 mL plastic syringe and some small bottles. Two drops of red food coloring were added to 10 mL of water to create the first solution. Three mL of the stock solution was added to seven mL of water. This 2nd solution was then diluted in similar fashion and so forth, to get five total solutions. Tap water was used. The green LED was disconnected and the dark current was measured. Next, tap water was used as a blank. Then the voltage for each sample was recorded (voltage measurements are taken at point F in Figure 8). Listing 1 shows the computational steps. Figure 16 shows the samples from most concentrated to least concentrated.\n\n\n\n\n\n\nFigure 16: Calibration samples.\n\n\n\n\n\n\n\nListing 1: Computation of absorbance values.\n\n\ndark &lt;- 6.0e-3 # dark voltage\nblank &lt;- 0.281 # tap water\nvoltage &lt;- c(26.2e-3, 26.8e-3, 34.0e-3, 99.9e-3, 196.0e-3) # sample readings\nstock &lt;- 1.0 # 2 drops red food coloring in 10 mL tap water\ndil &lt;- 3/10 # serial dilution factor\nconc &lt;- c(stock, dil^(1:4))\nDF &lt;- data.frame(Concentration = conc, Voltage = voltage)\nDF$Absorbance &lt;- -log((DF$Voltage - dark)/(blank - dark))\n\n\n\n\nTable 1 shows the results. A calibration curve is shown in Figure 17. Clearly the most concentrated samples exceed the linear behavior expected for Beer’s Law (as observed by McClain). If the two most concentrated samples are dropped, the result is a nice linear relationship, as seen in Figure 18 and the summary of the fit in Listing 2.\n\n\n\n\nTable 1: Relative sample concentrations and corresponding voltages and absorbances.\n\n\n\n\n \n  \n    Concentration \n    Voltage \n    Absorbance \n  \n \n\n  \n    1.0000 \n    0.0262 \n    2.611089 \n  \n  \n    0.3000 \n    0.0268 \n    2.581818 \n  \n  \n    0.0900 \n    0.0340 \n    2.284567 \n  \n  \n    0.0270 \n    0.0999 \n    1.074541 \n  \n  \n    0.0081 \n    0.1960 \n    0.369747 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 17: Calibration curve, hardware version 3.\n\n\n\n\n\n\n\n\n\nListing 2: Results of fitting the three lowest concentration samples.\n\n\nDF35 &lt;- DF[3:5,]\nfit &lt;- lm(DF35$Absorbance ~ DF35$Concentration)\nsummary(fit)\n\n\n\n\n\nCall:\nlm(formula = DF35$Absorbance ~ DF35$Concentration)\n\nResiduals:\n       1        2        3 \n-0.03688  0.15983 -0.12294 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)          0.3118     0.1840   1.694   0.3394  \nDF35$Concentration  22.3292     3.3801   6.606   0.0956 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.205 on 1 degrees of freedom\nMultiple R-squared:  0.9776,    Adjusted R-squared:  0.9552 \nF-statistic: 43.64 on 1 and 1 DF,  p-value: 0.09564\n\n\n\n\n\n\n\n\n\n\nFigure 18: Calibration curve, hardware version 3, dropping the two most concentrated samples.\n\n\n\n\n\nNot too bad!"
  },
  {
    "objectID": "posts/2023-07-16-Photometer/Photometer.html#footnotes",
    "href": "posts/2023-07-16-Photometer/Photometer.html#footnotes",
    "title": "Home Built Photometer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI have not tested the fluoresence measurement as other projects are calling me. In addition to changing the position of the photodiode, a few resistors may need to be changed in order to achieve sufficient signal.↩︎"
  },
  {
    "objectID": "posts/2021-05-22-GSOC-hyperSpec-ChemoSpec/2021-05-22-GSOC-hyperSpec-ChemoSpec.html",
    "href": "posts/2021-05-22-GSOC-hyperSpec-ChemoSpec/2021-05-22-GSOC-hyperSpec-ChemoSpec.html",
    "title": "GSOC 2021: hyperSpec and ChemoSpec!",
    "section": "",
    "text": "I’m really happy to announce that this summer I’ll be a co-mentor on two Google Summer of Code spectroscopy projects:\n\nOnce again, I’ll co-mentor with Claudia and Vilmantas to continue the work Erick started last summer on hyperSpec (see here for Erick’s wrap up blog post at the end of last year). Sang Truong is the very talented student who will be joining us. Sang’s project is described here.\nNew this year: ChemoSpec will be upgraded to use ggplot2 graphics along with interactive graphics for many of the plots that are currently rendered in base graphics. Erick, who was the student working on hyperSpec last summer, will be my co-mentor on this project. We are looking forward to having Tejasvi Gupta as the student on this project.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2021,\n  author = {Hanson, Bryan},\n  title = {GSOC 2021: {hyperSpec} and {ChemoSpec!}},\n  date = {2021-05-22},\n  url = {http://chemospec.org/posts/2021-05-22-GSOC-hyperSpec-ChemoSpec/2021-05-22-GSOC-hyperSpec-ChemoSpec.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2021. “GSOC 2021: hyperSpec and ChemoSpec!”\nMay 22, 2021. http://chemospec.org/posts/2021-05-22-GSOC-hyperSpec-ChemoSpec/2021-05-22-GSOC-hyperSpec-ChemoSpec.html."
  },
  {
    "objectID": "posts/2020-01-22-F4S-update/2020-01-22-F4S-update.html",
    "href": "posts/2020-01-22-F4S-update/2020-01-22-F4S-update.html",
    "title": "FOSS for Spectroscopy Update",
    "section": "",
    "text": "FOSS for Spectroscopy has had a significant update! It’s really quite surprising how many projects are out there. There is a lot of variety and not too much overlap.\n\nAfter a lot of wrestling with Github access issues, the Status column in the table now gives the date of the most recent update to the project that I can find in an automated way.\nThe Notes column is now called Focus and reflects the focus of the projects as far as I can determine things. I’m using a more-or-less controlled vocabulary here, so sorting on the Focus column should bring related projects together.\nThe number of entries is greatly expanded (and I have more in the hopper).\nThe page is now automatically updated weekly, which will keep the links and dates fresh.\n\nAs always, I welcome your feedback in any form. You can use the comments below, or if you have additions/corrections to the page itself, there is info there about how to submit updates.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2020,\n  author = {Hanson, Bryan},\n  title = {FOSS for {Spectroscopy} {Update}},\n  date = {2020-01-22},\n  url = {http://chemospec.org/posts/2020-01-22-F4S-update/2020-01-22-F4S-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2020. “FOSS for Spectroscopy Update.”\nJanuary 22, 2020. http://chemospec.org/posts/2020-01-22-F4S-update/2020-01-22-F4S-update.html."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Bryan A. Hanson",
    "section": "",
    "text": "This is a summary of my professional experience related to software development projects. In prior times I taught chemistry and biochemistry for 32 years at DePauw University. If you would like a more traditional CV covering that phase, please click here. My research into plant metabolomics is featured here.\nExpertise in:"
  },
  {
    "objectID": "resume.html#positions-held",
    "href": "resume.html#positions-held",
    "title": "Bryan A. Hanson",
    "section": "Positions Held",
    "text": "Positions Held\n\n\n\nProfessor of Chemistry & Biochemistry, DePauw University\n\n\n   \n\n\n1986–2018"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Bryan A. Hanson",
    "section": "Education",
    "text": "Education\n\n\n\nPost-doctoral Associate, Oregon State University\n\n\n   \n\n\n1984–1986\n\n\n\n\nPh.D., Chemistry, University of California Los Angeles, California\n\n\n   \n\n\nSeptember 1984\n\n\n\n\nB.S., Biochemistry, California State University Los Angeles, California\n\n\n   \n\n\nOctober 1981"
  },
  {
    "objectID": "resume.html#r-software-packages-authored",
    "href": "resume.html#r-software-packages-authored",
    "title": "Bryan A. Hanson",
    "section": "R Software Packages Authored",
    "text": "R Software Packages Authored\nAll my work can be seen at Github. Here are some highlights:\n\nChemoSpec: Exploratory Chemometrics for Spectroscopy\nChemoSpec2D: Exploratory Chemometrics for 2D Spectroscopy\nLearnPCA A series of vignettes explaining PCA from the very beginning.\nexCon: Interactive Exploration of Contour Data live demo\nreadJDX: Import Data in the JCAMP-DX Format\nSpecHelpers: Spectroscopy Related Utilities\nunmixR: Hyperspectral Unmixing with R, with Anton Belov, Conor McManus, Claudia Beleites and Simon Fuller\nHiveR: Hive Plots in 2D and 3D\nLindenmayeR: Functions to Explore L-Systems (Lindenmayer Systems)"
  },
  {
    "objectID": "resume.html#selected-presentations",
    "href": "resume.html#selected-presentations",
    "title": "Bryan A. Hanson",
    "section": "Selected Presentations",
    "text": "Selected Presentations\n\n“Development of Chemometric Tools for 2D NMR Data Sets” Poster at PANIC 2018 (Practical Applications of NMR in Industry Conference), La Jolla (San Diego) California, March 2018. Download\n“Using R to Make Sense of NMR Data Sets” invited talk at PANIC 2017 (Practical Applications of NMR in Industry Conference), Hilton Head South Carolina, February 2017. Download\n“unmixR: Hyperspectral Unmixing in ” with Conor McManus, Simon Fuller, and Claudia Beleites. Poster at 2014, University of California at Los Angeles, June 30–July 3, 2014. Download\n“Preliminary Metabolic Investigation of Saline-Stressed Portulac olercea using 1H NMR” with Paulina J. Haight and John S. Harwood. Poster at the ACS National Meeting, Indianpolis, September 2013. Download\n“HiveR: 2 and 3D Hive Plots of Networks” Invited talk at useR! 2012 Conference, Nashville Tennessee, June 12–15, 2012. Download\n“HiveR: 2 and 3D Hive Plots of Networks” Poster at useR! 2012 Conference, Nashville Tennessee, June 12–15, 2012. Download\n“Implementation of ANOVA-PCA in R for Multivariate Exploration” with M. J. Keinsley. Poster at useR! 2012 Conference, Nashville Tennessee, June 12–15, 2012. Download\n“The Effect of Climate Change on the Medicinal Plant Purslane Portulaca oleracea” with Elizabeth Botts, Coutney Brimmer, Tanner Miller, Kelley Summers and Dana Dudle. Presented at the 52\\(^{nd}\\) Annual Meeting of the Society for Economic Botany, July 9–13, 2011, St Louis Missouri. Download\n“ChemoSpec: an R Package for the Chemometric Analysis of Spectroscopic Data” useR! 2010 Conference, National Institute of Standards and Technology, Gaithersburg Maryland, July 2010. Download\n“Assessing Serenoa repens (Arecaceae) Quality at the Retail Level Using Spectroscopic and Chemometric Methods” with Tao Ye and M. Daniel Raftery. Presented at the 49\\(^{th}\\) Annual Meeting of the Society for Economic Botany, June 1–5, 2008, Duke University. Download"
  },
  {
    "objectID": "resume.html#miscellaneous",
    "href": "resume.html#miscellaneous",
    "title": "Bryan A. Hanson",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nGoogle Summer of Code 2021: Mentored Tejasvi Gupta (India) who adapted ChemoSpec to provide ggplot2 and plotly graphics options.\nGoogle Summer of Code 2021: Co-mentored Sang Truong (USA) who continued work on the hyperSpec package.\nGoogle Summer of Code 2020: Co-mentored Erick Oduniyi (USA) who began the work of refactoring hyperSpec into sub-packages.\nGoogle Summer of Code 2016: Co-mentored Anton Belov (Russia) in writing new functions for the unmixR package.\nGoogle Summer of Code 2013: Co-mentored Conor McManus (Ireland) in writing the unmixR package.\nI am a significant contributor to the Valid NMR Chemometrics Wiki.\nYou can see my StackExchange Activity."
  },
  {
    "objectID": "posts/2024-01-30-Bitwise-Operators/Bitwise-Operators.html",
    "href": "posts/2024-01-30-Bitwise-Operators/Bitwise-Operators.html",
    "title": "Bitwise Operators in C",
    "section": "",
    "text": "For the EF-NMR project, I’ve turned my attention to writing the software to capture the FID, which seemed easier than writing a pulse transmitter.1 This requires the use of the ADC (analog to digital converter) on the Arduino. Configuring, starting, and stopping the ADC is handled by directly setting bits in a particular register on the Arduino. New territory! This post will serve as a set of notes on what I’ve learned about how this is done. In particular, I want to focus on the code people actually write, which is generally more complex than what one sees in the language reference."
  },
  {
    "objectID": "posts/2024-01-30-Bitwise-Operators/Bitwise-Operators.html#the-bitwise-operators-in-c",
    "href": "posts/2024-01-30-Bitwise-Operators/Bitwise-Operators.html#the-bitwise-operators-in-c",
    "title": "Bitwise Operators in C",
    "section": "The Bitwise Operators in C",
    "text": "The Bitwise Operators in C\nThe definitions of the bitwise C operators can be found in numerous places, stated with various levels of clarity and understandability. Sometimes the definitions are very terse and seemingly quite clear, but after reading, one simply doesn’t know how to use it. The revered text known as “K & R” doesn’t even devote much space to them, though that may be because microcontrollers were a relatively new thing at the time of Kernighan and Ritchie (1988). The Arduino reference documents give quite a bit more detail but don’t have the complexity seen in the wild.\nThe following gives my own interpretation and understanding of the individual operators. To be clear, these definitions don’t really give a sense of why they might be useful or how one would use them.\n\nOR (operator: |) compares two bits and sets the destination bit to 1 unless both inputs are 0. It sets a bit.\nAND (operator: &) compares two bits and sets the destination bit to 0 unless both inputs are 1. It clears a bit.\nXOR (operator: ^) compares two bits and if they are the same, returns 0, if different, returns 1. It toggles a bit.\nNOT (operator: ~) is a unary operator which flips all 1’s to 0’s and vice versa.\nLeft Shift (operator: &lt;&lt;) shifts a series of bits left and fills the space with 0’s. Equivalent to multiplying by 2^n.\nRight Shift (operator: &gt;&gt;) shifts a series of bits right and fills the space with 0’s. Equivalent to dividing by 2^n.\n\nA key thing to note is that these operators compare two bits (which are either 0 or 1) and returns an updated bit. The exceptions are:\n\nThe left and right shift operators: These operate on a series of bits. You can’t shift a single bit without stomping on adjacent memory. Very ungraceful!\nNOT, the toggle: This flips or toggles a single bit, nothing is compared.\n\nThe reality is that one rarely sees these operators used on a single bit, even NOT. More often, one sees them applied to a byte, a set of 8 bits residing contiguously in memory. Those bytes, at least in the current use, turn out to be registers on the Arduino, our next topic."
  },
  {
    "objectID": "posts/2024-01-30-Bitwise-Operators/Bitwise-Operators.html#adc-registers",
    "href": "posts/2024-01-30-Bitwise-Operators/Bitwise-Operators.html#adc-registers",
    "title": "Bitwise Operators in C",
    "section": "ADC Registers",
    "text": "ADC Registers\nThe ATmega328P microcontroller used in the Arduino Uno has several registers that control the ADC:2\n\nADMUX = ADC multiplexer selection register\nADCSRA = ADC status and control register A\nADCSRB = ADC status and control register B\nADCL and ADCH = ADC data registers\n\nWe’ll use ADCSRA as our example. ADCSRA is of course an acronym. If you look at the iom238p.h file where these things are defined, you find that ADCSRA is an alias for a specific memory address(Figure 1).3 It is the address of the first bit of a single byte, composed of 8 bits, numbered 0-7. In the datasheet we can see what is stored in this register. Each of the individual bits has a name, for instance ADEN, which stands for “ADc ENable”, and in the header file, the name ADEN is aliased to bit 7 (Figure 2). So we have an 8 bit memory address with a name and each bit has its own name to make remembering their roles easier. These are the bits we need to control with the bitwise operators in order to configure the ADC.\n\n\n\n\n\n\nFigure 1: A portion of the iom328p.h header file showing how acronyms are aliased to memory locations.\n\n\n\n\n\n\n\n\n\nFigure 2: Documentation of the ADCSRA register from the datasheet."
  },
  {
    "objectID": "posts/2024-01-30-Bitwise-Operators/Bitwise-Operators.html#wild-type-operator-constructs-in-use",
    "href": "posts/2024-01-30-Bitwise-Operators/Bitwise-Operators.html#wild-type-operator-constructs-in-use",
    "title": "Bitwise Operators in C",
    "section": "Wild-Type Operator Constructs in Use",
    "text": "Wild-Type Operator Constructs in Use\nAs I hinted at earlier, what people actually write is rather different from the simple definitions seen in the reference documents (or my version above). So let’s explore these wild-type examples in detail.\n\nSimple Direct Assignment\nOne simple example often seen doesn’t even use the bitwise operators.\nADCSRA = 0;\nIn this case, the right-hand-side (RHS) 0 is interpreted as an 8 bit binary number, 0000 0000 and this sets all 8 bits to zero at once. This incantation is probably most appropriate to reset the entire register, as all zeros is the default setting for this particular register (though not necessarily other registers).\n\n\nDirect Assignment via Binary Literals\nIf you know the value for every bit you want to set, and want to set them all at once, you can use a binary literal:\nADCSRA = B00101010; // prefix binary number with B, or\nADCSRA = 0b00101010; // prefix binary number with 0b\nThe downside here is that future readers of your code have to look up the details of a register’s bit settings everytime they look at your code. Other methods discussed here use aliases for particular bits (e.g. ADEN) which provide at least some mnemonic assistance. Binary literals are only supported in more recent versions of C but you are likely to be using such a version.\n\n\nTypical Bitwise Operator Use in the Wild\n\nExample 1\nADCSRA |= (1 &lt;&lt; ADEN);\nIn this incantation there are several interesting things going on. Let’s unpack it starting from the RHS. We see this expression: (1 &lt;&lt; ADEN), which uses the left shift operator. This means take 1 in binary, so 0000 0001, and shift the 1 left ADEN times. If we look at either Figure 2 or Figure 1, we see that ADEN is 7, so we shift the first bit left 7 places, which gives 1000 0000 in binary. This is a “bit mask”, it’s used in the next step.\nThe operator |= is a variation on the OR operator. It means take whatever is on the RHS, and OR it against the left-hand-side (LHS), and put the result in the LHS.4 What is the current value of ADCSRA in the LHS? We don’t know in this simple example; presumably you would know in a real life example. Whatever it is, when we OR it with the RHS, bit 7, ADEN, gets set to 1, because of how OR is defined. So bit 7 is set to 1, and all other positions are unchanged.\nxxxx xxxx // whatever is in ADCSRA\n1000 0000 // bitmask from RHS\n1xxx xxxx // result of OR (used to overwrite existing ADCSRA)\n\n\nExample 2\nA more involved example using direct assignment as well as bitwise operators is:\nADCSRA = (1 &lt;&lt; ADPS2) | (1 &lt;&lt; ADPS1) | (1 &lt;&lt; ADPS0);\nwhich can be unpacked as three bitmasks, OR’d against each other to get a final result to be put directly into ADCSRA. Using the values of ADPS*, we have:\n0000 0001 // 1 &lt;&lt; ADPS0 (note ADPS0 = 0 so this is no shift at all)\n0000 0010 // 1 &lt;&lt; ADPS1\n0000 0100 // 1 &lt;&lt; ADPS2\n0000 0111 // result put directly into ADCSRA overwriting what is there originally\nNote that the result overwrites the current value of ADCSRA; the four most significant bits are set to zero, regardless of whatever value was there. The next example shows you how to avoid that.\n\n\nExample 3\nAlmost the same action can be accomplished with the following code, except it preserves the current settings in ADCSRA and uses a helper function, bit(), which is specific to Arduino:\nADCSRA |= bit(ADPS0) | bit(ADPS1) | bit(ADPS2);\nbit() is an Arduino function that takes an integer argument and returns an 8 bit value with 1 in the position given by the argument, and zeros elsewhere.5 Thus it unpacks to:\n0000 0001 // bit(ADPS0)\n0000 0010 // bit(ADPS1)\n0000 0100 // bit(ADPS2)\n// the above 3 lines create the same bitmasks as in Example 2; together they become:\n0000 0111 // result of OR the above 3 bitmasks\nxxxx xxxx // whatever is in ADCSRA\nxxxx x111 // result of OR ADCSRA against 0000 0111\nIn the previous two examples 1 &lt;&lt; ADPS0 or bit(ADPS0) does very little since ADPS0 is 0. However, many coders seem to prefer a little verbosity to make clear what they are trying to achieve.6\n\n\nExample 4\nLet’s say you wanted to turn the ADC on if it was off, and off if it was on. This is a job for the ^ or toggle operator. You can use ADCSRA ^= (1 &lt;&lt; ADEN) which unpacks as follows (ADEN is 7):\n1xxx xxxx // initial (on state) of the ADC; other bits unknown\n1000 0000 // result of (1 &lt;&lt; ADEN) \n0xxx xxxx // result of toggling lines 1 and 2; put into ADCSRA; ADC is off\n// or, starting with ADC off\n0xxx xxxx // ADC is off\n1000 0000 // result of (1 &lt;&lt; ADEN)\n1xxx xxxx // result put into ADCSRA; ADC is now on\nNote that the x bits are toggled against 0, which means they are unchanged. See the truth table here.\n\n\n\nFunctions that are collections of bitwise operators\nThe function _BV(bit) is aliased to (1 &lt;&lt; (bit)) and for Arduino you can use bitSet(x, n) or sbi(x, n) to write a 1 to the n-th position of register x. Thus,\nADCSRA |= (1 &lt;&lt; ADEN); // seen earlier\nADCSRA |= _BV(ADEN);\nbitSet(ADCSRA, ADEN);\nsbi(ADCSRA, ADEN);\nare equivalent ways to change bit 7 in ADCSRA.\nFor Arduino, you also have bitClear(x, n) which writes a 0 at the n-th position of register x, essentially the complement of bitSet(x, n). Internally, it is defined as ((x) &= ~(1 &lt;&lt; (n))). Alternatively, one can use cbi(x, n), the complement of sbi(x, n). Let’s say you had 0000 0110 in ADCSRA and wanted to clear the 2nd bit.\nbitClear(ADCSRA, 1); // expands to the following steps:\n0000 0110 // initial value in ADCSRA\n0000 0010 // value of bit mask (1 &lt;&lt; 1)\n1111 1101 // value of ~(1 &lt;&lt; 1) where all bits have been toggled/flipped\n0000 0100 // value after & comparing line 2 to line 3, writing 1 if each mask position is 1\nNotice that the 2nd bit has been cleared. The = part of &= assigns the result to the LHS, namely ADCSRA.\nNote that sbi() and cbi() only work for certain registers on Arduino.\nThis StackOverflow Question has examples of more functions and an interesting discussion of pros, cons and caveats."
  },
  {
    "objectID": "posts/2024-01-30-Bitwise-Operators/Bitwise-Operators.html#sanity-preserving-helper-function",
    "href": "posts/2024-01-30-Bitwise-Operators/Bitwise-Operators.html#sanity-preserving-helper-function",
    "title": "Bitwise Operators in C",
    "section": "Sanity-Preserving Helper Function",
    "text": "Sanity-Preserving Helper Function\nI modified the function found here to print register contents (well, bytes generally) in an easy-to-read format.\nvoid print_bin(byte aByte) {\n  for (int8_t aBit = 7; aBit &gt;= 0; aBit--) {\n    if (aBit == 3) {\n      Serial.print(\" \"); // space between nibbles\n    }\n    Serial.print(bitRead(aByte, aBit) ? '1' : '0');\n  }\n  Serial.println(\" \");\n}\nLet’s use it to check a set of operations which blend Example 2 and Example 3 above, and stick to pure C operations. This code chunk\n  ADCSRA = B10001000; // arbitrary initial value\n  print_bin(ADCSRA);\n  ADCSRA |= (1 &lt;&lt; ADPS2) | (1 &lt;&lt; ADPS1) | (1 &lt;&lt; ADPS0);\n  print_bin(ADCSRA);\ndisplays the following:\n1000 1000 \n1000 1111\nUse it to check your work!"
  },
  {
    "objectID": "posts/2024-01-30-Bitwise-Operators/Bitwise-Operators.html#footnotes",
    "href": "posts/2024-01-30-Bitwise-Operators/Bitwise-Operators.html#footnotes",
    "title": "Bitwise Operators in C",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLet me state for the record that this is just a first version; additional complexity will almost certainly be needed later.↩︎\nThe details on each of these can be found on the datasheet which can be found via a search engine.↩︎\nThe header file is available many places on the internet.↩︎\nAll the operators can be used the same way: |=, ^=, &=, &lt;&lt;= and &gt;&gt;=. For example C &= 2 should be thought of as C = C & 2. See this SO answer.↩︎\nIt’s essential to be careful with language to be clear. A byte is 8 bits, numbered from the right position as 0, i.e. 76543210. So the first bit is at position 0, etc. Thus bit(0) returns 0000 0001.↩︎\nThese three bits are used as a group to set the clock speed of the ADC, so it makes sense to make it clear you are using all three values together.↩︎"
  },
  {
    "objectID": "posts/2020-01-24-CS-update/2020-01-24-CS-update.html",
    "href": "posts/2020-01-24-CS-update/2020-01-24-CS-update.html",
    "title": "ChemoSpec Update",
    "section": "",
    "text": "ChemoSpec has just been updated to version 5.2.12, and should be available on the mirrors shortly.\nThe most significant user-facing changes are actually in the update to ChemoSpecUtils from a few days ago. In addition, the following documentation changes were made:\n\nAdded documentation for updateGroups which has been in ChemoSpecUtils for a while but effectively hidden from users of ChemoSpec.\nFixed the example in mclustSpectra which had an error and used data that was not a good illustration.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2020,\n  author = {Hanson, Bryan},\n  title = {ChemoSpec {Update}},\n  date = {2020-01-24},\n  url = {http://chemospec.org/posts/2020-01-24-CS-update/2020-01-24-CS-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2020. “ChemoSpec Update.” January 24, 2020.\nhttp://chemospec.org/posts/2020-01-24-CS-update/2020-01-24-CS-update.html."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "",
    "text": "If you aren’t familiar with it, the FOSS for Spectroscopy web site lists Free and Open Source Software for spectroscopic applications. The collection is of course never really complete, and your package suggestions are most welcome (how to contribute). My methods for finding packages are improving and at this point the major repositories have been searched reasonably well.\nA few days ago I pushed a major update, and at this point Python packages outnumber R packages more than two to one. The update was made possible because I recently had time to figure out how to search the PyPi.org site automatically.\nIn a previous post I explained the methods I used to find packages related to spectroscopy. These have been updated considerably and the rest of this post will cover the updated methods."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#repos-topics",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#repos-topics",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Repos & Topics",
    "text": "Repos & Topics\nThere are four places I search for packages related to spectroscopy.1\n\nCRAN, searched manually using the packagefinder package.2\nGithub, searched using custom functions and scripts, detailed below.\nPyPi.org, searched as for Github.\njuliapackages.org, searched manually.\n\nThe topics I search are as follows:\n\nNMR\nEPR\nESR\nUV\nVIS\nspectrophotometry\nNIR (IR search terms overlap a lot, and also generate many false positives dealing with IR communications, e.g. TV remotes)\nFT-IR\nFTIR\nRaman\nXRF\nXAS\nLIBS (on PyPi.org one must use “laser induced breakdown spectroscopy” because LIBS is the name of a popular software and generates hundreds of false positives)"
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-cran",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-cran",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Searching CRAN",
    "text": "Searching CRAN\nI search CRAN using packagefinder; the process is quite straightforward and won’t be covered here. However, it is not an automated process (I should probably work on that)."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-github",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-github",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Searching Github",
    "text": "Searching Github\nThe broad approach used to search Github is the same as described in the original post. However, the scripts have been refined and updated, and now exist as functions in a new package I created called webu (for “webutilities”, but that name is taken on CRAN). The repo is here. webu is not on CRAN and I don’t currently intend to put it there, but you can install from the repo of course if you wish to try it out.\nSearching Github is now carried out by a supervising script called /utilities/run_searches.R (in the FOSS4Spectroscopy repo). The script contains some notes about finicky details, but is pretty simple overall and should be easy enough to follow."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-pypi.org",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-pypi.org",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Searching PyPi.org",
    "text": "Searching PyPi.org\nUnlike Github, it is not necessary to authenticate to use the PyPi.org API. That makes things simpler than the Github case. The needed functions are in webu and include some deliberate delays so as to not overload their servers. As for Github, searches are supervised by /utilities/run_searches.R.\nOne thing I observed at PyPi.org is that authors do not always fill out all the fields that PyPi.org can accept, which means some fields are NULL and we have to trap for that possibility. Package information is accessed via a JSON record, for instance the entry for nmrglue can be seen here. This package is pretty typical in that the author_email field is filled out, but the maintainer_email field is not (they are presumably the same). If one considers these JSON files to be analogous to DESCRIPTION in R packages, it looks like there is less oversight on PyPi.org compared to CRAN."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-julia",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-julia",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Searching Julia",
    "text": "Searching Julia\nJulia packages are readily searched manually at juliapackages.org."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#cleaning-final-vetting",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#cleaning-final-vetting",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Cleaning & Final Vetting",
    "text": "Cleaning & Final Vetting\nThe raw results from the searches described above still need a lot of inspection and cleaning to be usable. The PyPi.org and Github results are saved in an Excel worksheet with the relevant URLs. These links can be followed to determine the suitability of each package. In the /Utilities folder there are additional scripts to remove entries that are already in the main database (FOSS4Spec.xlsx), as well as to check the names of the packages: Python authors and/or policies seem to lead to cases where different packages can have names differing by case, but also authors are sometimes sloppy when referring to their own packages, sometimes using mypkg and at other times myPkg to refer to the same package."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#footnotes",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#footnotes",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOnce in a while users submit their own package to the repo, and I also find interesting packages in my literature reading.↩︎\npackagefinder has recently been archived, but hopefully will be back soon.↩︎"
  },
  {
    "objectID": "posts/2020-03-21-Data-Formats-Pt1/2020-03-21-Data-Formats-Pt1.html",
    "href": "posts/2020-03-21-Data-Formats-Pt1/2020-03-21-Data-Formats-Pt1.html",
    "title": "Data Sharing in the Age of Coronavirus, Part 1",
    "section": "",
    "text": "This is Part 1 of a series of posts about data formats for sharing spectroscopic data. Many folks are working from home due to a certain global pandemic. I hope you are all healthy and practicing proper social distancing!\nSharing data is intrinsic to any spectroscopic work. For many tasks, the data need never leave the instrument’s native format. Nowadays the data often goes immediately to some type of shared server, to be available for multiple users. So much of the time we don’t need to worry about format at all, especially if the acquisition software can do the chemometric analysis you need.\nIncreasingly however, publishers want all data deposited and documented somewhere in machine-readable, vendor-neutral form. This is one aspect of reproducible research, where all data and the scripts or steps needed to analyze them are provided electronically with every paper. Or, your data may be headed to one of the many databases out there, where specific formats are required for submission. And while most data acquisition softwares provide some analysis options, if you need to do serious chemometric analysis you likely need to get the data off the machine in a vendor-neutral form. So there are several reasons one should be familiar with the various means of sharing spectroscopic data.\nData sharing/exchange is admittedly a potentially mundane topic. After all, we just want to get on with the scientific question. However, it’s worth knowing something about the options and considering the future of the field. In general I’d say things are a bit of a mess with no clear path to a common format. This series of posts will cover several different vendor-neutral data sharing formats, their pros and cons and their future prospects.\n\nASCII Files\nAlmost all spectroscopic instruments have some means of exporting data as simple ASCII format files. For 1D data, these usually take the form of columns of wavelengths (or the equivalent) and some form of intensity values. There may or may not be metadata and/or headers in the file. The resolution of the data in the file is usually sufficient, but it can be as low as 8-bit precision. Simple inspection is usually enough to understand these files, and eventually, read them in with R or Python, since other than the metadata these files are simply x and y values in columns.\n2D NMR data in ASCII format are a bit more tedious to decipher. Assuming we are talking about data that has been processed, there are choices to be made about ordering the data and no standardization is evident in the wild. Do you export the data by rows (F2 values at fixed F1 value), by columns (F1 values at fixed F2 value), or an entire matrix? Do you export in a format that mirrors how we typically look at the data, namely the lowest F1 values are first and the lowest F2 values last? 2D NMR is unique among 2D plots in not having 0,0 in any corner. Or do you export in an increasing order, as though you were starting from 0,0? While there are a lot of combinations possible, through trial-and-error one can determine how the data was exported. This is naturally easier if you have a reference spectrum for comparison. I can say from experience that this task is do-able but annoying. Some vendors also export hypercomplex data, in which there is a copy of the data that has been transformed only along F2 and a copy in which transformation has occurred on both dimensions.\nIn addition to deciphering how the data is stored in an exported ASCII file, one needs to keep in mind file size, because ASCII values are not compressed. If one is dealing with IR or UV-Vis data, the typically small number of data points means the files are not large, making ASCII export a good option. For 1D NMR data with typically &gt; 16K data points, the size of the files begins to matter a bit, especially if you have large collections of spectra, which are becoming increasingly common with autosamplers. With 2D NMR, spectra in ASCII format begin to take up some serious space, and the time needed to read in the data becomes noticable.\n\nPros & Cons of the ASCII Format\n\n\n\n\nPros\nCons\n\n\nNear-universal availability\nRarely any metadata\n\n\nHuman readible\nRarely any documentation\n\n\n\nSlow to parse for large data sets\n\n\n\nFor 2D NMR, internal order must be deciphered\n\n\n\n\n\n\n\n\nThe Future of the ASCII Format\nBecause of it’s relative simplicity, and near-universal implementation in vendor software, ASCII formatted export files are here to stay.\n\n\n\nJCAMP-DX Files\n\nThe History of JCAMP-DX Format\nThe JCAMP-DX format and standard began at a time when hard drive space was expensive and read/write/transmission errors by hardware were a real issue. This was way before the internet: we are talking about transferring data via telephone/modem, magnetic tape and simple OCR. Hence, three key concerns were to compress the data, to build in data integrity checks and to be flexible for future expansion. Two spectroscopists working with IR data, Robert McDonald and Paul Wilks Jr., published the first standard in 1988 (McDonald and Wilks 1988), with input from instrument manufacturers. From the begininng JCAMP-DX was a project of JCAMP, the Joint Committee on Atomic and Physical Data, a committee of the IUPAC. Refinements were published in 1991 (Grasselli 1991), support for NMR was added in 1993 (A. Davies and Lampen 1993), and MS in 1994 (Lampen et al. 1994) by which time the standard was at version 5 (Lampen et al. 1999). Extensions for CD (Woollett et al. 2012), ion mobility spectrometry (Baumbach et al. 2001) and electron magnetic resonance have been proposed (Cammack et al. 2006). Interestingly, there was also an attempt to describe structure (connectivity) using the format (Gasteiger et al. 1991). In 2001 a JCAMP-DX standard for NMR pulse sequences was published (A. N. Davies et al. 2001).\n\n\nAn Example\nAnother goal for the format was to have the format be both human and machine readible. The format is composed of metadata describing the data and then the compressed data. There are several compression formats possible; some are more human readible than others! Here is a simple example of a JCAMP-DX file containing part of an IR spectrum. The blue box contains the metadata, which is clearly human readible and indeed, most meanings are immediately obvious. The orange box contains the compressed data in the “DIFFDUP” format. In another post we might dissect how that works, but for now, we can clearly read the characters but they need to be translated into actual numerical values.\n\n\n\nPros & Cons of the JCAMP-DX Format\n\n\n\n\nPros\nCons\n\n\nNear-universal availability\nMinimal compression by modern standards\n\n\nMetadata human readible\nError checking makes parsing slow\n\n\nCompression formats can be manually detangled for checking\nError checking probably no longer needed\n\n\n\nVendors do not always follow the standard exactly\n\n\n\n\n\n\n\n\nFuture of the JCAMP-DX Format\nBecause of its long history and universal availability, the JCAMP-DX format appears to be here for the long-haul in spite of its limitations. Future posts in this series will cover data sharing formats that may eventually replace JCAMP-DX.\n\n\n\n\n\n\nReferences\n\nBaumbach, JI, AN Davies, P Lampen, and H Schmidt. 2001. “JCAMP-DX. A Standard Format for the Exchange of Ion Mobility Spectrometry Data - (IUPAC recommendations 2001).” Pure and Applied Chemistry 73 (11): 1765–82. https://doi.org/10.1351/pac200173111765.\n\n\nCammack, R, Y Fann, RJ Lancashire, JP Maher, PS McIntyre, and R Morse. 2006. “JCAMP-DX for electron magnetic resonance(EMR).” Pure and Applied Chemistry 78 (3): 613–31. https://doi.org/10.1351/pac200678030613.\n\n\nDavies, AN, and P Lampen. 1993. “JCAMP-DX for NMR.” Applied Spectroscopy 47 (8): 1093–99. https://doi.org/10.1366/0003702934067874.\n\n\nDavies, Antony N., Jörg Lambert, Robert J. Lancashire, and Peter Lampen. 2001. “Guidelines for the Representation of Pulse Sequences for Solution-State Nuclear Magnetic Resonance Spectroscopy.” Pure and Applied Chemistry 73 (11): 1749–64.\n\n\nGasteiger, J., B. M. P. Hendricks, Hoever P., Jochum C., and Somberg H. 1991. “JCAMP-CS: A Standard Exchange Format for Chemical Structure Information in a Computer-Readible Form.” Applied Spectroscopy 45 (1): 4–11.\n\n\nGrasselli, JG. 1991. “JCAMP-DX, A Standard Format for Exchange of Infrared-Spectra in Computer Readible Form.” Pure and Applied Chemistry 63 (12): 1781–92. https://doi.org/10.1351/pac199163121781.\n\n\nLampen, P, H Hillig, AN Davies, and M Linscheid. 1994. “JCAMP-DX for Mass Spectrometry.” Applied Spectroscopy 48 (12): 1545–52.\n\n\nLampen, P, J Lambert, RJ Lancashire, RS McDonald, PS McIntyre, DN Rutledge, T Frohlich, and AN Davies. 1999. “An Extension to the JCAMP-DX Standard File Format, JCAMP-DX V.5.01 (IUPAC Recommendations 1999).” Pure and Applied Chemistry 71 (8): 1549–56. https://doi.org/10.1351/pac199971081549.\n\n\nMcDonald, RS, and PA Wilks. 1988. “JCAMP-DX, A Standard Format for Exchange of Infrared-Spectra in Computer Readible Form.” Applied Spectroscopy 42 (1): 151–62. https://doi.org/10.1366/0003702884428734.\n\n\nWoollett, Benjamin, Daniel Klose, Richard Cammack, Robert W. Janes, and B. A. Wallace. 2012. “JCAMP-DX for circular dichroism spectra and metadata (IUPAC Recommendations 2012).” Pure and Applied Chemistry 84 (10): 2171–82. https://doi.org/10.1351/PAC-REC-12-02-03.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2020,\n  author = {Hanson, Bryan},\n  title = {Data {Sharing} in the {Age} of {Coronavirus,} {Part} 1},\n  date = {2020-03-21},\n  url = {http://chemospec.org/posts/2020-03-21-Data-Formats-Pt1/2020-03-21-Data-Formats-Pt1.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2020. “Data Sharing in the Age of Coronavirus, Part\n1.” March 21, 2020. http://chemospec.org/posts/2020-03-21-Data-Formats-Pt1/2020-03-21-Data-Formats-Pt1.html."
  },
  {
    "objectID": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html",
    "href": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html",
    "title": "Aligning 2D NMR Spectra Part 2",
    "section": "",
    "text": "This is Part 2 of a series on aligning 2D NMR, as implemented in the package ChemoSpec2D. Part 1 Part 3"
  },
  {
    "objectID": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#the-hats-pr-algorithm",
    "href": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#the-hats-pr-algorithm",
    "title": "Aligning 2D NMR Spectra Part 2",
    "section": "The HATS-PR Algorithm",
    "text": "The HATS-PR Algorithm\nIn Part 1 I briefly mentioned that we would be using the HATS-PR algorithm of Robinette et al. (Robinette et al. 2011). I also discussed the choice of objective function which is used to report on the quality of the alignment. HATS-PR stands for “Hierachical Alignment of Two-Dimensional Spectra - Pattern Recognition”. In ChemoSpec2D the algorithm is implemented in the hats_alignSpectra2D function. Here are the major steps of the HATS-PR algorithm:\n\nContruct a guide tree using hierarchical clustering (HCA): compute the distance between the spectra, and use these distances to construct a dendrogram (the guide tree). As the name suggests, this tree is used to guide the alignment. The most similar spectra are aligned first, then the next most similar, and so on. In later rounds one applies the alignment procedure to sets of spectra that have already been aligned. In Robinette et al. they use the Pearson correlation coefficient as the distance measure. In ChemoSpec2D you can choose from a number of distance measures. I encourage you to experiment with the choices and see how they affect the alignment process for your data sets.\nFor each alignment event, check the alignment using the objective function, which recall is a distance measure. If the objective function is below the threshold, no alignment is needed (“below” assumes we are minimizing the objective function, but we might also be maximizing and hence trying to exceed the threshold). If alignment is necessary, move one of the spectra relative to the other in some fashion, checking each new postion with the objective function until the best alignment is found. This is an exercise in optimization."
  },
  {
    "objectID": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#finding-the-optimal-alignment",
    "href": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#finding-the-optimal-alignment",
    "title": "Aligning 2D NMR Spectra Part 2",
    "section": "Finding the Optimal Alignment",
    "text": "Finding the Optimal Alignment\nThe heart of the task is in the phrase in some fashion. At one extreme, one can imagine holding one spectrum fixed, and sliding the other spectrum left and right, up and down, over some range of values – essentially a grid of data points. At each position on the virtual grid, evaluate the objective function and keep track of the results. This will always find the answer, but such a brute force search will be very time-consuming and undesirable, especially if the search space is large. Alternatively, do something more efficient! Robinette et al. use a “simple gradient ascent” approach, but there is a vast literature on optimization strategies that we can consider. In ChemoSpec2D we use a machine learning approach (details next), but the function is written in such a way that one can add other optimization approaches seamlessly. Anything is better than a brute force approach."
  },
  {
    "objectID": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#optimizing-with-mlrmbo",
    "href": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#optimizing-with-mlrmbo",
    "title": "Aligning 2D NMR Spectra Part 2",
    "section": "Optimizing with mlrMBO",
    "text": "Optimizing with mlrMBO\nThe name mlrMBO comes from “machine learning with R model-based optimization.” mlrMBO is a powerful and flexible package for general purpose optimization, especially in the cases where the objective function is computationally expensive. There is a nice introductory vignette.\nThe basic steps in the model-based optimization using mlrMBO as implemented in hats_alignSpectra2D in package ChemoSpec2D are as follows:\n\nDefine your objective function. Our choice of the Euclidean distance was described in Part 1, along with other options. Most distance measures are not computationally expensive in terms of code. However, the huge number of data points in a typical 2D NMR spectrum bogs things down considerably. The approach taken in model-based optimation mitigates this to a great deal, since the objective function is only used for the initial response surface.\nGenerate an “initial design”, by which we mean a strategy to search the possible optimization space. hats_alignSpectra2D takes arguments maxF1 and maxF2 which define the space that will be considered as the two spectra are shifted relative to each other. The space potentially covered is -maxF1 to maxF1 and similarly for the F2 dimension. We take advantage of concepts from the design of experiments field, and use the lhs package to generate a Latin Hypercube Sample of our space.\nThe sample points selected by lhs are evaluated using the objective function.\nThe values of the objective function at the sample points are used to create a surrogate model, essentially a response surface. The key here is that the surrogate model is computationally fast and will stand in for the actual objective function during the optimization. mlrMBO provides many options for the surrogate model. For hats_alignSpectra2D we use a response surface based on kriging, which is a means of interpolating values that was originally developed in the geospatial statistics world.\nNew samples points are suggested by the kriging algorithm, evaluated using the surrogate function, and used to update (improve) the model. Each iteration improves the quality of the model.\nAfter reaching the designated threshold or the number of iterations specified, the best answer is returned. In this case the best answer is the optimal shift of one spectrum relative to the other, in each dimension."
  },
  {
    "objectID": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#other-details",
    "href": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#other-details",
    "title": "Aligning 2D NMR Spectra Part 2",
    "section": "Other Details",
    "text": "Other Details\nIn addition to the differences noted above, the implementation of HATS-PR in ChemoSpec2D carries out only global alignment. The algorithm described by Robinette et al. includes local alignment steps which I have not implemented. Local alignment is a possible future addition."
  },
  {
    "objectID": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#configure-your-workspace",
    "href": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#configure-your-workspace",
    "title": "Aligning 2D NMR Spectra Part 2",
    "section": "Configure Your Workspace",
    "text": "Configure Your Workspace\nIf you are going to actually execute the code here (as opposed to just reading along), you’ll need the development version of ChemoSpec2D (I improved some of the plots that track the alignment progress since the last CRAN release). And you’ll need certain packages. Here are the steps to install everything:\n\nchooseCRANmirror() # choose a CRAN mirror\ninstall.packages(\"remotes\")\nlibrary(\"remotes\")\n# devel branch -- you need 0.4.156 or higher\ninstall_github(repo = \"bryanhanson/ChemoSpec2D@devel\")\nlibrary(\"ChemoSpec2D\")\n# other packages needed\ninstall.packages(\"mlrMBO\") # will also install mlr, smoof, ParamHelpers\ninstall.packages(\"lhs\")\n\nNow you are ready for the main event! Part 3"
  },
  {
    "objectID": "posts/2020-09-08-GSOC-hyperSpec/2020-09-08-GSOC-hyperSpec.html",
    "href": "posts/2020-09-08-GSOC-hyperSpec/2020-09-08-GSOC-hyperSpec.html",
    "title": "GSOC Wrap Up",
    "section": "",
    "text": "Well, things have been busy lately! As reported back in May, I’ve been participating in Google Summer of Code which has now wrapped up. This was very rewarding for me, but today I want to share a guest post by Erick Oduniyi, the very talented student on the project. Bryan\n\n\n\n\nChecking in from Kansas!\nThis past summer (2020) I had the amazing opportunity to participate in the Google Summer of Code (GSoC or GSOC). As stated on the the GSOC website, GSOC is a “global program focused on bringing more student developers into open source software development. Students work with an open-source organization on a 3-month programming project during their break from school.”\nThis was a particularly meaningful experience as it was my last undergraduate summer internship. I’m a senior studying computer engineering at the University of Kansas, and at the beginning of the summer I still didn’t feel super comfortable working on public (open-source) projects. So, I thought this program would help build my confidence as a computer and software engineer. Moreover:\n\nI wanted to work with the R organization because that is my favorite programming language.\nI wanted to work with r-hyperspec because I thought that would be the most impactful in terms of practicing project management and software ecosystem development.\n\nIn the process I hoped to:\n\nBecome proficient using Git/Github, including continuous integration\nBecome proficient in using Trello\nBecome proficient in using R\nBecome familiar with the spectroscopy community\nBecome inspired to code more\nBecome inspired to document and write more open source projects.\nBecome excited to collaborate more across various industrial, academic, and community domains.\n\nAnd through a lot of hard work all of those things came to be! Truthfully, even though the summer project was successful there is still a lot of work to do:\n\nFortify hyperSpec for baseline with bridge packages\nFortify hyperSpec for EMSC with bridge packages\nFortify hyperSpec for matrixStats with bridge packages.\n\nSo, I’m excited to continue to work with the team! I think there are a ton of ideas I and the team have and hopefully we will get to explore them in deeper context. Speaking of the team, I have them to thank for an awesome GSOC 2020 experience. If you are interested in the journey that was the GSoC 2020 experience (perhaps you might be interested in trying the program next year), then please feel free to jump around here to get a feel for the things that I learned and how I worked with the r-hyperspec team this summer.\nBest, E. Oduniyi\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{oduniyi2020,\n  author = {Oduniyi, Erick},\n  title = {GSOC {Wrap} {Up}},\n  date = {2020-09-08},\n  url = {http://chemospec.org/posts/2020-09-08-GSOC-hyperSpec/2020-09-08-GSOC-hyperSpec.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOduniyi, Erick. 2020. “GSOC Wrap Up.” September 8, 2020. http://chemospec.org/posts/2020-09-08-GSOC-hyperSpec/2020-09-08-GSOC-hyperSpec.html."
  },
  {
    "objectID": "posts/2023-09-18-EF-NMR-2/EF-NMR-2.html",
    "href": "posts/2023-09-18-EF-NMR-2/EF-NMR-2.html",
    "title": "The n + 1 rule in Earth’s Field NMR",
    "section": "",
    "text": "I have been studying Earth’s Field NMR for a bit now. The other day I came across a paper that clued me into some additional interesting features of EF-NMR I was not aware of.\nAs all organic chemists know, in NMR we use the \\(n + 1\\) rule to determine splitting, and Pascal’s triangle as a nemonic to remember the relative areas of the peaks within a multiplet. For instance, we expect that the \\(\\ce{CH3}\\) group in ethanol to be a triplet with areas 1:2:1, due to the \\(\\ce{CH3}\\) group having two proton neighbors in the \\(\\ce{CH2}\\) group. We treat the two protons in \\(\\ce{CH2}\\) as magnetically equivalent."
  },
  {
    "objectID": "posts/2023-09-18-EF-NMR-2/EF-NMR-2.html#j-coupled-specta",
    "href": "posts/2023-09-18-EF-NMR-2/EF-NMR-2.html#j-coupled-specta",
    "title": "The n + 1 rule in Earth’s Field NMR",
    "section": "J-Coupled Specta",
    "text": "J-Coupled Specta\nThe \\(n+1\\) rule works at typical fields used for structural determination, let’s say 60 MHz and above.1 At these fields one is working in the so-called “weak coupling” region. However, as one lowers the field to really low values, one encounters the “strong coupling” region, where one observes “J-coupled-spectra” or JCS. Under strong coupling, the \\(\\ce{CH2}\\) protons in ethanol are no longer magnetically equivalent, and each of them couples differently to other nuclei, and the \\(n + 1\\) rule breaks down.\nThe strict requirement for JCS is that there be two or more protons attached to a spin \\(\\frac{1}{2}\\) heteroatom and the magnetic field be quite small. For a simple system, let’s say \\(\\ce{H_i\\bond{-}X\\bond{-} H_j}\\), the strict requirement to see separate lines for the no-longer-equivalent protons is:\n\\[\n\\lvert J_{XH_i} - J_{XH_j} \\rvert \\ge \\lvert J_{H_{i}H_{j}} \\rvert\n\\]\nIf this seems a bit strange, well, 1) it is, and 2) it has always been the case that the “equivalent” protons in for example a \\(\\ce{CH2}\\) group do couple, we just don’t normally see it or worry about it.2"
  },
  {
    "objectID": "posts/2023-09-18-EF-NMR-2/EF-NMR-2.html#how-small-is-small",
    "href": "posts/2023-09-18-EF-NMR-2/EF-NMR-2.html#how-small-is-small",
    "title": "The n + 1 rule in Earth’s Field NMR",
    "section": "How Small is Small?",
    "text": "How Small is Small?\nHow small does the magnetic field have to be for J-coupled spectra to appear? This is covered in detail in Appelt et al. (2010) but generally speaking JCS appear at around \\(10^{-6}\\) to \\(10^{-4}\\) Tesla.3 The magnetic field of earth is around 50 mT, right in the sweet spot. The Larmor resonance frequency for \\(\\ce{^{1}H}\\) at this field strength is around 2 kHz."
  },
  {
    "objectID": "posts/2023-09-18-EF-NMR-2/EF-NMR-2.html#what-replaces-the-n-1-rule",
    "href": "posts/2023-09-18-EF-NMR-2/EF-NMR-2.html#what-replaces-the-n-1-rule",
    "title": "The n + 1 rule in Earth’s Field NMR",
    "section": "What Replaces the \\(n + 1\\) Rule?",
    "text": "What Replaces the \\(n + 1\\) Rule?\nIn the case of a system like \\(\\ce{X\\bond{-}H_N}\\), the number of lines that will be observed is\n\\[\n\\textrm{no. of lines} = 2 \\sum_{n = 1, n \\in U}^{N}N - n + 1\n\\]\nWhere \\(U\\) is the set of odd numbers (for odd \\(N\\), one evaluates until \\(N -n = 0\\); for even \\(N\\), evaluate until \\(N-n=1\\)). The leading multiplier of 2 accounts for the doublet due to \\(\\ce{J_{XH}}\\). This formula doesn’t exactly roll off the tongue. We can evaluate it to get the first few terms:\n\nN &lt;- 5L # evaluate 1:N terms\nno.lines &lt;- rep(NA_integer_, N) # initialize storage\nfor (i in 1:N) {\n  odd &lt;- (1:i) %% 2\n  n &lt;- (1:i)[as.logical(odd)] # get odd n no larger than N\n  no.lines[i] &lt;- sum(i - n + 1) # take advantage of R's vectorization\n}\nnames(no.lines) &lt;- paste(\"N=\", 1:N, sep = \"\") # pretty it up\nno.lines &lt;- no.lines * 2 # account for J_HX\nno.lines\n\nN=1 N=2 N=3 N=4 N=5 \n  2   4   8  12  18"
  },
  {
    "objectID": "posts/2023-09-18-EF-NMR-2/EF-NMR-2.html#examples",
    "href": "posts/2023-09-18-EF-NMR-2/EF-NMR-2.html#examples",
    "title": "The n + 1 rule in Earth’s Field NMR",
    "section": "Examples",
    "text": "Examples\nA couple of examples should clarify the situation. All of these will be from the perspective of observing \\(\\ce{^{1}H}\\).4 These examples are taken from Appelt et al. (2007).\n\n\\(\\ce{PH2}\\)\nAt high field the \\(\\ce{^{1}H}\\) spectrum of \\(\\ce{PH2}\\) would be a symmetric doublet with a peak separation of \\(\\ce{J_{PH}}\\).\nIn earth’s field, the spectrum is first split into a doublet by \\(\\ce{J_{PH}}\\), but the spacing is not symmetric. Then, each part of the doublet is split further into two peaks, also asymmetrically and with varying linewidths. Figure 1 shows how the splitting changes as a function of field strength. Note that in the strong coupling region there are four peaks, as predicted above.\n\n\n\n\n\n\nFigure 1: Figure 4 from Appelt et al. (2007), showing the field dependence of the \\(\\ce{PH2}\\) spectrum.\n\n\n\n\n\n\\(\\ce{CH3OH}\\)\nFor the case of methanol in Earth’s field, the spectrum is first asymmetrically split by the \\(\\ce{^{13}C}\\) with spacing \\(\\ce{J_{CH}}\\). Then each part of the doublet is further split into four peaks. Figure 2 shows the EF spectrum of methanol. The asymmetry of the line spacing and line widths is apparent.\n\n\n\n\n\n\nFigure 2: Figure 5 from Appelt et al. (2007), showing the spectrum of methanol.\n\n\n\n\n\nFurther Reading\nFor a broad overview of this topic, take a look at Kaseman et al. (2020); for a detailed walk-through of the theory with many more examples, see Appelt et al. (2007), and other papers by Appelt et al. Be prepared to spend some time with these papers."
  },
  {
    "objectID": "posts/2023-09-18-EF-NMR-2/EF-NMR-2.html#footnotes",
    "href": "posts/2023-09-18-EF-NMR-2/EF-NMR-2.html#footnotes",
    "title": "The n + 1 rule in Earth’s Field NMR",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n60 MHz chosen simply because commercial instruments have been available at that field for forever.↩︎\nThe protons in something like \\(\\ce{CH2Cl2}\\) actually do couple to each other. With a little trick, you can measure \\(J_{HH}\\).↩︎\nThis is a general trend. The exact boundaries between various coupling regimes depends on the nuclei involved, the coupling constants and the peak separation in Larmor frequency (in Hz).↩︎\nRemember, signals are very weak at EF so observing heteronuclei is significantly more challenging. See the previous post for details.↩︎"
  },
  {
    "objectID": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html",
    "href": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html",
    "title": "Notes on Linear Algebra Part 1",
    "section": "",
    "text": "If you are already familiar with much of linear algebra, as well as the relevant functions in R, read no further and do something else!\nIf you are like me, you’ve had no formal training in linear algebra, which means you learn what you need to when you need to use it. Eventually, you cobble together some hard-won knowledge. That’s good, because almost everything in chemometrics involves linear algebra.\nThis post is essentially a set of personal notes about the dot product and the cross product, two important manipulations in linear algebra. I’ve tried to harmonize things I learned way back in college physics and math courses, and integrate information I’ve found in various sources I have leaned on more recently. Without a doubt, the greatest impediment to really understanding this material is the use of multiple terminology and notations. I’m going to try really hard to be clear and to the point in my dicussion.\nThe main sources I’ve relied on are:\nLet’s get started. For sanity and consistency, let’s define two 3D vectors and two matrices to illustrate our examples. Most of the time I’m going to write vectors with an arrow over the name, as a nod to the treatment usually given in a physics course. This reminds us that we are thinking about a quantity with direction and magnitude in some coordinate system, something geometric. Of course in the R language a vector is simply a list of numbers with the same data type; R doesn’t care if a vector is a vector in the geometric sense or a list of states.\n\\[\n\\vec{u} = (u_x, u_y, u_z)\n\\tag{1}\\]\n\\[\n\\vec{v} = (v_x, v_y, v_z)\n\\tag{2}\\]\n\\[\n\\mathbf{A} =\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\n\\end{bmatrix}\n\\tag{3}\\]\n\\[\n\\mathbf{B} =\n\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} \\\\\nb_{21} & b_{22} & b_{23} \\\\\nb_{31} & b_{32} & b_{33} \\\\\n\\end{bmatrix}\n\\tag{4}\\]"
  },
  {
    "objectID": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#dot-product",
    "href": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#dot-product",
    "title": "Notes on Linear Algebra Part 1",
    "section": "Dot Product",
    "text": "Dot Product\n\nTerminology\nThe dot product goes by these other names: inner product, scalar product. Typical notations include:1\n\n\\(\\vec{u} \\cdot \\vec{v}\\) (the \\(\\cdot\\) is the origin of the name “dot” product)\n\\(u \\cdot v\\)\n\\(u^\\mathsf{T}v\\) (when thinking of the vectors as column vectors)\n\\(\\langle u, v \\rangle\\) (typically used when \\(u, v\\) are complex)\n\\(\\langle u | v \\rangle\\)\n\n\n\nFormulas\nThere are two main formulas for the dot product with vectors, the algebraic formula (Equation 5) and the geometric formula (Equation 6).\n\\[\n\\vec{u} \\cdot \\vec{v} = \\sum{u_i v_i}\n\\tag{5}\\]\n\\[\n\\vec{u} \\cdot \\vec{v} = \\| \\vec{u} \\| \\| \\vec{v} \\| cos \\theta\n\\tag{6}\\]\n\\(\\| \\vec{x} \\|\\) refers to the \\(L_2\\) or Euclidian norm, namely the length of the vector:2\n\\[\n\\| \\vec{x} \\| =  \\sqrt{x^{2}_1 + \\ldots + x^{2}_n}\n\\tag{7}\\]\nThe result of the dot product is a scalar. The dot product is also commutative: \\(\\vec{u} \\cdot \\vec{v} = \\vec{v} \\cdot \\vec{u}\\).\n\n\n\n\n\n\nWatch out when using row or column vectors\n\n\n\n\n\nFrom the perspective of matrices, if we think of \\(\\vec{u}\\) and \\(\\vec{v}\\) as column vectors with dimensions 3 x 1, then transposing \\(\\vec{u}\\) gives us conformable matrices and we find the result of matrix multiplication is the dot product (compare to Equation 5):\n\\[\n\\vec{u} \\cdot \\vec{v} = \\vec{u}^\\mathsf{T} \\ \\vec{v} = \\begin{bmatrix} u_x & u_y & u_z\\end{bmatrix} \\begin{bmatrix} v_x \\\\ v_y \\\\ v_z \\end{bmatrix} = u_x v_x + u_y v_y + u_z v_z\n\\tag{8}\\]\nEven though this is matrix multiplication, the answer is still a scalar.\nNow, rather confusingly, if we think of \\(\\vec{u}\\) and \\(\\vec{v}\\) as row vectors, and we transpose \\(\\vec{v}\\),then we get the dot product:\n\\[\n\\vec{u} \\cdot \\vec{v} = \\vec{u} \\ \\vec{v}^\\mathsf{T} = \\begin{bmatrix} u_x & u_y & u_z\\end{bmatrix} \\begin{bmatrix} v_x \\\\ v_y \\\\ v_z \\end{bmatrix} = u_x v_x + u_y v_y + u_z v_z\n\\tag{9}\\]\nEquations Equation 8 and Equation 9 can be a source of real confusion at first. They give the impression that the dot product can be either \\(\\vec{u}^\\mathsf{T} \\ \\vec{v}\\) or \\(\\vec{u} \\ \\vec{v}^\\mathsf{T}\\). However, this is only true in the limited contexts defined above. To summarize:\n\nThinking of the vectors as column vectors with dimensions \\(n \\times 1\\) then one can use \\(\\vec{u}^\\mathsf{T} \\ \\vec{v}\\)\nThinking of the vectors as row vectors with dimensions \\(1 \\times n\\) then one can use \\(\\vec{u} \\ \\vec{v}^\\mathsf{T}\\)\n\nUnfortunately I think this distinction is not always clearly made by authors, and is a source of great confusion to linear algebra learners. Be careful when working with row and column vectors.\n\n\n\n\n\nMatrix Multiplication\nSuppose we wanted to compute \\(\\mathbf{A}\\mathbf{B} = \\mathbf{C}\\).3 We use the idea of row and column vectors to accomplish this task. In the process, we discover that matrix multiplication is a series of dot products:\n\\[\n\\begin{multline}\n\\mathbf{A}\\mathbf{B} = \\mathbf{C} =\n\\begin{bmatrix}\n\\textcolor{red}{a_{11}} & \\textcolor{red}{a_{12}} & \\textcolor{red}{a_{13}} \\\\\na_{21} & a_{22} & a_{23} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\textcolor{red}{b_{11}} & b_{12} & b_{13} \\\\\n\\textcolor{red}{b_{21}} & b_{22} & b_{23} \\\\\n\\textcolor{red}{b_{31}} & b_{32} & b_{33} \\\\\n\\end{bmatrix} = \\\\\n\\begin{bmatrix}\n\\textcolor{red}{a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31}} & a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32} & a_{11}b_{13} + a_{12}b_{23} + a_{13}b_{33}\\\\\na_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31} & a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32} & a_{21}b_{13} + a_{22}b_{23} + a_{23}b_{33}\\\\\n\\end{bmatrix}\n\\end{multline}\n\\tag{10}\\]\nThe red color shows how the dot product of the first row of \\(\\mathbf{A}\\) and the first column of \\(\\mathbf{B}\\) gives the first entry in \\(\\mathbf{C}\\). Every entry in \\(\\mathbf{C}\\) results from a dot product. Every entry is a scalar, embedded in a matrix.\n\n\nWhat Can We Do With the Dot Product?\n\nDetermine the angle between two vectors, as in Equation 6.\nAs such, determine if two vectors intersect at a right angle (at least in 2-3D). More generally, two vectors of any dimension are orthogonal if their dot product is zero.\nMatrix multiplication, when applied repeatedly.\nCompute the length of a vector, via \\(\\sqrt{v \\cdot v}\\)\nCompute the projection of one vector on another, for instance how much of a force is along the \\(x\\)-direction? A verbal interpretation of \\(\\vec{u} \\cdot \\vec{v}\\) is it gives the amount of \\(\\vec{v}\\) in the direction of \\(\\vec{u}\\)."
  },
  {
    "objectID": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#cross-product",
    "href": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#cross-product",
    "title": "Notes on Linear Algebra Part 1",
    "section": "Cross Product",
    "text": "Cross Product\n\nTerminology and Notation\nThe cross product goes by these other names: outer product4, tensor product, vector product.\n\n\nFormulas\nThe cross product of two vectors returns a vector rather than a scalar. Vectors are defined in terms of a basis which is a coordinate system. Earlier, when we defined \\(\\vec{u} = (u_x, u_y, u_z)\\) it was intrinsically defined in terms of the standard basis set \\(\\hat{i}, \\hat{j}, \\hat{k}\\) (in some fields this would be called the unit coordinate system). Thus a fuller definition of \\(\\vec{u}\\) would be:\n\\[\n\\vec{u} = u_x\\hat{i} + u_y\\hat{j} + u_z\\hat{k}\n\\tag{11}\\]\nIn terms of vectors, the cross product is defined as: \\[\n\\vec{u} \\times \\vec{v} = (a_{y}b_{z} - a_{z}b_{y})\\hat{i} + (a_{z}b_{x} - a_{x}b_{z})\\hat{j} + (a_{x}b_{y} -a_{y}b_{x})\\hat{k}\n\\tag{12}\\]\nIn my opinion, this is not exactly intuitive, but there is a pattern to it: notice that the terms for \\(\\hat{i}\\) don’t involve the \\(x\\) component. The details of how this result is computed relies on some properties of the basis set; this Wikipedia article has a nice explanation. We need not dwell on it however.\nThere is also a geometric formula for the cross product:\n\\[\n\\vec{u} \\times \\vec{v} = \\| \\vec{u} \\| \\| \\vec{v} \\| sin \\theta \\hat{n}\n\\tag{13}\\]\nwhere \\(\\hat{n}\\) is the unit vector perpendicular to the plane defined by \\(\\vec{u}\\) and \\(\\vec{v}\\). The direction of \\(\\hat{n}\\) is defined by the right-hand rule. Because of this, the cross product is not commutative, i.e. \\(\\vec{u} \\times \\vec{v} \\ne \\vec{v} \\times \\vec{u}\\). The cross product is however anti-commutative: \\(\\vec{u} \\times \\vec{v} = - (\\vec{v} \\times \\vec{u})\\)\n\n\n\n\n\n\nCross product using column vectors\n\n\n\n\n\nAs we did for the dot product, we can look at the cross product from the perspective of column vectors. Instead of transposing the first matrix as we did for the dot product, we transpose the second one: \\[\n\\vec{u} \\times \\vec{v} = \\vec{u} \\ \\vec{v}^\\mathsf{T} = \\begin{bmatrix} u_x \\\\ u_y \\\\ u_z\\end{bmatrix} \\begin{bmatrix} v_x & v_y & v_z \\end{bmatrix} = \\begin{bmatrix} u_{x}v_{x} & u_{x}v_{y} & u_{x}v_{z} \\\\ u_{y}v_{x} & u_{y}v_{y} & u_{y}v_{z} \\\\ u_{z}v_{x} & u_{z}v_{y} & u_{z}v_{z}\\\\ \\end{bmatrix}\n\\tag{14}\\]\nInterestingly, we are using the dot product to compute the cross product.\nThe case where we treat \\(\\vec{u}\\) and \\(\\vec{v}\\) as row vectors is left to the reader.5\n\n\n\nFinally, there is a matrix definition of the cross product as well. Evaluation of the following determinant gives the cross product:\n\\[\n\\vec{u} \\times \\vec{v} = \\begin{vmatrix} \\hat{i} & \\hat{j} & \\hat{k}\\\\ u_{x} & u_{y} & u_{z} \\\\ v_{x} & v_{y} & v_{z}\\\\\\end{vmatrix}\n\\]\n\n\nWhat Can We Do With the Cross Product?\n\nIn 3D, the result of the cross product is perpendicular or normal to the plane defined by the two input vectors.\nIf however, the two vectors are parallel or anti-parallel, the cross product is zero.\nThe length of the cross product is the area of the parallelogram defined by the two input vectors: \\(\\| \\vec{u} \\times \\vec{v} \\|\\)"
  },
  {
    "objectID": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#r-functions",
    "href": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#r-functions",
    "title": "Notes on Linear Algebra Part 1",
    "section": "R Functions",
    "text": "R Functions\n\n%*%\nThe workhorse for matrix multiplication in R is the %*% function. This function will accept any combination of vectors and matrices as inputs, so it is flexible. It is also smart: given a vector and a matrix, the vector will be treated as row or column matrix as needed to ensure conformity, if possible. Let’s look at some examples:\n\n# Some data for examples\np &lt;- 1:5\nq &lt;- 6:10\nM &lt;- matrix(1:15, nrow = 3, ncol = 5)\nM\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    4    7   10   13\n[2,]    2    5    8   11   14\n[3,]    3    6    9   12   15\n\n\n\n# A vector times a vector\np %*% q\n\n     [,1]\n[1,]  130\n\n\nNotice that R returns a data type of matrix, but it is a \\(1 \\times 1\\) matrix, and thus a scalar value. That means we just computed the dot product, a descision R made internally. We can verify this by noting that q %*% p gives the same answer. Thus, R handled these vectors as column vectors and computed \\(p^{\\intercal}q\\).\n\n# A vector times a matrix\nM %*% p\n\n     [,1]\n[1,]  135\n[2,]  150\n[3,]  165\n\n\nAs M had dimensions \\(3 \\times 5\\), R treated p as a \\(5 \\times 1\\) column vector in order to be conformable. The result is a \\(3 \\times 1\\) vector, so this is the cross product.\nIf we try to compute p %*% M we get an error, because there is nothing R can do to p which will make it conformable to M.\n\np %*% M\n\nError in p %*% M: non-conformable arguments\n\n\nWhat about multiplying matrices?\n\nM %*% M\n\nError in M %*% M: non-conformable arguments\n\n\nAs you can see, when dealing with matrices, %*% will not change a thing, and if your matrices are non-conformable then it’s an error. Of course, if we transpose either instance of M we do have conformable matrices, but the answers are different, and this is neither the dot product or the cross product, just matrix multiplication.\n\nt(M) %*% M\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   14   32   50   68   86\n[2,]   32   77  122  167  212\n[3,]   50  122  194  266  338\n[4,]   68  167  266  365  464\n[5,]   86  212  338  464  590\n\nM %*% t(M)\n\n     [,1] [,2] [,3]\n[1,]  335  370  405\n[2,]  370  410  450\n[3,]  405  450  495\n\n\nWhat can we take from these examples?\n\nR will give you the dot product if you give it two vectors. Note that this is a design decision, as it could have returned the cross product (see Equation 14).\nR will promote a vector to a row or column vector if it can to make it conformable with a matrix you provide. If it cannot, R will give you an error. If it can, the cross product is returned.\nWhen it comes to two matrices, R will give an error when they are not conformable.\nOne function, %*%, does it all: dot product, cross product, or matrix multiplication, but you need to pay attention.\nThe documentation says as much, but more tersely: “Multiplies two matrices, if they are conformable. If one argument is a vector, it will be promoted to either a row or column matrix to make the two arguments conformable. If both are vectors of the same length, it will return the inner product (as a matrix)”\n\n\n\nOther Functions\nThere are other R functions that do some of the same work:\n\ncrossprod equivalent to t(M) %*% M but faster.\ntcrossprod equivalent to M %*% t(M) but faster.\nouter or %o%\n\nThe first two functions will accept combinations of vectors and matrices, as does %*%. Let’s try it with two vectors:\n\ncrossprod(p, q)\n\n     [,1]\n[1,]  130\n\n\nHuh. crossprod is returning the dot product! So this is the case where “the cross product is not the cross product.” From a clarity perspective, this is not ideal. Let’s try the other function:\n\ntcrossprod(p, q)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    6    7    8    9   10\n[2,]   12   14   16   18   20\n[3,]   18   21   24   27   30\n[4,]   24   28   32   36   40\n[5,]   30   35   40   45   50\n\n\nThere’s the cross product!\nWhat about outer? Remember that another name for the cross product is the outer product. So is outer the same as tcrossprod? In the case of two vectors, it is:\n\nidentical(outer(p, q), tcrossprod(p, q))\n\n[1] TRUE\n\n\nWhat about a vector with a matrix?\n\ntst &lt;- outer(p, M)\ndim(tst)\n\n[1] 5 3 5\n\n\nAlright, that clearly is not a cross product. The result is an array with dimensions \\(5 \\times 3 \\times 5\\), not a matrix (which would have only two dimensions). outer does correspond to the cross product in the case of two vectors, but anything with higher dimensions gives a different beast. So perhaps using “outer” as a synonym for cross product is not a good idea."
  },
  {
    "objectID": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#advice",
    "href": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#advice",
    "title": "Notes on Linear Algebra Part 1",
    "section": "Advice",
    "text": "Advice\nGiven what we’ve seen above, make your life simple and stick to %*%, and pay close attention to the dimensions of the arguments, especially if row or column vectors are in use. In my experience, thinking about the units and dimensions of whatever it is you are calculating is very helpful. Later, if speed is really important in your work, you can use one of the faster alternatives."
  },
  {
    "objectID": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#footnotes",
    "href": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#footnotes",
    "title": "Notes on Linear Algebra Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn extensive dicussion of notations can be found here.↩︎\nAnd curiously, the \\(L_2\\) norm works out to be equal to the square root of the dot product of a vector with itself: \\(\\| v \\| = \\sqrt{v \\cdot v}\\)↩︎\nTo be multiplied, matrices must be conformable, namely the number of columns of the first matrix must match the number of rows of the second matrix. The reason is so that the dot product terms will match. In the present case we have \\(\\mathbf{A}^{2\\times3}\\mathbf{B}^{3\\times3} = \\mathbf{C}^{2\\times3}\\).↩︎\nBe careful, it turns out that “outer” may not be a great synonym for cross product, as explained later.↩︎\nOK fine, here is the answer when treating \\(\\vec{u}\\) and \\(\\vec{v}\\) as row vectors: \\(\\vec{u} \\times \\vec{v} = \\vec{u}^\\mathsf{T} \\ \\vec{v}\\) which expands exactly as the right-hand side of Equation 14.↩︎"
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "",
    "text": "One of the projects I maintain is the FOSS for Spectroscopy web site. The table at that site lists various software for use in spectroscopy. Historically, I have used the Github or Python Package Index search engines to manually search by topic such as “NMR” to find repositories of interest. Recently, I decided to try to automate at least some of this process. In this post I’ll present the code and steps I developed to search Github by topics. Fortunately, I wasn’t starting from scratch, as I had learned some basic web-scraping techniques when I wrote the functions that get the date of the most recent repository update. All the code for this website and project can be viewed here. The steps reported here are current as of the publication of this post, but are subject to change in the future.1\nFirst off, did you know Github allows repository owners to tag their repositories using topical keywords? I didn’t know this for a long time. So add topics to your repositories if you don’t have them already. By the way, the Achilles heel of this project is that good pieces of software may not have any topical tags at all. If you run into this, perhaps you would consider creating an issue to ask the owner to add tags."
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#the-overall-approach",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#the-overall-approach",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "The Overall Approach",
    "text": "The Overall Approach\nIf you look at the Utilities directory of the project, you’ll see the scripts and functions that power this search process.\n\nSearch Repos for Topics Script.R supervises the whole process. It sources:\nsearchRepos.R (a function)\nsearchTopic.R (a function)\n\nFirst let’s look at the supervising script. First, the necessary preliminaries:\n\nlibrary(\"jsonlite\")\nlibrary(\"httr\")\nlibrary(\"stringr\")\nlibrary(\"readxl\")\nlibrary(\"WriteXLS\")\n\nsource(\"Utilities/searchTopic.R\")\nsource(\"Utilities/searchRepos.R\")\n\nNote that this assumes one has the top level directory, FOSS4Spectroscopy, as the working directory (this is a bit easier than constantly jumping around).\nNext, we pull in the Excel spreadsheet that contains all the basic data about the repositories that we already know about, so we can eventually remove those from the search results.\n\nknown &lt;- as.data.frame(read_xlsx(\"FOSS4Spec.xlsx\"))\nknown &lt;- known$name\n\nNow we define some topics and run the search (more on the search functions in a moment):\n\ntopics &lt;- c(\"NMR\", \"EPR\", \"ESR\")\nres &lt;- searchRepos(topics, \"github_token\", known.repos = known)\n\nWe’ll also talk about that github_token in a moment. With the search results in hand, we have a few steps to make a useful file name and save it in the Searches folder for future use.\n\nfile_name &lt;- paste(topics, collapse = \"_\")\nfile_name &lt;- paste(\"Search\", file_name, sep = \"_\")\nfile_name &lt;- paste(file_name, \"xlsx\", sep = \".\")\nfile_name &lt;- paste(\"Searches\", file_name, sep = \"/\")\nWriteXLS(res, file_name,\n      row.names = FALSE, col.names = TRUE, na = \"NA\")\n\nAt this point, one can open the spreadsheet in Excel and check each URL (the links are live in the spreadsheet). After vetting each site,2 one can append the new results to the existing FOSS4Spec.xlsx data base and refresh the entire site so the table is updated.\nTo make this job easier, I like to have the search results spreadsheet open and then open all the URLs using the as follows. Then I can quickly clean up the spreadsheet (it helps to have two monitors for this process).\n\nfound &lt;- as.data.frame(read_xlsx(file_name))\nfor (i in 1:nrow(found)) {\n  if (grepl(\"^https?://\", found$url[i], ignore.case = TRUE)) BROWSE(found$url[i])\n}"
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#authentificating",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#authentificating",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "Authentificating",
    "text": "Authentificating\nIn order to use the Github API, you have to authenticate. Otherwise you will be severely rate-limited. If you are authenticated, you can make up to 5,000 API queries per hour.\nTo authenticate, you need to first establish some credentials with Github, by setting up a “key” and a “secret”. You can set these up here by choosing the “Oauth Apps” tab. Record these items in a secure way, and be certain you don’t actually publish them by pushing.\nNow you are ready to authenticate your R instance using “Web Application Flow”.3\n\nmyapp &lt;- oauth_app(\"FOSS\", key = \"put_your_key_here\", secret = \"put_your_secret_here\")\ngithub_token &lt;- oauth2.0_token(oauth_endpoints(\"github\"), myapp)\n\nIf successful, this will open a web page which you can immediately close. In the R console, you’ll need to choose whether to do a one-time authentification, or leave a hidden file behind with authentification details. I use the one-time option, as I don’t want to accidently publish the secrets in the hidden file (since they are easy to overlook, being hidden and all)."
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#searchtopic",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#searchtopic",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "searchTopic",
    "text": "searchTopic\nsearchTopic is a function that accesses the Github API to search for a single topic.4 This function is “pretty simple” in that it is short, but there are six helper functions defined in the same file. So, “short not short”. This function does all the heavy lifting; the major steps are:\n\nCarry out an authenticated query of the topics associated with all Github repositories. This first “hit” returns up to 30 results, and also a header than tells how many more pages of results are out there.\nProcess that first set of results by converting the response to a JSON structure, because nice people have already built functions to handle such things (I’m looking at you httr).\n\nCheck that structure for a message that will tell us if we got stopped by Github access issues (and if so, report access stats).\nExtract only the name, description and repository URL from the huge volume of information captured.\n\nInspect the first response to see how many more pages there are, then loop over page two (we already have page 1) to the number of pages, basically repeating step 2.\n\nAlong the way, all the results are stored in a data.frame."
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#searchrepos",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#searchrepos",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "searchRepos",
    "text": "searchRepos\nsearchRepos does two simple things:\n\nLoops over all topics, since searchTopic only handles one topic at a time.\nOptionally, dereplicates the results by excluding any repositories that we already know about."
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#other-stuff-to-make-life-easier",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#other-stuff-to-make-life-easier",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "Other Stuff to Make Life Easier",
    "text": "Other Stuff to Make Life Easier\nThere are two other scripts in the Utilities folder that streamline maintenance of the project.\n\nmergeSearches.R which will merge several search results into one, removing duplicates along the way.\nmergeMaintainers.R which will query CRAN for the maintainers of all packages in FOSS4Spec.xlsx, and add this info to the file.5 Maintainers are not currently displayed on the main website. However, I hope to eventually e-mail all maintainers so they can fine-tune the information about their entries."
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#future-work-contributing",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#future-work-contributing",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "Future Work / Contributing",
    "text": "Future Work / Contributing\nClearly it would be good for someone who knows Python to step in and write the analogous search code for PyPi.org. Depending upon time contraints, I may use this as an opportunity to learn more Python, but really, if you want to help that would be quicker!\nAnd that folks, is how the sausage is made."
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#footnotes",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#footnotes",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis code has been tested on a number of searches and I’ve captured every exception I’ve encountered. If you have problems using this code, please file an issue. It’s nearly impossible that it is perfect at this point!↩︎\nSome search terms produce quite a few false positives. I also review each repository to make sure the project is actually FOSS, is not a student project etc (more details on the main web site).↩︎\nWhile I link to the documentation for completeness, the steps described next do all the work.↩︎\nSee notes in the file: I have not been able to get the Github API to work with multiple terms, so we search each one individually.↩︎\nWant to contribute? If you know the workings of the PyPi.org API it would be nice to automatically pull the maintainer’s contact info.↩︎"
  },
  {
    "objectID": "posts/2023-08-23-CS-Delta/CS-Delta.html",
    "href": "posts/2023-08-23-CS-Delta/CS-Delta.html",
    "title": "JEOL’s Delta Now Includes ChemoSpec",
    "section": "",
    "text": "Over on Twitter I caught news of a new application note from JEOL: Their Delta software for NMR now contains an interface to my R package ChemoSpec. The application note is here and gives a pretty complete overview of what they call the “chemometrics tool”. The JEOL software developers have added a number of short dialog boxes to access the various chemometric methods. The dialog boxes capture the arguments for each underlying function and then the full function call is assembled and passed to Rscript, which is a command line version of R intended for embedded uses such as this one.\nThis is a good example of Free and Open Source Software (FOSS). ChemoSpec is licensed under GPL-3 which permits any reasonable use as long as there is attribution to the original authors.\nCheck out the first line of the “About Delta” box:\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2023,\n  author = {Hanson, Bryan},\n  title = {JEOL’s {Delta} {Now} {Includes} {ChemoSpec}},\n  date = {2023-08-23},\n  url = {http://chemospec.org/posts/2023-08-23-CS-Delta/CS-Delta.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2023. “JEOL’s Delta Now Includes ChemoSpec.”\nAugust 23, 2023. http://chemospec.org/posts/2023-08-23-CS-Delta/CS-Delta.html."
  },
  {
    "objectID": "posts/2021-02-08-PLS/2021-02-08-PLS.html",
    "href": "posts/2021-02-08-PLS/2021-02-08-PLS.html",
    "title": "Interfacing ChemoSpec to PLS",
    "section": "",
    "text": "The ChemoSpec package carries out exploratory data analysis (EDA) on spectroscopic data. EDA is often described as “letting that data speak”, meaning that one studies various descriptive plots, carries out clustering (HCA) as well as dimension reduction (e.g. PCA), with the ultimate goal of finding any natural structure in the data.\nAs such, ChemoSpec does not feature any predictive modeling functions because other packages provide the necessary tools. I do however hear from users several times a year about how to interface a ChemoSpec object with these other packages, and it seems like a post about how to do this is overdue. I’ll illustrate how to carry out partial least squares (PLS) using data stored in a ChemoSpec object and the package chemometrics by Peter Filzmoser and Kurt Varmuza (Filzmoser and Varmuza 2017). One can also use the pls package (Mevik, Wehrens, and Liland 2020).\nPLS is a technique related to regression and PCA that tries to develop a mathematical model between a matrix of sample vectors, in our case, spectra, and one or more separately measured dependent variables that describe the same samples (typically, chemical analyses). If one can develop a reliable model, then going forward one can measure the spectrum of a new sample and use the model to predict the value of the dependent variables, presumably saving time and money. This post will focus on interfacing ChemoSpec objects with the needed functions in chemometrics. I won’t cover how to evaluate and refine your model, but you can find plenty on this in Varmuza and Filzmoser (2009) chapter 4, along with further background (there’s a lot of math in there, but if you aren’t too keen on the math, gloss over it to get the other nuggets). Alternatively, take a look at the vignette that ships with chemometrics via browseVignettes(\"chemometrics\").\nAs our example we’ll use the marzipan NIR data set that one can download in Matlab format from here.1 The corresponding publication is (Christensen et al. 2004). This data set contains NIR spectra of marzipan candies made with different recipes and recorded using several different instruments, along with data about moisture and sugar content. We’ll use the data recorded on the NIRSystems 6500 instrument, covering the 400-2500 nm range. The following code chunk gives a summary of the data set and shows a plot of the data. Because we are focused on how to carry out PLS, we won’t worry about whether this data needs to be normalized or otherwise pre-processed (see the Christensen paper for lots of details).\nlibrary(\"ChemoSpec\")\n\nLoading required package: ChemoSpecUtils\n\n\n\nAs of version 6, ChemoSpec offers new graphics output options\n\n\nFor details, please see ?GraphicsOptions\n\n\n\nThe ChemoSpec graphics option is set to 'ggplot2'\n\n\nTo change it, do\n    options(ChemoSpecGraphics = 'option'),\n    where 'option' is one of 'base' or 'ggplot2' or'plotly'.\n\nload(\"Marzipan.RData\")\nsumSpectra(Marzipan)\n\n\n Marzipan NIR data set from www.models.life.ku.dk/Marzipan \n\n    There are 32 spectra in this set.\n    The y-axis unit is absorbance.\n\n    The frequency scale runs from\n    450 to 2448 wavelength (nm)\n    There are 1000 frequency values.\n    The frequency resolution is\n    2 wavelength (nm)/point.\n\n\n    The spectra are divided into 9 groups: \n\n  group no.     color symbol alt.sym\n1     a   5 #FB0D16FF      1       a\n2     b   4 #FFC0CBFF     16       b\n3     c   4 #2AA30DFF      2       c\n4     d   4 #9BCD9BFF     17       d\n5     e   3 #700D87FF      3       e\n6     f   3 #A777F2FF      8       f\n7     g   2 #FD16D4FF      4       g\n8     h   3 #B9820DFF      5       h\n9     i   4 #B9820DFF      5       i\n\n\n*** Note: this is an S3 object\nof class 'Spectra'\n\nplotSpectra(Marzipan, which = 1:32, lab.pos = 3000)\nIn order to carry out PLS, one needs to provide a matrix of spectroscopic data, with samples in rows (let’s call it \\(X\\), you’ll see why in a moment). Fortunately this data is available directly from the ChemoSpec object as Marzipan$data.2 One also needs to provide a matrix of the additional dependent data (let’s call it \\(Y\\)). It is critical that the order of rows in \\(Y\\) correspond to the order of rows in the matrix of spectroscopic data, \\(X\\).\nSince we are working in R we know there are a lot of ways to do most tasks. Likely you will have the additional data in a spreadsheet, so let’s see how to bring that into the workspace. You’ll need samples in rows, and variables in columns. For your sanity and error-avoidance, you should include a header of variable names and the names of the samples in the first column. Save the spreadsheet as a csv file. I did these steps using the sugar and moisture data from the original paper. Read the file in as follows.\nY &lt;- read.csv(\"Marzipan.csv\", header = TRUE)\nstr(Y)\n\n'data.frame':   32 obs. of  3 variables:\n $ sample  : chr  \"a1\" \"a2\" \"a3\" \"a4\" ...\n $ sugar   : num  32.7 34.9 33.9 33.2 33.2 ...\n $ moisture: num  15 14.9 14.7 14.9 14.9 ...\nThe function we’ll be using wants a matrix as input, so convert the data frame that read.csv generates to a matrix. Note that we’ll select only the numeric variables on the fly, as unlike a data frame, a matrix can only be composed of one data type.\nY &lt;- as.matrix(Y[, c(\"sugar\", \"moisture\")])\nstr(Y)\n\n num [1:32, 1:2] 32.7 34.9 33.9 33.2 33.2 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:2] \"sugar\" \"moisture\"\nNow we are ready to carry out PLS. Since we have a multivariate \\(Y\\), we need to use the appropriate function (use pls1_nipals if your \\(Y\\) matrix is univariate).\nlibrary(\"chemometrics\")\n\nLoading required package: rpart\n\npls_out &lt;- pls2_nipals(X = Marzipan$data, Y, a = 5)\nAnd we’re done! Be sure to take a look at str(pls_out) to see what you got back from the calculation. For the next steps in evaluating your model, see section 3.3 in the chemometrics vignette."
  },
  {
    "objectID": "posts/2021-02-08-PLS/2021-02-08-PLS.html#footnotes",
    "href": "posts/2021-02-08-PLS/2021-02-08-PLS.html#footnotes",
    "title": "Interfacing ChemoSpec to PLS",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI have converted the data from Matlab to a ChemoSpec object; if anyone wants to know how to do this let me know and I’ll put up a post on that process.↩︎\nstr(Marzipan) will show you the structure of the ChemoSpec object (or in general, any R object). The official definition of a ChemoSpec object can be seen via ?Spectra.↩︎"
  },
  {
    "objectID": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html",
    "href": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html",
    "title": "Metabolic Phenotyping Protocol Part 1",
    "section": "",
    "text": "If you aren’t familiar with ChemoSpec, you might wish to look at the introductory vignette first."
  },
  {
    "objectID": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html#process-the-raw-data",
    "href": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html#process-the-raw-data",
    "title": "Metabolic Phenotyping Protocol Part 1",
    "section": "Process the Raw Data",
    "text": "Process the Raw Data\nFirst, we’ll take the results in raw and convert them to the proper form. Each element of raw is a data frame.\n\n# frequencies are in the 1st list element\nfreq &lt;- unlist(raw[[1]], use.names = FALSE)\n\n# intensities are in the 2nd list element\ndata &lt;- as.matrix(raw[[2]])\ndimnames(data) &lt;- NULL  # remove the default data frame col names\nns &lt;- nrow(data)  # ns = number of samples - used later\n\n# get genotype & lifestage, recode into something more readible\nyvars &lt;- raw[[3]]\nnames(yvars) &lt;- c(\"genotype\", \"stage\")\nyvars$genotype &lt;- ifelse(yvars$genotype == 1L, \"WT\", \"Mut\")\nyvars$stage &lt;- ifelse(yvars$stage == 1L, \"L2\", \"L4\")\ntable(yvars)  # quick look at how many in each group\n\n        stage\ngenotype L2 L4\n     Mut 32 33\n     WT  34 40"
  },
  {
    "objectID": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html#assemble-the-spectra-object-by-hand",
    "href": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html#assemble-the-spectra-object-by-hand",
    "title": "Metabolic Phenotyping Protocol Part 1",
    "section": "Assemble the Spectra Object by Hand",
    "text": "Assemble the Spectra Object by Hand\nNext we’ll construct some useful sample names, create the groups vector, assign the colors and symbols, and finally put it all together into a Spectra object.\n\n# build up sample names to include the group membership\nsample_names &lt;- as.character(1:ns)\nsample_names &lt;- paste(sample_names, yvars$genotype, sep = \"_\")\nsample_names &lt;- paste(sample_names, yvars$stage, sep = \"_\")\nhead(sample_names)\n\n[1] \"1_WT_L4\"  \"2_Mut_L4\" \"3_Mut_L4\" \"4_WT_L4\"  \"5_Mut_L4\" \"6_WT_L4\" \n\n# use the sample names to create the groups vector\ngrp &lt;- gsub(\"[0-9]+_\", \"\", sample_names)  # remove 1_ etc, leaving WT_L2 etc\ngroups &lt;- as.factor(grp)\nlevels(groups)\n\n[1] \"Mut_L2\" \"Mut_L4\" \"WT_L2\"  \"WT_L4\" \n\n# set up the colors based on group membership\ndata(Col12)  # see ?colorSymbol for a swatch\ncolors &lt;- grp\ncolors &lt;- ifelse(colors == \"WT_L2\", Col12[1], colors)\ncolors &lt;- ifelse(colors == \"WT_L4\", Col12[2], colors)\ncolors &lt;- ifelse(colors == \"Mut_L2\", Col12[3], colors)\ncolors &lt;- ifelse(colors == \"Mut_L4\", Col12[4], colors)\n\n# set up the symbols based on group membership\nsym &lt;- grp  # see ?points for the symbol codes\nsym &lt;- ifelse(sym == \"WT_L2\", 1, sym)\nsym &lt;- ifelse(sym == \"WT_L4\", 16, sym)\nsym &lt;- ifelse(sym == \"Mut_L2\", 0, sym)\nsym &lt;- ifelse(sym == \"Mut_L4\", 15, sym)\nsym &lt;- as.integer(sym)\n\n# set up the alt symbols based on group membership\nalt.sym &lt;- grp\nalt.sym &lt;- ifelse(alt.sym == \"WT_L2\", \"w2\", alt.sym)\nalt.sym &lt;- ifelse(alt.sym == \"WT_L4\", \"w4\", alt.sym)\nalt.sym &lt;- ifelse(alt.sym == \"Mut_L2\", \"m2\", alt.sym)\nalt.sym &lt;- ifelse(alt.sym == \"Mut_L4\", \"m4\", alt.sym)\n\n# put it all together; see ?Spectra for requirements\nWorms &lt;- list()\nWorms$freq &lt;- freq\nWorms$data &lt;- data\nWorms$names &lt;- sample_names\nWorms$groups &lt;- groups\nWorms$colors &lt;- colors\nWorms$sym &lt;- sym\nWorms$alt.sym &lt;- alt.sym\nWorms$unit &lt;- c(\"ppm\", \"intensity\")\nWorms$desc &lt;- \"C. elegans metabolic phenotyping study (Blaise 2007)\"\nclass(Worms) &lt;- \"Spectra\"\nchkSpectra(Worms)  # verify we have everything correct\nsumSpectra(Worms)\n\n\n C. elegans metabolic phenotyping study (Blaise 2007) \n\n    There are 139 spectra in this set.\n    The y-axis unit is intensity.\n\n    The frequency scale runs from\n    8.9995 to 5e-04 ppm\n    There are 8600 frequency values.\n    The frequency resolution is\n    0.001 ppm/point.\n\n    This data set is not continuous\n    along the frequency axis.\n    Here are the data chunks:\n\n  beg.freq end.freq   size beg.indx end.indx\n1   8.9995   5.0005 -3.999        1     4000\n2   4.5995   0.0005 -4.599     4001     8600\n\n    The spectra are divided into 4 groups: \n\n   group no.     color symbol alt.sym\n1 Mut_L2  32 #FB0D16FF      0      m2\n2 Mut_L4  33 #FFC0CBFF     15      m4\n3  WT_L2  34 #511CFCFF      1      w2\n4  WT_L4  40 #2E94E9FF     16      w4\n\n\n*** Note: this is an S3 object\nof class 'Spectra'"
  },
  {
    "objectID": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html#plot-it-to-check-it",
    "href": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html#plot-it-to-check-it",
    "title": "Metabolic Phenotyping Protocol Part 1",
    "section": "Plot it to check it",
    "text": "Plot it to check it\nLet’s look at one sample from each group to make sure everything looks reasonable (Figure Figure 1). At least these four spectra look good. Note that we are using the latest ChemoSpec that uses ggplot2 graphics by default (announced here).\n\np &lt;- plotSpectra(Worms, which = c(35, 1, 34, 2), lab.pos = 7.5, offset = 0.008, amplify = 35,\n    yrange = c(-0.05, 1.1))\np\n\n\n\n\n\n\n\nFigure 1: Sample spectra from each group.\n\n\n\n\n\nIn the next post we’ll continue with some basic exploratory data analysis.\n\nThis post was created using ChemoSpec version 6.1.3 and ChemoSpecUtils version 1.0.0."
  },
  {
    "objectID": "posts/2021-03-27-Spec-Suite-update/2021-03-27-Spec-Suite-update.html",
    "href": "posts/2021-03-27-Spec-Suite-update/2021-03-27-Spec-Suite-update.html",
    "title": "Spectroscopy Suite Update",
    "section": "",
    "text": "My suite of spectroscopy R packages has been updated on CRAN. There are only a few small changes, but they will be important to some of you:\n\nChemoSpecUtils now provides a set of colorblind-friendly colors, see ?colorSymbol. These are available for use in ChemoSpec and ChemoSpec2D.\nAt the request of several folks, readJDX now includes a function, splitMultiblockDX, that will split a multiblock JCAMP-DX file into separate files, which can then be imported via the usual functions in the package.\nAll packages are built against the upcoming R 4.1 release (due in April).\n\nHere are the links to the documentation:\n\nChemoSpec\nChemSpec2D\nChemoSpecUtils\nreadJDX\n\nAs always, let me know if you discover trouble or have questions.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2021,\n  author = {Hanson, Bryan},\n  title = {Spectroscopy {Suite} {Update}},\n  date = {2021-03-27},\n  url = {http://chemospec.org/posts/2021-03-27-Spec-Suite-update/2021-03-27-Spec-Suite-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2021. “Spectroscopy Suite Update.” March 27,\n2021. http://chemospec.org/posts/2021-03-27-Spec-Suite-update/2021-03-27-Spec-Suite-update.html."
  },
  {
    "objectID": "posts/2023-08-15-F4S-Update/F4S-Update.html",
    "href": "posts/2023-08-15-F4S-Update/F4S-Update.html",
    "title": "FOSS4Spectroscopy Update",
    "section": "",
    "text": "Yesterday I pushed a major update to the FOSS for Spectroscopy web site. Remember that this is a lightly curated and imperfect process; I have some scripts that automate the discovery of packages, but there is still a considerable amount of manual inspection and decision making. If you think I’ve missed a package, please let me know.\nIt’s been nearly a year, and there are a number of new entries. Let’s do a quick comparison of the results from November 2022 versus August 2023. Back in November 2022 there were 246 packages; nearly a year later there are 287. Figure 1 shows a Venn diagram of the changes."
  },
  {
    "objectID": "posts/2023-08-15-F4S-Update/F4S-Update.html#package-language",
    "href": "posts/2023-08-15-F4S-Update/F4S-Update.html#package-language",
    "title": "FOSS4Spectroscopy Update",
    "section": "Package Language",
    "text": "Package Language\nSoftware development in spectroscopy is clearly actively occurring in the Python ecosystem; R has stalled (see Table 1). Interpretation of this observation is challenging. A few thoughts:\n\nOne could claim that the R ecosystem for spectroscopy is mature and further development is naturally going to be limited.\nThe growing popularity of the Python language surely contributes significantly.\nOne motivation for people to write packages is to learn the language and the package delivery system. There’s nothing wrong with these motivations, however this leads to packages that largely overlap in their features.\n\n\n\n\n\nTable 1: Package language, 2022 vs 2023.\n\n\n\n\n \n  \n    language \n    Nov 2022 \n    Aug 2023 \n  \n \n\n  \n    Python \n    162 \n    198 \n  \n  \n    R \n    60 \n    61 \n  \n  \n    C++ \n    4 \n    5 \n  \n  \n    Java \n    4 \n    4 \n  \n  \n    Julia \n    4 \n    5 \n  \n  \n    C \n    2 \n    2 \n  \n  \n    Qt \n    2 \n    2 \n  \n  \n    C-shell \n    1 \n    1 \n  \n  \n    C# \n    1 \n    2 \n  \n  \n    Fortran \n    1 \n    1 \n  \n  \n    Go \n    1 \n    1 \n  \n  \n    html \n    1 \n    1 \n  \n  \n    JavaScript \n    1 \n    2 \n  \n  \n    TypeScript \n    1 \n    1 \n  \n  \n    XML \n    1 \n    1"
  },
  {
    "objectID": "posts/2023-08-15-F4S-Update/F4S-Update.html#package-focus",
    "href": "posts/2023-08-15-F4S-Update/F4S-Update.html#package-focus",
    "title": "FOSS4Spectroscopy Update",
    "section": "Package Focus",
    "text": "Package Focus\nTable 2 shows the change in package focus. Most categories grew modestly.\n\n\n\n\nTable 2: Package focus, 2022 vs 2023.\n\n\n\n\n \n  \n    category \n    Nov 2022 \n    Aug 2023 \n  \n \n\n  \n    Any \n    32 \n    34 \n  \n  \n    Data Sharing \n    33 \n    41 \n  \n  \n    EEM \n    3 \n    3 \n  \n  \n    EPR, ESR \n    5 \n    7 \n  \n  \n    IR (all flavors) \n    35 \n    38 \n  \n  \n    Raman \n    28 \n    34 \n  \n  \n    UV-Vis, UV, Vis \n    19 \n    20 \n  \n  \n    LIBS \n    3 \n    5 \n  \n  \n    Muon \n    1 \n    0 \n  \n  \n    PES \n    1 \n    2 \n  \n  \n    XRF, XAS \n    10 \n    15 \n  \n  \n    NMR \n    87 \n    97 \n  \n  \n    Time Series \n    3 \n    3"
  },
  {
    "objectID": "posts/2023-08-15-F4S-Update/F4S-Update.html#personal-perspective",
    "href": "posts/2023-08-15-F4S-Update/F4S-Update.html#personal-perspective",
    "title": "FOSS4Spectroscopy Update",
    "section": "Personal Perspective",
    "text": "Personal Perspective\nI’ve curated this site for several years now. One thing that is clear is that there is a lot of duplication of effort and features. I mentioned above a few reasons for this, but at some point it makes more sense to add to an existing package than to write one from scratch. However, this can only happen if people look around for existing software first. That of course is one purpose of the FOSS for Spectroscopy web site.\nAs I look at it,\n\nOne-dimensional spectroscopic techniques produce collections of x,y data, usually spectra1, and can thus be stored in a matrix. In terms of organization there’s nothing different between an IR spectrum and a UV-Vis spectrum.\nTwo-dimensional techniques produce data that can be stored in one of two ways:\n\nOne spectrum (or one wavelength) can be stored as matrix, so a set of spectra is a stack of matrices (termed an array in some languages). Think of 2D NMR spectra: one element of the stack is a single 2D spectrum.\nAlternatively, individual spectra can be stored in a matrix and an additional data structure provides a key to how each spectrum relates to the others. Think of a Raman image: spectra are collected over a set of x,y locations.\n\n\nThis design decision is the core of building a package. Once you have decided on a structure:\n\nYou need import methods, these are always tedious to write.\n\nBroadly accepted formats, like JCAMP-DX or plain old csv.\nManufacturer specific formats, some of which may be poorly documented.\n\nYou need processing methods.\n\nWidely used methods, like normalization and smoothing.\nTechnique specific methods, such as zero-filling.\n\nYou need analysis methods.\n\nCommon techniques like PCA.\nAnalysis unique to a specific technique.\n\nYou need visualization methods.\n\nIn an ideal world, a data storage structure is chosen and everything else can be built later, quickly at first and then more slowly. The reality however is that people keep reinventing most of the wheel. I suppose this is not too different from people inventing entirely new computer languages…"
  },
  {
    "objectID": "posts/2023-08-15-F4S-Update/F4S-Update.html#footnotes",
    "href": "posts/2023-08-15-F4S-Update/F4S-Update.html#footnotes",
    "title": "FOSS4Spectroscopy Update",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI say “usually spectra” because for some instruments, depending upon the goal of the package, one may store raw data that must be transformed in a separate step. The best example is raw time-domain NMR data which must be Fourier transformed into frequency-domain spectra before analysis.↩︎"
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html",
    "title": "Notes on Linear Algebra Part 2",
    "section": "",
    "text": "Linear algebra is complex. We need a way to penetrate the thicket. Here’s one.\nLinear systems of equations are at the heart, not surprisingly, of linear algebra.\nA key application is linear regression, which has a matrix solution.\nSolving the needed equations requires inverting a matrix.\nInverting a matrix is more easily done after decomposing the matrix into upper and lower triangular matrices.\nThe upper and lower triangular matrices are individually easy to invert, giving access to the inverse of the original matrix.\nChanges in notations and symbols as you move between presentations add significantly to the cognitive burden in learning this material.\n\n\nFor Part 1 of this series, see here.\nIf you open a linear algebra text, it’s quickly apparent how complex the field is. There are so many special types of matrices, so many different decompositions of matrices. Why are all these needed? Should I care about null spaces? What’s really important? What are the threads that tie the different concepts together? As someone who is trying to improve their understanding of the field, especially with regard to its applications in chemometrics, it can be a tough slog.\nIn this post I’m going to try to demonstrate how some simple chemometric tasks can be solved using linear algebra. Though I cover some math here, the math is secondary right now – the conceptual connections are more important. I’m more interested in finding (and sharing) a path through the thicket of linear algebra. We can return as needed to expand the basic math concepts. The cognitive effort to work through the math details is likely a lot lower if we have a sense of the big picture.\nIn this post, matrices, including row and column vectors, will be shown in bold e.g. \\(\\mathbf{A}\\) while scalars and variables will be shown in script, e.g. \\(n\\). Variables used in R code will appear like A."
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#systems-of-equations",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#systems-of-equations",
    "title": "Notes on Linear Algebra Part 2",
    "section": "Systems of Equations",
    "text": "Systems of Equations\nIf you’ve had algebra, you have certainly run into “system of equations” such as the following:\n\\[\n\\begin{multline}\nx + 2y -3z = 3 \\\\\n2x - y - z = 11 \\\\\n3x + 2y + z = -5 \\\\\n\\end{multline}\n\\tag{1}\\]\nIn algebra, such systems can be solved several ways, for instance by isolating one or more variables and substituting, or geometrically (particularly for 2D systems, by plotting the lines and looking for the intersection). Once there are more than a few variables however, the only manageable way to solve them is with matrix operations, or more explicitly, linear algebra. This sort of problem is the core of linear algebra, and the reason the field is called linear algebra.\nTo solve the system above using linear algebra, we have to write it in the form of matrices and column vectors:\n\\[\n\\begin{bmatrix}\n1 & 2 & -3 \\\\ 2 & -1 & -1 \\\\ 3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\ y \\\\ z\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 \\\\ 11 \\\\ -5\n\\end{bmatrix}\n\\tag{2}\\]\nor more generally\n\\[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}\n\\tag{3}\\]\nwhere \\(\\mathbf{A}\\) is the matrix of coefficients, \\(\\mathbf{x}\\) is the column vector of variable names1 and \\(\\mathbf{b}\\) is a column vector of constants. Notice that these matrices are conformable:2\n\\[\n\\mathbf{A}^{3 \\times 3}\\mathbf{x}^{3 \\times 1} = \\mathbf{b}^{3 \\times 1}\n\\tag{4}\\]\nTo solve such a system, when we have \\(n\\) unknowns, we need \\(n\\) equations.3 This means that \\(\\mathbf{A}\\) has to be a square matrix, and square matrices play a special role in linear algebra. I’m not sure this point is always conveyed clearly when this material is introduced. In fact, it seems like many texts on linear algebra seem to bury the lede.\nTo find the values of \\(\\mathbf{x} = x, y, z\\)4, we can do a little rearranging following the rules of linear algebra and matrix operations. First we pre-multiply both sides by the inverse of \\(\\mathbf{A}\\), which then gives us the identity matrix \\(\\mathbf{I}\\), which drops out.5\n\\[\n\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{x} = \\mathbf{I}\\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}\n\\tag{5}\\]\nSo it’s all sounding pretty simple right? Ha. This is actually where things potentially break down. For this to work, \\(\\mathbf{A}\\) must be invertible, which is not always the case.6 If there is no inverse, then the system of equations either has no solution or infinite solutions. So finding the inverse of a matrix, or discovering it doesn’t exist, is essential to solving these systems of linear equations.7 More on this eventually, but for now, we know \\(\\mathbf{A}\\) must be a square matrix and we hope it is invertible."
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#a-key-application-linear-regression",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#a-key-application-linear-regression",
    "title": "Notes on Linear Algebra Part 2",
    "section": "A Key Application: Linear Regression",
    "text": "A Key Application: Linear Regression\nWe learn in algebra that a line takes the form \\(y = mx + b\\). If one has measurements in the form of \\(x, y\\) pairs that one expects to fit to a line, we need linear regression. Carrying out a linear regression is arguably one of the most important, and certainly a very common application of the linear systems described above. One can get the values of \\(m\\) and \\(b\\) by hand using algebra, but any computer will solve the system using a matrix approach.8 Consider this data:\n\\[\n\\begin{matrix}\nx & y \\\\\n2.1 & 11.8 \\\\\n0.9 & 7.2 \\\\\n3.9 & 21.5 \\\\\n3.2 & 17.2 \\\\\n5.1 & 26.8 \\\\\n\\end{matrix}\n\\tag{6}\\]\nTo express this in a matrix form, we recast\n\\[\ny = mx + b\n\\tag{7}\\]\ninto\n\\[\n\\mathbf{y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\n\\tag{8}\\]\nwhere:\n\n\\(\\mathbf{y}\\) is the column vector of \\(y\\) values. That seems sensible.\n\\(\\mathbf{X}\\) is a matrix composed of a column of ones plus a column of the \\(x\\) values. This is called a design matrix. At least here \\(\\mathbf{X}\\) contains only \\(x\\) values as variables.\n\\(\\mathbf{\\beta}\\) is a column vector of coefficients (including, as we will see, the values of \\(m\\) and \\(b\\) if we are thinking back \\(y = mx + b\\)).\n\\(\\mathbf{\\epsilon}\\) is new, it is a column vector giving the errors at each point.\n\nWith our data above, this looks like:\n\\[\n\\begin{bmatrix}\n11.8 \\\\\n7.2 \\\\\n21.5 \\\\\n17.2 \\\\\n26.8 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 2.1 \\\\\n1 & 0.9 \\\\\n1 & 3.9 \\\\\n1 & 3.2 \\\\\n1 & 5.1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5\n\\end{bmatrix}\n\\tag{9}\\]\nIf we multiply this out, each row works out to be an instance of \\(y_i = \\beta_1 x_i + \\beta_0\\). Hopefully you can appreciate that \\(\\beta_1\\) corresponds to \\(m\\) and \\(\\beta_0\\) corresponds to \\(b\\).9\nThis looks similar to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) seen in Equation 3, if you set \\(\\mathbf{b}\\) to \\(\\mathbf{y}\\), \\(\\mathbf{A}\\) to \\(\\mathbf{X}\\) and \\(\\mathbf{x}\\) to \\(\\beta\\):\n\\[\n\\begin{align}\n\\mathbf{b} &\\approx \\mathbf{A}\\mathbf{x} \\\\\n\\downarrow &\\phantom{\\approx} \\; \\downarrow \\; \\downarrow \\\\\n\\mathbf{y} &\\approx \\mathbf{X}\\mathbf{\\beta} \\\\\n\\end{align}\n\\tag{10}\\]\nThis contortion of symbols is pretty nasty, but honestly not uncommon when moving about in the world of linear algebra.\nAs it is composed of real data, presumably with measurement errors, there is not an exact solution to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) due to the error term. There is however, an approximate solution, which is what is meant when we say we are looking for the line of best fit. This is how linear regression is carried out on a computer. The relevant equation is:\n\\[\n\\hat{\\beta} = (\\mathbf{X}^{\\mathsf{T}}\\mathbf{X})^{-1}\\mathbf{X}^{\\mathsf{T}}y\n\\tag{11}\\]\nThe key point here is that once again we need to invert a matrix to solve this. The details of where Equation 11 comes from are covered in a number of places, but I will note here that \\(\\hat{\\beta}\\) refers to the best estimate of \\(\\beta\\).10"
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#inverting-matrices",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#inverting-matrices",
    "title": "Notes on Linear Algebra Part 2",
    "section": "Inverting Matrices",
    "text": "Inverting Matrices\nWe now have two examples where inverting a matrix is a key step: solving a system of linear equations, and approximating the solution to a system of linear equations (the regression case). These cases are not outliers, the ability to invert a matrix is very important. So how do we do this? The LU decomposition can do it, and is widely used so worth spending some time on. A decomposition is the process of breaking a matrix into pieces that are easier to handle, or that give us special insight, or both. If you are a chemometrician you have almost certainly carried out Principal Components Analysis (PCA). Under the hood, PCA requires either a singular value decomposition, or an eigen decomposition (more info here).\nSo, about the LU decomposition: it breaks a matrix into two matrices, \\(\\mathbf{L}\\), a “lower triangular matrix”, and \\(\\mathbf{U}\\), an “upper triangular matrix”. These special matrices contain only zeros except along the diagonal and the entries below it (in the lower case), or along the diagonal and the entries above it (in the upper case). The advantage of triangular matrices is that they are very easy to invert (all those zeros make many terms drop out). So the LU decomposition breaks the tough job of inverting \\(\\mathbf{A}\\) into two easier jobs.\n\\[\n\\begin{align}\n\\mathbf{A} &= \\mathbf{L}\\mathbf{U} \\\\\n\\mathbf{A}^{-1} &= (\\mathbf{L}\\mathbf{U})^{-1} \\\\\n\\mathbf{A}^{-1} &= \\mathbf{U}^{-1}\\mathbf{L}^{-1} \\\\\n\\end{align}\n\\tag{12}\\]\nWhen all is done, we only need to figure out \\(\\mathbf{U}^{-1}\\) and \\(\\mathbf{L}^{-1}\\) which as mentioned is straightforward.11\nTo summarize, if we want to solve a system of equations we need to carry out matrix inversion, which is turn is much easier to do if one uses the LU decomposition to get two easy to invert triangular matrices. I hope you are beginning to see how pieces of linear algebra fit together, and why it might be good to learn more."
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#r-functions",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#r-functions",
    "title": "Notes on Linear Algebra Part 2",
    "section": "R Functions",
    "text": "R Functions\n\nInverting Matrices\nLet’s look at how R does these operations, and check our understanding along the way. R makes this really easy. We’ll start with the issue of invertibility. Let’s create a matrix for testing.\n\nA1 &lt;- matrix(c(3, 5, -1, 11, 2, 0, -5, 2, 5), ncol = 3)\nA1\n\n     [,1] [,2] [,3]\n[1,]    3   11   -5\n[2,]    5    2    2\n[3,]   -1    0    5\n\n\nIn the matlib package there is a function inv that inverts matrices. It returns the inverted matrix, which we can verify by multiplying the inverted matrix by the original matrix to give the identity matrix (if inversion was successful). diag(3) creates a 3 x 3 matrix with 1’s on the diagonal, in other words an identity matrix.\n\nlibrary(\"matlib\")\nA1_inv &lt;- inv(A1)\nall.equal(A1_inv %*% A1, diag(3)) \n\n[1] \"Mean relative difference: 8.999999e-08\"\n\n\nThe difference here is really small, but not zero. Let’s use a different function, solve which is part of base R. If solve is given a single matrix, it returns the inverse of that matrix.\n\nA1_solve &lt;- solve(A1) %*% A1\nall.equal(A1_solve, diag(3))\n\n[1] TRUE\n\n\nThat’s a better result. Why are there differences? inv uses a method called Gaussian elimination which is similar to how one would invert a matrix using pencil and paper. On the other hand, solve uses the LU decomposition discussed earlier, and no matrix inversion is necessary. Looks like the LU decomposition gives a somewhat better numerical result.\nNow let’s look at a different matrix, created by replacing the third column of A1 with different values.\n\nA2 &lt;- matrix(c(3, 5, -1, 11, 2, 0, 6, 10, -2), ncol = 3)\nA2\n\n     [,1] [,2] [,3]\n[1,]    3   11    6\n[2,]    5    2   10\n[3,]   -1    0   -2\n\n\nAnd let’s compute its inverse using solve.\n\nsolve(A2)\n\nError in solve.default(A2): system is computationally singular: reciprocal condition number = 6.71337e-19\n\n\nWhen R reports that A2 is computationally singular, it is saying that it cannot be inverted. Why not? If you look at A2, notice that column 3 is a multiple of column 1. Anytime one column is a multiple of another, or one row is a multiple of another, then the matrix cannot be inverted because the rows or columns are not independent.12 If this was a matrix of coefficients from an experimental measurement of variables, this would mean that some of your variables are not independent, they must be measuring the same underlying phenomenon.\n\n\nSolving Systems of Linear Equations\nLet’s solve the system from Equation 2. It turns out that the solve function also handles this case, if you give it two arguments. Remember, solve is using the LU decomposition behind the scenes, no matrix inversion is required.\n\nA3 &lt;- matrix(c(1, 2, 3, 2, -1, 2, -3, -1, 1), ncol = 3)\nA3\n\n     [,1] [,2] [,3]\n[1,]    1    2   -3\n[2,]    2   -1   -1\n[3,]    3    2    1\n\ncolnames(A3) &lt;-c(\"x\", \"y\", \"z\") # naming the columns will label the answer\nb &lt;- c(3, 11, -5)\nsolve(A3, b)\n\n x  y  z \n 2 -4 -3 \n\n\nThe answer is the values of \\(x, y, z\\) that make the system of equations true.\n\n\n\n\n\n\nHow does LU decomposition avoid inversion?\n\n\n\n\n\nWhile we’ve emphasized the importance and challenges of inverting matrices, we’ve also pointed out that to solve a linear system there are alternatives to looking at the problem from the perspective of Equation 5. Here’s an approach using the LU decomposition, starting with substituting \\(\\mathbf{A}\\) with \\(\\mathbf{L}\\mathbf{U}\\):\n\\[\n\\mathbf{A}\\mathbf{x} = \\mathbf{L}\\mathbf{U}\\mathbf{x} = \\mathbf{b}\n\\tag{13}\\]\nWe want to solve for \\(\\mathbf{x}\\) the column vector of variables. To do so, define a new vector \\(\\mathbf{y} = \\mathbf{U}\\mathbf{x}\\) and substitute it in:\n\\[\n\\mathbf{L}\\mathbf{U}\\mathbf{x} = \\mathbf{L}\\mathbf{y} = \\mathbf{b}\n\\tag{14}\\]\nNext we solve for \\(\\mathbf{y}\\). One way we could do this is to pre-multiply both sides by \\(\\mathbf{L}^{-1}\\) but we are looking for a way to avoid using the inverse. Instead, we evaluate \\(\\mathbf{L}\\mathbf{y}\\) to give a series of expressions using the dot product (in other words plain matrix multiplication). Because \\(\\mathbf{L}\\) is lower triangular, many of the terms we might have gotten actually disappear because of the zero coefficients. What remains is simple enough that we can algebraically find each element of \\(\\mathbf{y}\\) starting from the first row (this is called forward substitution). Once we have \\(\\mathbf{y}\\), we can find \\(\\mathbf{x}\\) by solving \\(\\mathbf{y} = \\mathbf{U}\\mathbf{x}\\) using a similar approach, but working from the last row upward (this is backward substitution). This is a good illustration of the utility of triangular matrices: some operations can move from the linear algebra realm to the algebra realm. Wikipedia has a good illustration of forward and backward substitution.\n\n\n\n\n\nComputing Linear Regression\nLet’s compute the values for \\(m, b\\) in our regression data shown in Equation 6. First, let’s set up the needed matrices and plot the data since visualizing the data is always a good idea.\n\ny = matrix(c(11.8, 7.2, 21.5, 17.2, 26.8), ncol = 1)\nX = matrix(c(rep(1, 5), 2.1, 0.9, 3.9, 3.2, 5.1), ncol = 2) # design matrix\nX\n\n     [,1] [,2]\n[1,]    1  2.1\n[2,]    1  0.9\n[3,]    1  3.9\n[4,]    1  3.2\n[5,]    1  5.1\n\nplot(X[,2], y, xlab = \"x\") # column 2 of X has the x values\n\n\n\n\n\n\n\n\nThe value of \\(\\hat{\\beta}\\) can be found via Equation 11:\n\nsolve((t(X) %*% X)) %*%  t(X) %*% y\n\n         [,1]\n[1,] 2.399618\n[2,] 4.769862\n\n\nThe first value is for \\(b\\) or \\(\\beta_0\\) or intecept, the second value is for \\(m\\) or \\(\\beta_1\\) or slope.\nLet’s compare this answer to R’s built-in lm function (for linear model):\n\nfit &lt;- lm(y ~ X[,2])\nfit\n\n\nCall:\nlm(formula = y ~ X[, 2])\n\nCoefficients:\n(Intercept)       X[, 2]  \n       2.40         4.77  \n\n\nWe have good agreement! If you care to learn about the goodness of the fit, the residuals etc, then you can look at the help file ?lm and str(fit). lm returns pretty much all one needs to know about the results, but if you wish to calculate all the interesting values yourself you can do so by manipulating Equation 11 and its relatives.\nFinally, let’s plot the line of best fit found by lm to make sure everything looks reasonable.\n\nplot(X[,2], y, xlab = \"x\")\nabline(coef = coef(fit), col = \"red\")\n\n\n\n\n\n\n\n\n\nThat’s all for now, and a lot to digest. I hope you are closer to finding your own path through linear algebra. Remember that investing in learning the fundamentals prepares you for tackling the more complex topics. Thanks for reading!"
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#annotated-bibliography",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#annotated-bibliography",
    "title": "Notes on Linear Algebra Part 2",
    "section": "Annotated Bibliography",
    "text": "Annotated Bibliography\nThese are the main sources I relied on for this post.\n\nThe No Bullshit Guide to Linear Algebra by Ivan Savov.\n\nSection 1.15: Solving systems of linear equations.\nSection 6.6: LU decomposition.\n\nLinear Algebra: step by step by Kuldeep Singh, Oxford Univerity Press, 2014.\n\nSection 1.8.5: Singluar (non-invertible) matrices mean there is no solution or infinite solutions to the linear system. For graphical illustration see sections 1.1.3 and 1.7.2.\nSection 1.6.4: Definition of the inverse and conceptual meaning.\nSection 1.8.4: Solving linear systems when \\(\\mathbf{A}\\) is invertible.\nSection 6.4: LU decomposition.\nSection 6.4.3: Solving linear systems without using inversion, via the LU decomposition.\n\nLinear Models with R by Julian J. Faraway, Chapman and Hall/CRC, 2005.\n\nSections 2.1-2.4: Linear regression from the algebraic and matrix perspectives, derivation of Equation 11.\n\nThe vignettes of the matlib package are very helpful."
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#footnotes",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#footnotes",
    "title": "Notes on Linear Algebra Part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere we have the slightly unfortunate circumstance where symbol conventions cannot be completely harmonized. We are saying that \\(\\mathbf{x} = x, y, z\\) which seems a bit silly since vector \\(\\mathbf{x}\\) contains \\(y\\) and \\(z\\) components in addition to \\(x\\). I ask you to accept this for two reasons: First, most linear algebra texts use the symbols in Equation 3 as the general form for this topic, so if you go to study this further that’s what you’ll find. Second, I feel like using \\(x\\), \\(y\\) and \\(z\\) in Equation 1 will be familar to the most people. If you want to get rid of this infelicity, then you have to write Equation 1 (in part) as \\(x_1 + 2x_2 + 3x_3 = 3\\) which I think clouds the interpretation. Perhaps however you feel my choices are equally bad.↩︎\nConformable means that the number of columns in the first matrix equals the number of rows in the second matrix. This is necessary because of the dot product definition of matrix multiplication. More details here.↩︎\nRemember “story problems” where you had to read closely to express what was given in terms of equations, and find enough equations? “If Sally bought 10 pieces of candy and a drink for $1.50…”↩︎\nWe could also write this as \\(\\mathbf{x} = (x, y, z)^\\mathsf{T}\\) to emphasize that it is a column vector. One might prefer this because the only vector one can write in a row of text is a row vector, so if we mean a column vector many people would prefer to write it transposed.↩︎\nThe inverse of a matrix is analogous to dividing a variable by itself, since it leads to that variable canceling out and thus simplifying the equation. However, strictly speaking there is no operation that qualifies as division in the matrix world.↩︎\nFor a matrix \\(\\mathbf{A}\\) to be invertible, there must exist another matrix \\(\\mathbf{B}\\) such that \\(\\mathbf{A}\\mathbf{B} = \\mathbf{B}\\mathbf{A} = \\mathbf{I}\\). However, this definition doesn’t offer any clues about how we might find the inverse.↩︎\nIn truth, there are other ways to solve \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) that don’t require inversion of a matrix. However, if a matrix \\(\\mathbf{A}\\) isn’t invertible, these other methods will also break down. We’ll demonstrate this later when we talk about the LU decomposition.↩︎\nA very good discussion of the algebraic approach is available here.↩︎\nThis is another example of an infelicity of symbol conventions. The typical math/statistics text symbols are not the same as the symbols a student in Physics 101 would likely encounter.↩︎\nThe careful reader will note that the data set shown in Equation 9 is not square, there are more observations (rows) than variables (columns). This is fine and desireable for a linear regression, we don’t want to use just two data points as that would have no error but not necessarily be accurate. However, only square matrices have inverses, so what’s going on here? In practice, what’s happening is we are using something called a pseudoinverse. The first part of the right side of Equation 11 is in fact the pseudoinverse: \\((\\mathbf{X}^{\\mathsf{T}}\\mathbf{X})^{-1}\\mathbf{X}^{\\mathsf{T}}\\). Perhaps we’ll cover this in a future post.↩︎\nThe switch in the order of matrices on the last line of Equation 12 is one of the properties of the inverse operator.↩︎\nThis means that the rank of the matrix is less than the number of columns. You can get the rank of a matrix by counting the number of non-zero eigenvalues via eigen(A2)$values, which in this case gives 8.9330344, -5.9330344, -3.5953271^{-16}. There are only two non-zero values, so the rank is two. Perhaps in another post we’ll discuss this in more detail.↩︎"
  },
  {
    "objectID": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html",
    "href": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html",
    "title": "Building an EF-NMR Part 1",
    "section": "",
    "text": "Readers may have noticed an Earth Field NMR theme in several recent posts (here, here and here). Behind the scenes, my interest in this topic was growing, fertilized in large part my desire to learn more about electronics. I may have lost my mind, but I have now embarked on a project to build an EF-NMR!\nI was inspired by the really simple EF-NMR instrument developed by Andy Nichol (“Nuclear Magnetic Resonance for Everybody”). Nichol’s work made it clear that one could observe an NMR signal without complex equipment. As I did more reading however, I settled on following the build of Carl Michal (Michal (2010)) as it will allow for more complex experiments, and provides more opportunity to learn electronic circuits.\nMichal’s design uses two coils: a polarization coil, and a transmit/receive (T/R) coil. This post will cover the construction of the polarization coil. Michal’s polarization coil is a three-layer solenoid constructed with 18 AWG magnet wire. Each layer is a separate wire but in operation, the three layers are wired in parallel. I scaled the coil dimensions down somewhat so that I could use materials that are readily accessible to me.1 The plan is to use a 50 mL centrifuge tube as the sample holder. The sample will be placed in a T/R coil wound around a 1.25” schedule 40 PVC pipe. The T/R coil will be located inside the polarization coil, which will be wound on a 2” schedule 40 PVC pipe. The dimensions of these pipes were chosen to allow the sample to nest easily inside the T/R coil which nests inside the polarization coil. Figure 1 shows a cross-section of the design.2\nFigure 1: Cross section of the coils and sample. Grey indicates the PVC pipe components. Red indicates windings (dimensions approximate). Blue represents the sample. Dotted lines show the outer extent of the retainer rings. Scale is in mm."
  },
  {
    "objectID": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#constructing-the-form",
    "href": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#constructing-the-form",
    "title": "Building an EF-NMR Part 1",
    "section": "Constructing the Form",
    "text": "Constructing the Form\nThe form for the polarization coil was made from a 12 cm length of 2” PVC pipe. Two retaining rings were very carefully cut from a 2” PVC coupling. The retaining rings were 1 cm wide. The parts are shown in Figure 2. The rings were then glued to the ends of the form using a minimal amount of standard PVC glue. The inner edges of the rings correspond to the original end of the coupling which provides a clean and straight edge where it will rest against the magnet wire. The ends of the assembly were lightly sanded. As built, the length available for the windings is 102 mm.\n\n\n\n\n\n\nFigure 2: The form and two retaining rings before assembly.\n\n\n\nNext, three holes were drilled close to each of the retaining rings, about 1 cm apart. The magnet wire will pass through these holes, which will serve to keep the wire in place as it is wound. Figure 3 shows these holes. A short length of wire was placed in the holes as a “keeper” as the winding was carried out. This ensured that the winding for the first layer did not block the holes for the second and third layers of wire (Figure 4).\n\n\n\n\n\n\nFigure 3: View of the holes on each end of the form assembly.\n\n\n\n\n\n\n\n\n\nFigure 4: Detail of the wire keepers. The open hole will be used for the first winding layer. The keepers will maintain access to the holes for the subsequent layers."
  },
  {
    "objectID": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#the-winding-jig",
    "href": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#the-winding-jig",
    "title": "Building an EF-NMR Part 1",
    "section": "The Winding Jig",
    "text": "The Winding Jig\nA winding jig was constructed from 1/4” hobby plywood. The base is 6 x 12”. Small nails and glue were used to assemble the sides and back. A 1/4” threaded rod serves as the rotational axis. Nuts and washers secure a simple handle as well as position the rod overall in the jig. Figure 5 and Figure 6 show the jig.\n\n\n\n\n\n\nFigure 5: The winding jig.\n\n\n\n\n\n\n\n\n\nFigure 6: Another view of the winding jig."
  },
  {
    "objectID": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#wire-spool-holder",
    "href": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#wire-spool-holder",
    "title": "Building an EF-NMR Part 1",
    "section": "Wire Spool Holder",
    "text": "Wire Spool Holder\nA holder for the wire spool was constructed with 1/16” x 1” aluminum bar. The bar was bent into a shape that would provide a way to apply friction to the sides of the spool, thus controlling the tension on the wire as it pays out. The spool is mounted on a 1/4” threaded rod and there are wingnuts on each side, which when tightened press the aluminum bar against the spool. The threaded rod does tend to unscrew as the wire is spooled out, but the process is slow enough that one can correct this as needed. If I were going to do this alot I would replace the wingnut on the side that tends to unwind with two nuts locked against each other. The holder is loosely attached to the work bench so that it can pivot as needed to accommodate the changing angle of the wire as it moves across the form. Figure 7 shows the design.\n\n\n\n\n\n\nFigure 7: The wire spool holder. Tightening the wingnuts pushes the aluminum bar against the spool and gives some control over the wire tension as it pays out."
  },
  {
    "objectID": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#the-winding-process",
    "href": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#the-winding-process",
    "title": "Building an EF-NMR Part 1",
    "section": "The Winding Process",
    "text": "The Winding Process\nThe form was more or less centered on the threaded rod using a couple of wooden guide pieces. The winding process is shown in Figure 8. The wire for the first layer comes from inside the form and up through one of the holes and is wound on the form. The action of the keepers is apparent. The fingers are used to position the wire correctly. In principle tension on the wire is provided by tightening the wing nuts on the wire supply holder. However, I did not tighten them enough and I had to wrestle with getting layer one tight enough. This caused problems with the subsequent layers as you will see!\n\n\n\n\n\n\nFigure 8: Winding the first layer.\n\n\n\nThe completed layer one is shown in Figure 9. The winding looks even. Layer two is shown in Figure 10. Because layer one was a little loose, the wire for layer two would sometimes slip in-between the wires of layer one and force them apart. This was exacerbated because I was using more tension on the wire supply for layer two. Clearly the layer is not even. In addition, winding layer two was more difficult because without the white background one cannot see the progress very well.\n\n\n\n\n\n\nFigure 9: Completed layer one. It looks nice and even but the winding is loose.\n\n\n\n\n\n\n\n\n\nFigure 10: Completed layer two. Technique short-comings are evident!\n\n\n\nThe problems only worsened with layer 3 (Figure 11). I am not happy with the final result, but the wire is positionally stable and it should carry out its function well enough. What I’ve learned here will help when winding the T/R coil.\n\n\n\n\n\n\nFigure 11: Completed layer three. Not nearly as pretty as I had hoped!"
  },
  {
    "objectID": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#checking-continuity",
    "href": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#checking-continuity",
    "title": "Building an EF-NMR Part 1",
    "section": "Checking Continuity",
    "text": "Checking Continuity\nThe polymeric insulation on the leads was sanded off (Figure 12) and the resistance of each coil was measured. Each gave a resistance of about 0.7 \\(\\ohm\\) and there were no shorts between the layers.\n\n\n\n\n\n\nFigure 12: Leads for each layer with insulation sanded off."
  },
  {
    "objectID": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#whats-next",
    "href": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#whats-next",
    "title": "Building an EF-NMR Part 1",
    "section": "What’s Next",
    "text": "What’s Next\nThe next step will be the construction of the polarization coil power supply, and integration of the Arduino controller. I’m not in a hurry!"
  },
  {
    "objectID": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#footnotes",
    "href": "posts/2023-10-24-EF-NMR-Build-1/EF-NMR-Build-1.html#footnotes",
    "title": "Building an EF-NMR Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOf course, there will be less sample and therefore a smaller signal.↩︎\nThe dimensions of schedule 40 PVC products are readily available online, which made planning the overall design much simpler.↩︎"
  },
  {
    "objectID": "posts/2023-06-12-DIY-NMR/DIY-NMR.html",
    "href": "posts/2023-06-12-DIY-NMR/DIY-NMR.html",
    "title": "DIY NMR in Earth’s Field",
    "section": "",
    "text": "I have always loved every aspect of NMR. My first introduction was as an undergraduate at Cal State Los Angeles, where I was introduced to a Bruker instrument that used a folded punched tape to read in its operating system. Fortunately, that machine was already quite older and there was a Varian EM-360 which was the work horse for routine spectra (bonus points if you can guess roughly what year this was!). Besides the extremely broad usefulness of NMR instruments, the combination of physics, chemistry, computer science and electronics that undergird the practical aspects of NMR are endlessly fascinating to me.\nThe development of simple, home-built NMR instruments over the past two decades is very interesting and appealing. These instruments typically don’t have a magnet, but rather use the earth’s magnetic field and some type of polarization process to improve sensitivity. Most of these instruments use an inexpensive microprocessor like an Arduino or Raspberry Pi to control the instrument, along with some purpose-built electronic circuits. Good examples are the work of Michal (Michal (2010), Michal (2020)), Trevelyan (Manley (2019)) and Bryden (Bryden et al. (2021)). These instruments of course aren’t able to give the same results as higher-field instruments with superconducting magnets or Halbach arrays. What can you do with these instruments? Because earth’s magnetic field is very homogeneous locally, the line widths are very narrow, and thus coupling constants and \\(T_2\\) can be measured.1 However, the chemical shift range is really small, so structural studies are out. Sensitivity is relatively poor as well. Imaging (MRI) is in principle possible. By the way, there are also examples of DIY Nuclear Quadropole Resonce (NQR) instruments as well, which require no magnetic field (Hiblot et al. (2008)).\nRecently, a simpler DIY NMR instrument was published as a Hackaday project by Andy Nichol. This “Nuclear Magnetic Resonance for Everybody” project is unique due to its use of only off-the-shelf commericially available hardware components. Because the hydrogen Larmor precession frequency in earth’s magnetic field is in the audio range, the project uses a standard and readily available audio amplifier to simplify the signal detection process. In addition, the complexities of pulse programming are avoided in this project by using a mechanical switch to switch between polarization and detection modes. Finally, a single coil is employed for both polarization and detection. Signal processing is handled by readily available software.\nThis is an interesting project and it is the most basic entry point into DIY NMR that I have encountered. If it whets your appetite, the project can be made progressively more sophisticated by selectively bringing in the more advanced features of some of the other designs."
  },
  {
    "objectID": "posts/2023-06-12-DIY-NMR/DIY-NMR.html#footnotes",
    "href": "posts/2023-06-12-DIY-NMR/DIY-NMR.html#footnotes",
    "title": "DIY NMR in Earth’s Field",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLocally homogeneous provided you are away from buildings, electrical transmission lines etc.↩︎"
  },
  {
    "objectID": "posts/2020-05-07-GSOC-hyperSpec/2020-05-07-GSOC-hyperSpec.html",
    "href": "posts/2020-05-07-GSOC-hyperSpec/2020-05-07-GSOC-hyperSpec.html",
    "title": "Fortifying hyperSpec: Getting Ready for GSOC",
    "section": "",
    "text": "hyperSpec is an R package for working with hyperspectral data sets. Hyperspectral data can take many forms, but a common application is a series of spectra collected over an x, y grid, for instance Raman imaging of medical specimens. hyperSpec was originally written by Claudia Beleites and she currently guides a core group of contributors.1\nClaudia, regular hyperSpec contributor Roman Kiselev and myself have joined forces this summer in a Google Summer of Code project to fortify hyperSpec. We are pleased to report that the project was accepted by R-GSOC administrators, and, as of a few days ago, the excellent proposal written by Erick Oduniyi was approved by Google. Erick is a senior computer engineering major at Wichita State University in Kansas. Erick gravitates toward interdisciplinary projects. This, and his experience with R, Python and related skills gives him an excellent background for this project.\nThe focus of this project is to fortify the infrastructure of hyperSpec. Over the years, keeping hyperSpec up-to-date has grown a bit unwieldy. While to-do lists always evolve, the current interrelated goals include:\nAddressing each of these goals will make hyperSpec much easier to maintain, less fragile, and easier for others to contribute. Every step will bring enhanced documentation and vignettes, along with new unit tests. Work will begin in earnest on June 1st, and we are looking forward to a very productive summer.\nFinally, on behalf of all participants, let me just say how grateful we are to Google for establishing the GSOC program and for supporting Erick’s work this summer!"
  },
  {
    "objectID": "posts/2020-05-07-GSOC-hyperSpec/2020-05-07-GSOC-hyperSpec.html#footnotes",
    "href": "posts/2020-05-07-GSOC-hyperSpec/2020-05-07-GSOC-hyperSpec.html#footnotes",
    "title": "Fortifying hyperSpec: Getting Ready for GSOC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA little history for the curious: the hyperSpec and ChemoSpec packages were written around the same time, independent of each other (~2009). Eventually, Claudia and I became aware of each other’s work, and we have collaborated in ways large and small ever since (I like working with Claudia because I always learn from her!). We have jointly mentored GSOC students twice before. One side project is hyperChemoBridge, a small package that converts hyperSpec objects into Spectra objects (the native ChemoSpec format) and vice-versa.↩︎\nThe descriptors here are Erick’s clever choice of words.↩︎"
  },
  {
    "objectID": "posts/2022-11-07-Announce-Subscribe/2022-09-26-Linear-Alg-Notes-Pt4/Linear-Alg-Notes-Pt4.html",
    "href": "posts/2022-11-07-Announce-Subscribe/2022-09-26-Linear-Alg-Notes-Pt4/Linear-Alg-Notes-Pt4.html",
    "title": "Notes on Linear Algebra Part 4",
    "section": "",
    "text": "Series: Part 1 Part 2 Part 3\nBack in Part 2 I mentioned some of the challenges of learning linear algebra. One of those challenges is making sense of all the special types of matrices one encounters. In this post I hope to shed a little light on that topic."
  },
  {
    "objectID": "posts/2022-11-07-Announce-Subscribe/2022-09-26-Linear-Alg-Notes-Pt4/Linear-Alg-Notes-Pt4.html#structure-examples",
    "href": "posts/2022-11-07-Announce-Subscribe/2022-09-26-Linear-Alg-Notes-Pt4/Linear-Alg-Notes-Pt4.html#structure-examples",
    "title": "Notes on Linear Algebra Part 4",
    "section": "Structure Examples",
    "text": "Structure Examples\nLet’s use R to construct and inspect examples of each type of matrix. We’ll use integer matrices to keep the print output nice and neat, but of course real numbers could be used as well.3 Most of these are pretty straightforward so we’ll keep comments to a minimum for the simple cases.\n\nRectangular Matrix \\(m \\times n\\)\n\nA_rect &lt;- matrix(1:12, nrow = 3) # if you give nrow,\nA_rect # R will compute ncol from the length of the data\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nNotice that R is “column major” meaning data fills the first column, then the second column and so forth.\n\n\nRow Matrix/Vector \\(1 \\times n\\)\n\nA_row &lt;- matrix(1:4, nrow = 1)\nA_row\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n\n\n\n\nColumn Matrix/Vector \\(m \\times 1\\)\n\nA_col &lt;- matrix(1:4, ncol = 1)\nA_col\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[4,]    4\n\n\nKeep in mind that to save space in a text-dense document one would often write A_col as its transpose.4\n\n\nSquare Matrix \\(n \\times n\\)\n\nA_sq &lt;- matrix(1:9, nrow = 3)\nA_sq\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n\nUpper and Lower Triangular Matrices\nCreating an upper triangular matrix requires a few more steps. Function upper.tri() returns a logical matrix which can be used as a mask to select entries. Function lower.tri() can be used similarly. Both functions have an argument diag = TRUE/FALSE indicating whether to include the diagonal.5\n\nupper.tri(A_sq, diag = TRUE)\n\n      [,1]  [,2] [,3]\n[1,]  TRUE  TRUE TRUE\n[2,] FALSE  TRUE TRUE\n[3,] FALSE FALSE TRUE\n\nA_upper &lt;- A_sq[upper.tri(A_sq)] # gives a logical matrix\nA_upper # notice that a vector is returned, not quite what might have been expected!\n\n[1] 4 7 8\n\nA_upper &lt;- A_sq # instead, create a copy to be modified\nA_upper[lower.tri(A_upper)] &lt;- 0L # assign the lower entries to zero\nA_upper\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    0    5    8\n[3,]    0    0    9\n\n\nNotice to create an upper triangular matrix we use lower.tri() to assign zeros to the lower part of an existing matrix.\n\n\nIdentity Matrix\nIf you give diag() a single value it defines the dimensions and creates a matrix with ones on the diagonal, in other words, an identity matrix.\n\nA_ident &lt;- diag(4)\nA_ident\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n\n\n\n\nDiagonal Matrix\nIf instead you give diag() a vector of values these go on the diagonal and the length of the vector determines the dimensions.\n\nA_diag &lt;- diag(1:4)\nA_diag\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    2    0    0\n[3,]    0    0    3    0\n[4,]    0    0    0    4\n\n\n\n\nSymmetric Matrices\nMatrices created by diag() are symmetric matrices, but any matrix where \\(a_{ij} = a_{ji}\\) is symmetric. There is no general function to create symmetric matrices since there is no way to know what data should be used. However, one can ask if a matrix is symmetric, using the function isSymmetric().\n\nisSymmetric(A_diag)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/2022-11-07-Announce-Subscribe/2022-09-26-Linear-Alg-Notes-Pt4/Linear-Alg-Notes-Pt4.html#the-queries",
    "href": "posts/2022-11-07-Announce-Subscribe/2022-09-26-Linear-Alg-Notes-Pt4/Linear-Alg-Notes-Pt4.html#the-queries",
    "title": "Notes on Linear Algebra Part 4",
    "section": "The Queries",
    "text": "The Queries\nLet’s take the queries in the taxonomy in order, as the hierarchy is everything.\n\nIs the Matrix Singular or Invertible?\nA singular matrix is one in which one or more rows are multiples of another row, or alternatively, one or more columns are multiples of another column. Why do we care? Well, it turns out a singular matrix is a bit of a dead end, you can’t do much with it. An invertible matrix, however, is a very useful entity and has many applications. What is an invertible matrix? In simple terms, being invertible means the matrix has an inverse. This is not the same as the algebraic definition of an inverse, which is related to division:\n\\[\nx^{-1} = \\frac{1}{x}\n\\tag{1}\\]\nInstead, for matrices, invertibility of \\(\\mathbf{A}\\) is defined as the existence of another matrix \\(\\mathbf{B}\\) such that\n\\[\n\\mathbf{A}\\mathbf{B} = \\mathbf{B}\\mathbf{A} = \\mathbf{I}\n\\tag{2}\\]\nJust as \\(x^{-1}\\) cancels out \\(x\\) in \\(x^{-1}x = \\frac{x}{x} = 1\\), \\(\\mathbf{B}\\) cancels out \\(\\mathbf{A}\\) to give the identity matrix. In other words, \\(\\mathbf{B}\\) is really \\(\\mathbf{A}^{-1}\\).\nA singular matrix has determinant of zero. On the other hand, an invertible matrix has a non-zero determinant. So to determine which type of matrix we have before us, we can simply compute the determinant.\nLet’s look at a few simple examples.\n\nA_singular &lt;- matrix(c(1, -2, -3, 6), nrow = 2, ncol = 2)\nA_singular # notice that col 2 is col 1 * -3, they are not independent\n\n     [,1] [,2]\n[1,]    1   -3\n[2,]   -2    6\n\ndet(A_singular)\n\n[1] 0\n\n\n\nA_invertible &lt;- matrix(c(2, 2, 7, 8), nrow = 2, ncol = 2)\nA_invertible\n\n     [,1] [,2]\n[1,]    2    7\n[2,]    2    8\n\ndet(A_invertible)\n\n[1] 2\n\n\n\n\nIs the Matrix Diagonalizable?\nA matrix \\(\\mathbf{A}\\) that is diagonalizable can be expressed as:\n\\[\n\\mathbf{\\Lambda} = \\mathbf{X}^{-1}\\mathbf{A}\\mathbf{X}\n\\tag{3}\\]\nwhere \\(\\mathbf{\\Lambda}\\) is a diagonal matrix – the diagonalized version of the original matrix \\(\\mathbf{A}\\). How do we find out if this is possible, and if possible, what are the values of \\(\\mathbf{X}\\) and \\(\\mathbf{\\Lambda}\\)? The answer is to decompose \\(\\mathbf{A}\\) using the eigendecomposition:\n\\[\n\\mathbf{A} = \\mathbf{X}\\mathbf{\\Lambda}\\mathbf{X}^{-1}\n\\tag{4}\\]\nNow there is a lot to know about the eigendecomposition, but for now let’s just focus on a few key points:\n\nThe columns of \\(\\mathbf{X}\\) contains the eigenvectors. Eigenvectors are the most natural basis for describing the data in \\(\\mathbf{A}\\).6\n\\(\\mathbf{\\Lambda}\\) is a diagonal matrix with the eigenvalues on the diagonal, in descending order. The individual eigenvalues are typically denoted \\(\\lambda_i\\).\nEigenvectors and eigenvalues always come in pairs.\n\nWe can answer the original question by using the eigen() function in R. Let’s do an example.\n\nA_eigen &lt;- matrix(c(1, 0, 2, 2, 3, -4, 0, 0, 2), ncol = 3)\nA_eigen\n\n     [,1] [,2] [,3]\n[1,]    1    2    0\n[2,]    0    3    0\n[3,]    2   -4    2\n\neA &lt;- eigen(A_eigen)\neA\n\neigen() decomposition\n$values\n[1] 3 2 1\n\n$vectors\n           [,1] [,2]       [,3]\n[1,]  0.4082483    0  0.4472136\n[2,]  0.4082483    0  0.0000000\n[3,] -0.8164966    1 -0.8944272\n\n\nSince eigen(A_eigen) was successful, we can conclude that A_eigen was diagonalizable. You can see the eigenvalues and eigenvectors in the returned value. We can reconstruct A_eigen using Equation 4:\n\neA$vectors %*% diag(eA$values) %*% solve(eA$vectors)\n\n     [,1] [,2] [,3]\n[1,]    1    2    0\n[2,]    0    3    0\n[3,]    2   -4    2\n\n\nRemember, diag() creates a matrix with the values along the diagonal, and solve() computes the inverse when it gets only one argument.\nThe only loose end is which matrices are not diagonalizable? These are covered in this Wikipedia article. Briefly, most non-diagonalizable matrices are fairly exotic and real data sets will likely not be a problem.\n\n\n\n\n\n\nNuances About the Presentation of “Eigenstuff”\n\n\n\n\n\nIn texts, eigenvalues and eigenvectors are universally introduced as a scaling relationship\n\\[\n\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\n\\tag{5}\\]\nwhere \\(\\mathbf{v}\\) is a column eigenvector and \\(\\lambda\\) is a scalar eigenvalue. One says “\\(\\mathbf{A}\\) scales \\(\\mathbf{v}\\) by a factor of \\(\\lambda\\).” A single vector is used as one can readily illustrate how that vector grows or shrinks in length when multiplied by \\(\\lambda\\). Let’s call this the “bottom up” explanation.\nLet’s check that is true using our values from above by extracting the first eigenvector and eigenvalue from eA. Notice that we are using regular multiplication on the right-hand-side, i.e. *, rather than %*%, because eA$values[1] is a scalar. Also on the right-hand-side, we have to add drop = FALSE to the subsetting process or the result is no longer a matrix.7\n\nisTRUE(all.equal(\n  A_eigen %*% eA$vectors[,1],\n  eA$values[1] * eA$vectors[,1, drop = FALSE]))\n\n[1] TRUE\n\n\nIf instead we start from Equation 4 and rearrange it to show the relationship between \\(\\mathbf{A}\\) and \\(\\mathbf{\\Lambda}\\) we get:\n\\[\n\\mathbf{A}\\mathbf{X} = \\mathbf{X}\\mathbf{\\Lambda}\n\\tag{6}\\]\nLet’s call this the “top down” explanation. We can verify this as well, making sure to convert eA$values to a diagonal matrix as the values are stored as a vector to save space.\n\nisTRUE(all.equal(A_eigen %*% eA$vectors, eA$vectors %*% diag(eA$values)))\n\n[1] TRUE\n\n\nNotice that in Equation 6 \\(\\Lambda\\) is on the right of \\(\\mathbf{X}\\), but in Equation 5 the corresponding value, \\(\\lambda\\), is to the left of \\(\\mathbf{v}\\). This is a bit confusing until one realizes that Equation 5 could have been written\n\\[\n\\mathbf{A}\\mathbf{v} = \\mathbf{v}\\lambda\n\\]\nsince \\(\\lambda\\) is a scalar. It’s too bad that the usual, bottom up, presentation seems to conflict with the top down approach. Perhaps the choice in Equation 5 is a historical artifact.\n\n\n\n\n\nIs the Matrix Normal?\nA normal matrix is one where \\(\\mathbf{A}^{\\mathsf{T}}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{\\mathsf{T}}\\). As far as I know, there is no function in R to check this condition, but we’ll write our own in a moment. One reason being “normal” is interesting is if \\(\\mathbf{A}\\) is a normal matrix, then the results of the eigendecomposition change slightly:\n\\[\n\\mathbf{A} = \\mathbf{O}\\mathbf{\\Lambda}\\mathbf{O}^{-1}\n\\tag{7}\\]\nwhere \\(\\mathbf{O}\\) is an orthogonal matrix, which we’ll talk about next.\n\n\nIs the Matrix Orthogonal?\nAn orthogonal matrix takes the definition of a normal matrix one step further: \\(\\mathbf{A}^{\\mathsf{T}}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{\\mathsf{T}} = \\mathbb{I}\\). If a matrix is orthogonal, then its transpose is equal to its inverse: \\(\\mathbf{A}^{-1} = \\mathbf{A}^{\\mathsf{T}}\\), which of course makes any special computation of the inverse unnecessary. This is a significant advantage in computations.\nTo aid our learning, let’s write a simple function that will report if a matrix is normal, orthogonal, or neither.8\n\nnormal_or_orthogonal &lt;- function(M) {\n  if (!inherits(M, \"matrix\")) stop(\"M must be a matrix\")\n  norm &lt;- orthog &lt;- FALSE\n  tst1 &lt;- M %*% t(M)\n  tst2 &lt;- t(M) %*% M\n  norm &lt;- isTRUE(all.equal(tst1, tst2))\n  if (norm) orthog &lt;- isTRUE(all.equal(tst1, diag(dim(M)[1])))\n  if (orthog) message(\"This matrix is orthogonal\\n\") else \n    if (norm) message(\"This matrix is normal\\n\") else\n    message(\"This matrix is neither orthogonal nor normal\\n\")\n  invisible(NULL)\n}\n\nAnd let’s run a couple of tests.\n\nnormal_or_orthogonal(A_singular)\n\nThis matrix is neither orthogonal nor normal\n\nNorm &lt;- matrix(c(1, 0, 1, 1, 1, 0, 0, 1, 1), nrow = 3)\nnormal_or_orthogonal(Norm)\n\nThis matrix is normal\n\nnormal_or_orthogonal(diag(3)) # the identity matrix is orthogonal\n\nThis matrix is orthogonal\n\nOrth &lt;- matrix(c(0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0), nrow = 4)\nnormal_or_orthogonal(Orth)\n\nThis matrix is orthogonal\n\n\n\n\n\n\n\n\nSome other properties of an orthogonal matrix\n\n\n\n\n\nThe columns of an orthogonal matrix are orthogonal to each other. We can show this by taking the dot product between any pair of columns. Remember is the dot product is zero the vectors are orthogonal.\n\nt(Orth[,1]) %*% Orth[,2] # col 1 dot col 2\n\n     [,1]\n[1,]    0\n\nt(Orth[,1]) %*% Orth[,3] # col 1 dot col 3\n\n     [,1]\n[1,]    0\n\n\nFinally, not only are the columns orthogonal, but each column vector has length one, making them orthonormal.\n\nsqrt(sum(Orth[,1]^2))\n\n[1] 1\n\n\n\n\n\n\n\nAppreciating the Queries\nTaking these queries together, we see that symmetric and diagonal matrices are necessarily invertible, diagonalizable and normal. They are not however orthogonal. Identity matrices however, have all these properties. Let’s double-check these statements.\n\nA_sym &lt;- matrix(\n  c(1, 5, 4, 5, 2, 9, 4, 9, 3),\n  ncol = 3) # symmetric matrix, not diagonal\nA_sym\n\n     [,1] [,2] [,3]\n[1,]    1    5    4\n[2,]    5    2    9\n[3,]    4    9    3\n\nnormal_or_orthogonal(A_sym)\n\nThis matrix is normal\n\nnormal_or_orthogonal(diag(1:3)) # diagonal matrix, symmetric, but not the identity matrix\n\nThis matrix is normal\n\nnormal_or_orthogonal(diag(3)) # identity matrix (also symmetric, diagonal)\n\nThis matrix is orthogonal\n\n\nSo what’s the value of these queries? As mentioned, they help us understand the relationships between different types of matrices, so they help us learn more deeply. On a practical computational level they may not have much value, especially when dealing with real-world data sets. However, there are some other interesting aspects of these queries that deal with decompositions and eigenvalues. We might cover these in the future."
  },
  {
    "objectID": "posts/2022-11-07-Announce-Subscribe/2022-09-26-Linear-Alg-Notes-Pt4/Linear-Alg-Notes-Pt4.html#footnotes",
    "href": "posts/2022-11-07-Announce-Subscribe/2022-09-26-Linear-Alg-Notes-Pt4/Linear-Alg-Notes-Pt4.html#footnotes",
    "title": "Notes on Linear Algebra Part 4",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m only using a portion because the Hiranbe’s original contains a bit too much information for someone trying to get their footing in the field.↩︎\nI’m using the term taxonomy a little loosely of course, you can call it whatever you want. The name is not so important really, what is important is the hierarchy of concepts.↩︎\nAs could complex numbers.↩︎\nUsually in written text a row matrix, sometimes called a row vector, is written as \\(\\mathbf{x} = \\begin{bmatrix}1 & 2 & 3\\end{bmatrix}\\). In order to save space in documents, rather than writing \\(\\mathbf{x} = \\begin{bmatrix}1 \\\\ 2 \\\\ 3\\end{bmatrix}\\), a column matrix/vector can be kept to a single line by writing it as its transpose: \\(\\mathbf{x} = \\begin{bmatrix}1 & 2 & 3\\end{bmatrix}^{\\mathsf{T}}\\), but this requires a little mental gymnastics to visualize.↩︎\nUpper and lower triangular matrices play a special role in linear algebra. Because of the presence of many zeros, multiplying them and inverting them is relatively easy, because the zeros cause terms to drop out.↩︎\nThis idea of the “most natural basis” is most easily visualized in two dimensions. If you have some data plotted on \\(x\\) and \\(y\\) axes, determining the line of best fit is one way of finding the most natural basis for describing the data. However, more generally and in more dimensions, principal component analysis (PCA) is the most rigorous way of finding this natural basis, and PCA can be calculated with the eigen() function. Lots more information here.↩︎\nThe drop argument to subsetting/extracting defaults to TRUE which means that if subsetting reduces the necessary number of dimensions, the unneeded dimension attributes are dropped. Under the default, selecting a single column of a matrix leads to a vector, not a one column vector. In this all.equal() expression we need both sides to evaluate to a matrix.↩︎\nOne might ask why R does not provide a user-facing version of such a function. I think a good argument can be made that the authors of R passed down a robust and lean set of linear algebra functions, geared toward getting work done, and throwing errors as necessary.↩︎"
  },
  {
    "objectID": "posts/2020-01-02-readJDX-update/2020-01-02-readJDX-update.html",
    "href": "posts/2020-01-02-readJDX-update/2020-01-02-readJDX-update.html",
    "title": "readJDX Overhaul",
    "section": "",
    "text": "readJDX reads files in the JCAMP-DX format used in the field of spectroscopy. A recent overhaul has made it much more robust, and as such the version is now at 0.4.29.1 Most of the changes were internal, but three important user-facing changes are:\nYou can see more about the package here. As always, if you use the package and have troubles, please file an issue. The JCAMP-DX standard is challenging and vendors have a lot of flexibility, so please do share any problematic files you encounter."
  },
  {
    "objectID": "posts/2020-01-02-readJDX-update/2020-01-02-readJDX-update.html#footnotes",
    "href": "posts/2020-01-02-readJDX-update/2020-01-02-readJDX-update.html#footnotes",
    "title": "readJDX Overhaul",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe current version also includes changes in an unreleased version (0.3.372) in which several bugs were squashed.↩︎"
  },
  {
    "objectID": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html",
    "href": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html",
    "title": "Simulating Spectroscopic Data Part 1",
    "section": "",
    "text": "It is well-recognized that one of the virtues of the R language is the extensive tools it provides for working with distributions. Functions exist to generate random number draws, determine quantiles, and examine the probability density and cumulative distribution curves that describe each distribution.\nThis toolbox gives one the ability to create simulated data sets for testing very easily. If you need a few random numbers from a Gaussian distribution then rnorm is your friend:\nrnorm(3)\n\n[1] -1.67497913 -1.49447605 -0.02394601\nImagine you were developing a new technique to determine if two methods of manufacturing widgets produced widgets of the same mass.1 Even before the widgets were manufactured, you could test your code by simulating widget masses using rnorm:\nwidget_1_masses &lt;- rnorm(100, 5.0, 0.5) # mean mass 5.0\nwidget_2_masses &lt;- rnorm(100, 4.5, 0.5) # mean mass 4.5\nVariations on this approach can be used to simulate spectral data sets.2 The information I will share here is accumulated knowledge. I have no formal training in the theory behind the issues discussed, just skills I have picked up in various places and by experimenting. If you see something that is wrong or needs clarification or elaboration, please use the comments to set me straight!"
  },
  {
    "objectID": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html#peak-shapes",
    "href": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html#peak-shapes",
    "title": "Simulating Spectroscopic Data Part 1",
    "section": "Peak Shapes",
    "text": "Peak Shapes\nWhat peak shape is expected for a given type of spectroscopy? In principle this is based on the theory behind the method, either some quantum mechanical model or an approximation of it. For some methods, like NMR, this might be fairly straightforward, at least in simple systems. But the frequencies involved in some spectroscopies not too different from others, and coupling is observed. Two examples which “interfere” with each other are:\n\nElectronic transitions in UV spectra which are broadened by interactions with vibrational states.\nVibrational transitions in IR spectroscopy (bonds stretching and bond angles bending in various ways) are coupled to electronic transitions.\n\nAfter theoretical considerations, we should keep in mind that all spectroscopies have some sort of detector, electronic components and basic data processing that can affect peak shape. A CCD on a UV detector is one of the simpler situations. FT-IR has a mechanical interferometer, and the raw signal from both IR and NMR is Fourier-transformed prior to use. So there are not only theoretical issues to think about, but also engineering, instrument tuning, electrical engineering and mathematical issues to consider.\nEven with myriad theoretical and practical considerations, a Gaussian curve is a good approximation to a simple peak, and more complex peaks can be built by summing Gaussian curves. If we want to simulate a simple peak with a Gaussian shape, we can use the dnorm function, which gives us the “density” of the distribution:\n\nstd_deviations &lt;- seq(-5, 5, length.out = 100)\nGaussian_1 &lt;- dnorm(std_deviations)\nplot(std_deviations, Gaussian_1, type = \"l\",\n  xlab = \"standard deviations\", ylab = \"Gaussian Density\")\n\n\n\n\n\n\n\n\nIf we want this to look more like a “real” peak, we can increase the x range and use x values with realistic frequency values. And if we want our spectrum to be more complex, we can add several of these curves together. Keep in mind that the area under the density curve is 1.0, and the peak width is determined by the value of argument sd (the standard deviation). For example if you want to simulate the UV spectrum of vanillin, which has maxima at about 230, 280 and 315 nm, one can do something along these lines:\n\nwavelengths &lt;- seq(220, 350, by = 1.0)\nPeak1 &lt;- dnorm(wavelengths, 230, 22)\nPeak2 &lt;- dnorm(wavelengths, 280, 17)\nPeak3 &lt;- dnorm(wavelengths, 315, 17)\nPeaks123 &lt;- colSums(rbind(1.6 * Peak1, Peak2, Peak3))\nplot(wavelengths, Peaks123, type = \"l\",\n  xlab = \"wavelengths (nm)\", ylab = \"arbitrary intensity\")\n\n\n\n\n\n\n\n\nThe coefficient on Peak1 is needed to increase the contribution of that peak in order to better resemble the linked spectrum (note that the linked spectrum y-axis is \\(log \\epsilon\\); we’re just going for a rough visual approximation).\nIt’s a simple, if tedious, task to add Gaussian curves in this manner to simulate a single spectrum. One can also create several different spectra, and then combine them in various ratios to create a data set representing samples composed of mixtures of compounds. UV spectra are tougher due to the vibrational coupling; NMR spectra are quite straightforward since we know the area of each magnetic environment in the structure (but we also have to deal with doublets etc.). If you plan to do a lot of this, take a look at the SpecHelpers package, which is designed to streamline these tasks.\nA relatively minor exception to the typical Gaussian peak shape is NMR. Peaks in NMR are typically described as “Lorentzian”, which corresponds to the Cauchy distribution (Goldenberg 2016). This quick comparison shows that NMR peaks are expected to be less sharp and have fatter tails:\n\nGaussian_1 &lt;- dnorm(std_deviations)\nCauchy_1 &lt;- dcauchy(std_deviations)\nplot(std_deviations, Gaussian_1, type = \"l\",\n  xlab = \"standard deviations\", ylab = \"density\")\nlines(std_deviations, Cauchy_1, col = \"red\")"
  },
  {
    "objectID": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html#baselines",
    "href": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html#baselines",
    "title": "Simulating Spectroscopic Data Part 1",
    "section": "Baselines",
    "text": "Baselines\nFor many types of spectroscopies there is a need to correct the baseline when processing the data. But if you are simulating spectroscopic (or chromatographic) data, how can you introduce baseline anomalies? Such anomalies can take many forms, for instance a linear dependence on wavelength (i.e. a steadily rising baseline without curvature). But more often one sees complex rolling baseline issues.\nLet’s play with introducing different types of baseline abberations. First, let’s create a set of three simple spectra. We’ll use a simple function to scale the set of spectra so the range is on the interval [0…1] for ease of further manipulations.\n\nwavelengths &lt;- 200:800\nSpec1 &lt;- dnorm(wavelengths, 425, 30)\nSpec2 &lt;- dnorm(wavelengths, 550, 20) * 3 # boost the area\nSpec3 &lt;- dnorm(wavelengths, 615, 15)\nSpec123 &lt;- rbind(Spec1, Spec2, Spec3)\ndim(Spec123) # matrix with samples in rows\n\n[1]   3 601\n\n\n\nscale01 &lt;- function(M) {\n  # scales the range of the matrix to [0...1]\n  mn &lt;- min(M)\n  M &lt;- M - mn\n  mx &lt;- max(M)\n  M &lt;- M/mx\n}\n\nHere are the results; the dotted line is the sum of the three spectra, offset vertically for ease of comparison.\n\nSpec123 &lt;- scale01(Spec123)\nplot(wavelengths, Spec123[1,], col = \"black\", type = \"l\",\n  xlab = \"wavelength (nm)\", ylab = \"intensity\",\n  ylim = c(0, 1.3))\nlines(wavelengths, Spec123[2,], col = \"red\")\nlines(wavelengths, Spec123[3,], col = \"blue\")\nlines(wavelengths, colSums(Spec123) + 0.2, lty = 2)\n\n\n\n\n\n\n\n\nOne clever way to introduce baseline anomalies is to use a Vandermonde matrix. This is a trick I learned while working with the team on the hyperSpec overhaul funded by GSOC.3 It’s easiest to explain by an example:\n\nvander &lt;- function(x, order) outer(x, 0:order, `^`)\nvdm &lt;- vander(wavelengths, 2)\ndim(vdm)\n\n[1] 601   3\n\nvdm[1:5, 1:3]\n\n     [,1] [,2]  [,3]\n[1,]    1  200 40000\n[2,]    1  201 40401\n[3,]    1  202 40804\n[4,]    1  203 41209\n[5,]    1  204 41616\n\nvdm &lt;- scale(vdm, center = FALSE, scale = c(1, 50, 2000))\n\nLooking at the first few rows of vdm, you can see that the first column is a simple multiplier, in this case an identity vector. This can be viewed as an offset term.4 The second column contains the original wavelength values, in effect a linear term. The third column contains the square of the original wavelength values. If more terms had been requested, they would be the cubed values etc. In the code above we also scaled the columns of the matrix so that the influence of the linear and especially the squared terms don’t dominate the absolute values of the final result. Scaling does not affect the shape of the curves.\nTo use this Vandermonde matrix, we need another matrix which will function as a set of coefficients.\n\ncoefs &lt;- matrix(runif(nrow(Spec123) * 3), ncol = 3)\ncoefs\n\n           [,1]      [,2]      [,3]\n[1,] 0.81126877 0.9094830 0.2902550\n[2,] 0.15101528 0.9878546 0.3244570\n[3,] 0.05994409 0.5978804 0.9532016\n\n\nIf we multiply the coefficients by the tranposed Vandermonde matrix, we get back a set of offsets which are the rows of the Vandermonde matrix modified by the coefficients. We’ll scale things so that Spec123 and offsets are on the same overall scale and then further scale so that the spectra are not overwhelmed by the offsets in the next step.\n\noffsets &lt;- coefs %*% t(vdm)\ndim(offsets) # same dimensions as Spec123 above\n\n[1]   3 601\n\noffsets &lt;- scale01(offsets) * 0.1\n\nThese offsets can then be added to the original spectrum to obtain our spectra with a distorted baseline. Here we have summed the individual spectra. We have added a line based on extrapolating the first 20 points of the distorted data, which clearly shows the influence of the squared term.\n\nFinalSpec1 &lt;- offsets + Spec123\nplot(wavelengths, colSums(FinalSpec1), type = \"l\", col = \"red\",\n  xlab = \"wavelength (nm)\", ylab = \"intensity\")\nlines(wavelengths, colSums(Spec123))\nfit &lt;- lm(colSums(FinalSpec1)[1:20] ~ wavelengths[1:20])\nlines(wavelengths, fit$coef[2]*wavelengths + fit$coef[1],\n  col = \"red\", lty = 2) # good ol' y = mx + b\n\n\n\n\n\n\n\n\nThe Vandermonde matrix approach works by creating offsets that are added to the original spectrum. However, it is limited to creating baseline distortions that generally increase at higher values. To create other types of distortions, you can use your imagination. For instance, you could reverse the order of the rows of offsets and/or use higher terms, scale a row, etc. One could also play with various polynomial functions to create the desired effect over the wavelength range of interest. For instance, the following code adds a piece of an inverted parabola to the original spectrum to simulate a baseline hump.\n\nhump &lt;- -1*(15*(wavelengths - 450))^2 # piece of a parabola\nhump &lt;- scale01(hump)\nFinalSpec2 &lt;- hump * 0.1 + colSums(Spec123)\nplot(wavelengths, FinalSpec2, type = \"l\",\n  xlab = \"wavelengths (nm)\", ylab = \"intensity\")\nlines(wavelengths, hump * 0.1, lty = 2) # trace the hump\n\n\n\n\n\n\n\n\nIn the plot, the dotted line traces out the value of hump * 0.1, the offset.\nIn the next post we’ll look at ways to introduce noise into simulated spectra."
  },
  {
    "objectID": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html#footnotes",
    "href": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html#footnotes",
    "title": "Simulating Spectroscopic Data Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOf course, this is simply the t-test.↩︎\nFor that matter, you can also simulate chromatograms using the methods we are about to show. It’s even possible to introduce tailing of a peak. For a function to do this, see the SpecHelpers package.↩︎\nThe work I’m showing here is based on original code in package hyperSpec by Claudia Belietes.↩︎\nAs a vector of 1’s it will have no effect on the calculations to come. However, you could multiply this column by a value to add an offset to your simulated spectra. This would be a means of simulating a steady electronic bias in an instrument’s raw data.↩︎"
  },
  {
    "objectID": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html",
    "href": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html",
    "title": "Aligning 2D NMR Spectra Part 3",
    "section": "",
    "text": "This is Part 3 of a series on aligning 2D NMR, as implemented in the package ChemoSpec2D. Part 1 Part2\nLet’s get to work. The function to carry out alignment is hats_alignSpectra2D. The arguments maxF1 and maxF2 define the space that will be considered as the two spectra are shifted relative to each other. The space potentially covered is -maxF1 to maxF1 and similarly for the F2 dimension. dist_method, thres and minimize refer to the objective function, as described in Part 1. In this example we will consider two spectra succcessfully aligned when we get below the threshold. When one shifts one spectrum relative to the other, part of the shifted spectrum gets cutoff and part of it is empty space. fill = \"noise\" instructs the function to fill the empty space with an estimate of the noise from the original spectrum. We’ll set plot = FALSE here because the output is extensive. I’ll provide sample plotting output in a moment.\nlibrary(\"ChemoSpec2D\")\n## Loading required package: ChemoSpecUtils\n## \n## As of version 6, ChemoSpec2D offers new graphics output options\n## \n## Functions plotScores and plotScree will work with the new options\n## For details, please see ?GraphicsOptions\n## \n## The ChemoSpec graphics option is set to 'ggplot2'\n## To change it, do\n##  options(ChemoSpecGraphics = 'option'),\n##  where 'option' is one of 'base' or 'ggplot2' or'plotly'.\ndata(MUD2)\nset.seed(123)\nMUD2a &lt;- hats_alignSpectra2D(MUD2,\n    maxF1 = 5, maxF2 = 5,\n    dist_method = \"euclidean\", thres = 40, minimize = TRUE,\n    fill = \"noise\",\n    plot = FALSE)\n\nThis is a beta version of hats_alignSpectra2D.\n    You should set the seed for reproducible results.\n    Please check your results carefully, and consider sharing your data\n    for additional testing.  Contact Bryan Hanson via hanson@depauw.edu\n\n\n[ChemoSpec2D] Processing row  1  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 7 \n    with sample(s) 4 \n[ChemoSpec2D] Best alignment is to shift F2 by  0  and F1 by  -1 \n\n[ChemoSpec2D] Processing row  2  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 6 \n    with sample(s) 3 \n[ChemoSpec2D] Best alignment is to shift F2 by  0  and F1 by  -1 \n\n[ChemoSpec2D] Processing row  3  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 5 \n    with sample(s) 2 \n[ChemoSpec2D] Best alignment is to shift F2 by  0  and F1 by  -1 \n\n[ChemoSpec2D] Processing row  4  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 8 \n    with sample(s) 1 \n[ChemoSpec2D] Best alignment is to shift F2 by  0  and F1 by  -1 \n\n[ChemoSpec2D] Processing row  5  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 1, 8 \n    with sample(s) 9 \n[ChemoSpec2D] Best alignment is to shift F2 by  2  and F1 by  1 \n\n[ChemoSpec2D] Processing row  6  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 2, 5 \n    with sample(s) 3, 6 \n[ChemoSpec2D] Best alignment is to shift F2 by  2  and F1 by  0 \n\n[ChemoSpec2D] Processing row  7  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 4, 7 \n    with sample(s) 10 \n[ChemoSpec2D] Best alignment is to shift F2 by  0  and F1 by  3 \n\n[ChemoSpec2D] Processing row  8  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 2, 3, 5, 6 \n    with sample(s) 1, 8, 9 \n[ChemoSpec2D] Best alignment is to shift F2 by  0  and F1 by  3 \n\n[ChemoSpec2D] Processing row  9  of  9  from the guide tree:\n[ChemoSpec2D] Starting alignment of sample(s) 1, 2, 3, 5, 6, 8, 9 \n    with sample(s) 4, 7, 10 \n[ChemoSpec2D] Best alignment is to shift F2 by  -5  and F1 by  0 \n\n[ChemoSpec2D] Alignment steps and results:\n       Ref                Mask F2shift F1shift\n1        4                   7       0      -1\n2        3                   6       0      -1\n3        2                   5       0      -1\n4        1                   8       0      -1\n5        9                1, 8       2       1\n6     3, 6                2, 5       2       0\n7       10                4, 7       0       3\n8  1, 8, 9          2, 3, 5, 6       0       3\n9 4, 7, 10 1, 2, 3, 5, 6, 8, 9      -5       0\nAs the alignment proceeds, updates from the function are prefixed with [ChemoSpec2D]. In the first step we get a message that row 1 of 9 of the guide tree is being processed, in which sample 7 is being aligned with sample 4. The guide tree is shown below. One can see that samples 7 and 4 are very similar, so they are aligned first. If you inspect the output above, you can see that the four most similar pairs of spectra are aligned first, followed by groups of spectra according to similarity. For each alignment the needed shifts are reported. The last part of the output is a summary of all the alignments carried out. Note that the vertical scale on the guide tree is the same as the scale on the sampleDist plot in Part 1 (using the Euclidean distance)."
  },
  {
    "objectID": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html#diagnostics-on-space",
    "href": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html#diagnostics-on-space",
    "title": "Aligning 2D NMR Spectra Part 3",
    "section": "Diagnostics on Space",
    "text": "Diagnostics on Space\nTo save space, I suppressed the plotting of the results. However, there are plots! In fact there is a set of plots for each alignment step. Here are two of the plots produced if plot = TRUE; these deal with the X-Space which is the search space (the terminology comes from the mlrMBO package which is designed to handle many types of optimization). This plot is for Step 7. The upper plot shows the search space. Axis x1 corresponds to the F1 dimension, and axis x2 the F2 dimension. The red squares represent the initial experimental design, using the results from the objective function. The blue circles represent additional points added as the search proceeds. These represent new points on the response surface defined by the surrogate function (see Part 2 for background). The orange diamond is the best alignment, which in this case has no shift along F2 but a three data point shift along F1; this corresponds to the output above. The green triangle is the last position tested.\nThe lower plot represents the progress of the search over time. Axis “dob” stands for “date of birth” which is basically the time index of when the test point was added."
  },
  {
    "objectID": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html#diagnostics-on-the-objective-function",
    "href": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html#diagnostics-on-the-objective-function",
    "title": "Aligning 2D NMR Spectra Part 3",
    "section": "Diagnostics on the Objective Function",
    "text": "Diagnostics on the Objective Function\nThis second set of plots deals with what mlrMBO considers the Y-Space, which concerns the values of the objective function. The top plot is a histogram of the distance (objective function) values; in this case most of them were pretty bad (high, meaning a larger distance between the spectra). The middle plot is the value of the distance over time (dob). In this example the optimal alignment is found at dob = 4, but there is no particular significance to when the optimum is found. The lower plot shows the expected improvement (ei) at each dob. It is lowest when the optimum has been found. For more details about what’s going on under the hood, see the Arxiv paper."
  },
  {
    "objectID": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html#the-aligned-spectra",
    "href": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html#the-aligned-spectra",
    "title": "Aligning 2D NMR Spectra Part 3",
    "section": "The Aligned Spectra",
    "text": "The Aligned Spectra\nDid this process work? This final plot shows that it did. Let’s be clear that the task here was not terribly hard: MUD2 is an artificial example in which the shifts are pretty modest and global in nature. But still, it’s satisfying. I welcome everyone to give hats_alignSpectra2D a try and report any problems or suggestions.\n\nmylvls &lt;- seq(0, 30, length.out = 10)\nplotSpectra2D(MUD2a, which = c(1, 6), showGrid = TRUE,\n    lvls = LofL(mylvls, 2),\n    cols = LofC(c(\"red\", \"black\"), 2, length(mylvls), 2),\n  main = \"Aligned MUD2 Spectra 1 & 6\")"
  },
  {
    "objectID": "posts/2021-10-13-GSOC-CS-Graphics/2021-10-13-GSOC-CS-Graphics.html",
    "href": "posts/2021-10-13-GSOC-CS-Graphics/2021-10-13-GSOC-CS-Graphics.html",
    "title": "GSOC 2021: New Graphics for ChemoSpec",
    "section": "",
    "text": "It’s been quiet around this blog because supervising two students for Google Summer of Code has kept me pretty busy! But we have some news…\n\n\n\nThanks to Mr. Tejasvi Gupta and the support of GSOC, ChemoSpec and ChemoSpec2D were extended to produce ggplot2 graphics and plotly graphics! ggplot2 is now the default output, and the ggplot2 object is returned, so if one doesn’t like the choice of theme or any other aspect, one can customize the object to one’s desire. The ggplot2 graphics output are generally similar in layout and spirit to the base graphics output, but significant improvements have been made in labeling data points using the ggrepel package. The original base graphics are still available as well. Much of this work required changes in ChemoSpecUtils which supports the common needs of both packages.\nTejasvi did a really great job with this project, and I think users of these packages will really like the results. We have greatly expanded the pre-release testing of the graphics, and as far as we can see every thing works as intended. Of course, please file an issue if you see any problems or unexpected behavior.\nTo see more about how the new graphics options work, take a look at GraphicsOptions. Here are the functions that were updated:\n\nplotSpectra\nsurveySpectra\nsurveySpectra2\nreviewAllSpectra (formerly loopThruSpectra)\nplotScree (resides in ChemoSpecUtils)\nplotScores (resides in ChemoSpecUtils)\nplotLoadings (uses patchwork and hence plotly isn’t available)\nplot2Loadings\nsPlotSpectra\npcaDiag\nplotSampleDist\naovPCAscores\naovPCAloadings (uses patchwork and hence plotly isn’t available)\n\nTejasvi and I are looking forward to your feedback. There are many other smaller changes that we’ll let users discover as they work. And there’s more work to be done, but other projects need attention and I need a little rest!\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hanson2021,\n  author = {Hanson, Bryan},\n  title = {GSOC 2021: {New} {Graphics} for {ChemoSpec}},\n  date = {2021-10-13},\n  url = {http://chemospec.org/posts/2021-10-13-GSOC-CS-Graphics/2021-10-13-GSOC-CS-Graphics.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHanson, Bryan. 2021. “GSOC 2021: New Graphics for\nChemoSpec.” October 13, 2021. http://chemospec.org/posts/2021-10-13-GSOC-CS-Graphics/2021-10-13-GSOC-CS-Graphics.html."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Selected Projects",
    "section": "",
    "text": "R Software Packages\nAll my work can be seen at Github. Here are some highlights:\n\nChemoSpec: Exploratory Chemometrics for Spectroscopy\nChemoSpec2D: Exploratory Chemometrics for 2D Spectroscopy\nreadJDX: Import Data in the JCAMP-DX Format\nLearnPCA A series of vignettes explaining PCA from the very beginning.\nhyperSpec Tools for Spectroscopy. Joint project with Claudia Beleites, Vilmantas Gegzna, Erick Oduniyi, Sang Truong, and others.\nexCon: Interactive Exploration of Contour Data live demo\nSpecHelpers: Spectroscopy Related Utilities\nunmixR: Hyperspectral Unmixing with R, with Anton Belov, Conor McManus, Claudia Beleites and Simon Fuller\nHiveR: Hive Plots in 2D and 3D\nLindenmayeR: Functions to Explore L-Systems (Lindenmayer Systems)\n\n\n\nMisc Projects\n\nFOSS for Spectroscopy A collection of free and open source spectroscopy projects.\n\n\n\nFrom The Past!\nOver 32 years of teaching at DePauw University, I was honored to conduct research with a good number of very talented undergraduates. The last 15 years or so my focus was on plant metabolomics. Checkout this page presenting some of the work conducted by student researchers."
  },
  {
    "objectID": "PlantMetabolomics.html",
    "href": "PlantMetabolomics.html",
    "title": "Plant Metabolomics: Where Spectroscopy Meets Evolution & Ecology",
    "section": "",
    "text": "This page honors and makes available the research of my students. However, it is no longer being updated, as I retired in June 2018.\nIn my group, we conduct research on how plants respond to the stress of climate change. Such stress can take the form of too much heat, too much salt in the soil, or too little water. We use methods from metabolomics, chemistry and ecology, and benefit from collaboration with Prof. Dana Dudle in the Biology Department. We greatly appreciate support from DePauw’s Science Research Fellows program (now discontinued), the Faculty Development Committee, and the Mellon Foundation (through FDC).\nOur plant of choice is Portulaca oleracea, a weed more commonly known as purslane. We have chosen purslane because it is easy to grow and is interesting from a medicinal/nutritional perspective - it has the most omega-3 fatty acids of any plant. Our original objective was to determine whether purslane’s response to stress has a genetic component which also contributes to reproductive fitness. We have been able to confirm this, and we are now focussed on understanding the molecular nature of purslane’s response: What pathways are activated? What molecules are involved?\nOur approach is to blend metabolomic and ecological methods. Metabolomics is the study of an organism’s metabolites under a controlled set of conditions, in our case, normal versus stressful conditions. As far as possible, one tries to measure all the metabolites at once, in a holistic fashion, which is not an easy feat. Typically, this is done with NMR, MS, IR, or other forms of spectroscopy. We also supplement these instrumental techniques with more traditional single point chemical measurements such as antioxidant levels. From the ecological perspective, we record parameters of plant growth that represent measures of fitness, such as biomass produced, the number of flowers, and so forth. To be meaningful, we need to conduct these experiments on large numbers of plants. The resulting data sets, composed of very different sorts of measurements, represent the state of the plant under the conditions tested. We use various statistical methods to figure out which treatments have produced an interesting response, and whether those responses vary with genotype. Our statistical analyses are done with the open source computational statistical program R and in the case of spectroscopic data, with the ChemoSpec package for R, written by Bryan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following students have carried out this research. Click on their names to download the posters they prepared about their work:\n\nAcademic year 2016-2017\n\nEmma Veon\n\nSummer 2015\n\nShannon Jager\nBrian Saulnier\n\nSummer 2013\n\nKristina Mulry Kristina’s work has been published\n\nFall 2012\n\nPolly Haight\n\nSummer 2011\n\nMatt Kukurugya\nPolly Haight\nVincent Guzzetta\nMatt Keinsley\nPoster presented at the joint annual meeting of the Society for Economic Botany and The Botanical Society of America in St. Louis.\n\nSummer 2010\n\nElizabeth Botts\nMatt Keinsley\n\nSummer 2009 (The Pioneers!)\n\nTanner Miller\nCourtney Brimmer\nKelly Summers\n\n2008 (Bryan’s first metabolomics work!)\n\nPoster evaluating various brands of Saw Palmetto (Serenoa repens) by NMR and IR. Presented at the Society for Economic Botany meeting, July 2008."
  }
]