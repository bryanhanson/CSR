[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Bryan A. Hanson",
    "section": "",
    "text": "This is a summary of my professional experience related to software development projects. In prior times I taught chemistry and biochemistry for 32 years at DePauw University. If you would like a more traditional CV covering that phase, please click here. My research into plant metabolomics is featured here.\nExpertise in:"
  },
  {
    "objectID": "resume.html#positions-held",
    "href": "resume.html#positions-held",
    "title": "Bryan A. Hanson",
    "section": "Positions Held",
    "text": "Positions Held\n\n\n\n\nProfessor of Chemistry & Biochemistry, DePauw University\n\n\n   \n\n\n1986–2018"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Bryan A. Hanson",
    "section": "Education",
    "text": "Education\n\n\n\n\nPost-doctoral Associate, Oregon State University\n\n\n   \n\n\n1984–1986\n\n\n\n\nPh.D., Chemistry, University of California Los Angeles, California\n\n\n   \n\n\nSeptember 1984\n\n\n\n\nB.S., Biochemistry, California State University Los Angeles, California\n\n\n   \n\n\nOctober 1981"
  },
  {
    "objectID": "resume.html#r-software-packages-authored",
    "href": "resume.html#r-software-packages-authored",
    "title": "Bryan A. Hanson",
    "section": "R Software Packages Authored",
    "text": "R Software Packages Authored\nAll my work can be seen at Github. Here are some highlights:\n\nChemoSpec: Exploratory Chemometrics for Spectroscopy\nChemoSpec2D: Exploratory Chemometrics for 2D Spectroscopy\nLearnPCA A series of vignettes explaining PCA from the very beginning.\nexCon: Interactive Exploration of Contour Data live demo\nreadJDX: Import Data in the JCAMP-DX Format\nSpecHelpers: Spectroscopy Related Utilities\nunmixR: Hyperspectral Unmixing with R, with Anton Belov, Conor McManus, Claudia Beleites and Simon Fuller\nHiveR: Hive Plots in 2D and 3D\nLindenmayeR: Functions to Explore L-Systems (Lindenmayer Systems)"
  },
  {
    "objectID": "resume.html#selected-presentations",
    "href": "resume.html#selected-presentations",
    "title": "Bryan A. Hanson",
    "section": "Selected Presentations",
    "text": "Selected Presentations\n\n“Development of Chemometric Tools for 2D NMR Data Sets” Poster at PANIC 2018 (Practical Applications of NMR in Industry Conference), La Jolla (San Diego) California, March 2018. Download\n“Using R to Make Sense of NMR Data Sets” invited talk at PANIC 2017 (Practical Applications of NMR in Industry Conference), Hilton Head South Carolina, February 2017. Download\n“unmixR: Hyperspectral Unmixing in ” with Conor McManus, Simon Fuller, and Claudia Beleites. Poster at 2014, University of California at Los Angeles, June 30–July 3, 2014. Download\n“Preliminary Metabolic Investigation of Saline-Stressed Portulac olercea using 1H NMR” with Paulina J. Haight and John S. Harwood. Poster at the ACS National Meeting, Indianpolis, September 2013. Download\n“HiveR: 2 and 3D Hive Plots of Networks” Invited talk at useR! 2012 Conference, Nashville Tennessee, June 12–15, 2012. Download\n“HiveR: 2 and 3D Hive Plots of Networks” Poster at useR! 2012 Conference, Nashville Tennessee, June 12–15, 2012. Download\n“Implementation of ANOVA-PCA in R for Multivariate Exploration” with M. J. Keinsley. Poster at useR! 2012 Conference, Nashville Tennessee, June 12–15, 2012. Download\n“The Effect of Climate Change on the Medicinal Plant Purslane Portulaca oleracea” with Elizabeth Botts, Coutney Brimmer, Tanner Miller, Kelley Summers and Dana Dudle. Presented at the 52\\(^{nd}\\) Annual Meeting of the Society for Economic Botany, July 9–13, 2011, St Louis Missouri. Download\n“ChemoSpec: an R Package for the Chemometric Analysis of Spectroscopic Data” useR! 2010 Conference, National Institute of Standards and Technology, Gaithersburg Maryland, July 2010. Download\n“Assessing Serenoa repens (Arecaceae) Quality at the Retail Level Using Spectroscopic and Chemometric Methods” with Tao Ye and M. Daniel Raftery. Presented at the 49\\(^{th}\\) Annual Meeting of the Society for Economic Botany, June 1–5, 2008, Duke University. Download"
  },
  {
    "objectID": "resume.html#miscellaneous",
    "href": "resume.html#miscellaneous",
    "title": "Bryan A. Hanson",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nGoogle Summer of Code 2021: Mentored Tejasvi Gupta (India) who adapted ChemoSpec to provide ggplot2 and plotly graphics options.\nGoogle Summer of Code 2021: Co-mentored Sang Truong (USA) who continued work on the hyperSpec package.\nGoogle Summer of Code 2020: Co-mentored Erick Oduniyi (USA) who began the work of refactoring hyperSpec into sub-packages.\nGoogle Summer of Code 2016: Co-mentored Anton Belov (Russia) in writing new functions for the unmixR package.\nGoogle Summer of Code 2013: Co-mentored Conor McManus (Ireland) in writing the unmixR package.\nI am a significant contributor to the Valid NMR Chemometrics Wiki.\nYou can see my StackExchange Activity."
  },
  {
    "objectID": "posts/2020-04-25-Heatmaps/2020-04-25-Heatmaps.html",
    "href": "posts/2020-04-25-Heatmaps/2020-04-25-Heatmaps.html",
    "title": "Spectral Heatmaps",
    "section": "",
    "text": "Most everyone is familiar with heatmaps in a general way. It’s hard not to run into them. Let’s consider some variations:\n\nA heatmap is a 2D array of rectangular cells colored by value. Generally, the rows and columns are ordered in some purposeful manner. These are very commonly encountered in microarrays for example.\nAn image is a type of heatmap in which the ordering of the rows and columns is defined spatially – it would not make sense to reorder them. This kind of data arises from the physical dimensions of a sensor, for instance the sensor on a digital camera or a raman microscope. An image might also arise by a decision to subset and present data in a “square” format. An example would be the topographic maps provided by the US government which cover a rectangular latitude/longitude range. This type of data can also be presented as a contour plot. See the examples in ?image for image and contour plots of the classic Maunga Whau volcano data, as well as an overlay of the contours on the image plot.\nA chloropleth is a map with irregular geographic boundaries and regions colored by some value. These are typically used in presenting social or political data. A chloropleth is not really a heatmap but it is often mis-characterized as one.\n\nThese three types of plots are conceptually unified in that they require a 3D data set. In the case of the heatmap and the image, the underlying data are on a regular x, y grid of values; mathematically, a matrix. The row and column indices are mapped to the x, y values, and the matrix entries are the z values. A chloropleth can be thought of as a very warped matrix where the cells are not on a regular grid but instead a series of arbitrary connected paths, namely the geographic boundaries. There is a value inside each connected path (the z value), but naturally the specification of the paths requires a completely different data structure. An intermediate type would be the cartogram heatmap described by Wilke.\n\nHeatmaps in Spectroscopy\nThe hmapSpectra function in ChemoSpec displays a heatmap to help you focus on which frequencies drive the separation of your samples.1 We’ll use the example from ?hmapSpectra which uses the built-in SrE.IR data set. This data set is a series of IR spectra of commercial Serenoa repens oils which are composed of mixtures of triglycerides and free fatty acids (see ?SrE.IR for more information). Thus the carbonyl region is of particular interest. The example narrows the frequency range to the carbonyl region for easy interpretation. Let’s look first at the spectra.\nNote: rather than link every mention of a help page in this post, remember you can see all the documentation at this site.\n\nlibrary(\"ChemoSpec\")\n\nLoading required package: ChemoSpecUtils\n\n\n\nAs of version 6, ChemoSpec offers new graphics output options\n\n\nFor details, please see ?GraphicsOptions\n\n\n\nThe ChemoSpec graphics option is set to 'ggplot2'\n\n\nTo change it, do\n    options(ChemoSpecGraphics = 'option'),\n    where 'option' is one of 'base' or 'ggplot2' or'plotly'.\n\ndata(SrE.IR) # load the data set\n# limit to the carbonyl region\nIR <- removeFreq(SrE.IR, rem.freq = SrE.IR$freq > 1775 | SrE.IR$freq < 1660)\nplotSpectra(IR, which = 1:16, lab.pos = \"none\")\n\n\n\n\nThe blue and green spectra are samples composed only of triglycerides, and hence the ester carbonyl is the primary feature. All other samples are clearly mixtures of ester and carboxylic acid stretching peaks. And now for the heatmap, using defaults:\n\nres <- hmapSpectra(IR)\n\n\n\n\nIn this default display, you’ll notice that the rows and column labels are indices to the underlying sample names and frequency list. This is not so helpful. The color scheme is not so exciting either. hmapSpectra uses the package seriation which in turn uses the heatmap.2 function in package gplots. Fortunately we can use the ... argument to pass additional arguments to heatmap.2 to get a much more useful plot.\n\n\nCustomizing the hmapSpectra Display\n\n# Label samples and frequencies by passing arguments to heatmap.2\n# Also make a few other nice plot adjustments\nres <- hmapSpectra(IR,\n  col = heat.colors(5),\n  row_labels = IR$names, col_labels = as.character(round(IR$freq)),\n  margins = c(4, 6)\n)\n\n\n\n\nThis is a lot nicer plot, since the rows are labeled with the sample names, and the columns with frequencies. Note that not every column is labeled, only every few frequencies. If you need the actual frequencies, which you probably will, they can be obtained from the returned object (res in this case; see the end of this post for an example).\n\n\nInterpreting the Plot\nHow do we interpret this plot? This is a seriated heatmap, which means the rows and columns have been re-ordered according to some algorithm (more on this in a moment). The ordering puts the frequencies most important in distinguishing the samples in the upper left and lower right (the yellow regions). In the lower right corner, we see the two outlier samples TJ_OO and SV_EPO grouped together. On the frequency axis, we see that ester stretching peaks around 1740 \\(\\mathrm{cm}^1\\) are characteristic for these samples. In the upper left corner, we see several samples grouped together, and associated with the fatty acid carboxylic acid peak around 1710 \\(\\mathrm{cm}^1\\). From these two observations, we can conclude that these two peak ranges are most important in separating the samples. Of course, in this simple example using a small part of the spectrum, this answer was already clear by simple inspection. Using a simple/limited range of data helps us to be sure we understand what’s happening when we try a new technique.\n\n\nUsing a Different Distance Measure & Seriation Method\nThe default data treatments for hmapSpectra are inherited from hmap in package seriation. The default distance between the samples is the Euclidean distance. The default seriation method is “OLO” or “optimal leaf ordering”. The full list of seriation methods is described in ?seriate. There are more than 20 options. As with the display details, we can change these defaults via the ... arguments. Let’s use the cosine distance (the same as the Pearson distance), and seriate using the Gruvaeus-Wainer algorithm (there’s a brief explanation of this algorithm at ?seriate).\n\ncosine_dist <- function(x) as.dist(1 - cor(t(x)))\nres <- hmapSpectra(IR,\n  col = heat.colors(5),\n  row_labels = IR$names, col_labels = as.character(round(IR$freq)),\n  margins = c(4, 6),\n    distfun = cosine_dist,\n    method = \"GW\"\n)\n\nRegistered S3 method overwritten by 'gclus':\n  method         from     \n  reorder.hclust seriation\n\n\n\n\n\nYou can see that using different distance measures and seriation algorithms gives a rather different result: the ester “hot spots” which were in the lower right corner are now almost in the lower left corner. Which settings are best will depend on your data set, the goal of your analysis, and there are a lot of options from which to choose. The settings used here are simply for demonstration purposes, I make no claim these settings are appropriate!\nFinally, if you want to capture the re-ordered frequencies, you can access them in the returned object:\n\nround(IR$freq[res$colInd$order])\n\n [1] 1767 1765 1763 1761 1759 1757 1755 1753 1751 1749 1747 1745 1743 1741 1740\n[16] 1738 1736 1734 1732 1730 1768 1774 1770 1772 1728 1726 1724 1722 1720 1718\n[31] 1716 1714 1713 1711 1709 1707 1705 1703 1701 1699 1697 1695 1693 1691 1689\n[46] 1687 1660 1664 1666 1678 1680 1686 1682 1676 1674 1672 1662 1668 1670 1684\n\n\n\n\n\n\n\nFootnotes\n\n\nOther functions in ChemoSpec that can help you explore which frequencies are important are plotLoadings, plot2Loadings and sPlotSpectra.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2020,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {Spectral {Heatmaps}},\n  date = {2020-04-25},\n  url = {http://chemospec.org/2020-04-25-Heatmaps.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2020. “Spectral Heatmaps.” April 25, 2020. http://chemospec.org/2020-04-25-Heatmaps.html."
  },
  {
    "objectID": "posts/2020-01-02-readJDX-update/2020-01-02-readJDX-update.html",
    "href": "posts/2020-01-02-readJDX-update/2020-01-02-readJDX-update.html",
    "title": "readJDX Overhaul",
    "section": "",
    "text": "readJDX reads files in the JCAMP-DX format used in the field of spectroscopy. A recent overhaul has made it much more robust, and as such the version is now at 0.4.29.1 Most of the changes were internal, but three important user-facing changes are:\n\nimproved documentation\nthe addition of more vignettes\nimproved output when debug > 0\n2D NMR files are now handled\n\nYou can see more about the package here. As always, if you use the package and have troubles, please file an issue. The JCAMP-DX standard is challenging and vendors have a lot of flexibility, so please do share any problematic files you encounter.\n\n\n\n\nFootnotes\n\n\nThe current version also includes changes in an unreleased version (0.3.372) in which several bugs were squashed.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2020,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {readJDX {Overhaul}},\n  date = {2020-01-02},\n  url = {http://chemospec.org/2020-01-02-readJDX-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2020. “readJDX Overhaul.” January 2, 2020. http://chemospec.org/2020-01-02-readJDX-update.html."
  },
  {
    "objectID": "posts/2020-01-22-CSU-update/2020-01-22-CSU-update.html",
    "href": "posts/2020-01-22-CSU-update/2020-01-22-CSU-update.html",
    "title": "ChemoSpecUtils Update",
    "section": "",
    "text": "ChemoSpecUtils, a package that supports the common needs of ChemoSpec and ChemoSpec2D, has been updated on CRAN and is coming to a mirror near you. Noteworthy changes:\n\nThere are new color options available in addition to the auto color scheme used during data importing. These should be useful to normal-vision individuals when there are a lot of categories. The auto option remains the default to avoid breaking anyone’s code. All the built-in color schemes are shown below. They can be used in any of the import functions in either package. The code used to make the figure below is in ?colorSymbol. Note: you probably should get the devel version to ChemoSpec in order to see the documentation about how to use the new colors.\nThe function removeFreq in ChemoSpec now accepts a formula for the specification of the frequencies to remove. This brings it in line with the corresponding function in ChemoSpec2D. This should be a lot easier to use.\nThe function sampleDist is now available and replaces sampleDistSpectra. Again the functions in the two overlying packages are essentially as similar as they can be.\nThis version is compatible with the upcoming release of R 4.0.\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2020,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {ChemoSpecUtils {Update}},\n  date = {2020-01-22},\n  url = {http://chemospec.org/2020-01-22-CSU-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2020. “ChemoSpecUtils Update.” January 22,\n2020. http://chemospec.org/2020-01-22-CSU-update.html."
  },
  {
    "objectID": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html",
    "href": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html",
    "title": "Simulating Spectroscopic Data Part 1",
    "section": "",
    "text": "It is well-recognized that one of the virtues of the R language is the extensive tools it provides for working with distributions. Functions exist to generate random number draws, determine quantiles, and examine the probability density and cumulative distribution curves that describe each distribution.\nThis toolbox gives one the ability to create simulated data sets for testing very easily. If you need a few random numbers from a Gaussian distribution then rnorm is your friend:\nImagine you were developing a new technique to determine if two methods of manufacturing widgets produced widgets of the same mass.1 Even before the widgets were manufactured, you could test your code by simulating widget masses using rnorm:\nVariations on this approach can be used to simulate spectral data sets.2 The information I will share here is accumulated knowledge. I have no formal training in the theory behind the issues discussed, just skills I have picked up in various places and by experimenting. If you see something that is wrong or needs clarification or elaboration, please use the comments to set me straight!"
  },
  {
    "objectID": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html#peak-shapes",
    "href": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html#peak-shapes",
    "title": "Simulating Spectroscopic Data Part 1",
    "section": "Peak Shapes",
    "text": "Peak Shapes\nWhat peak shape is expected for a given type of spectroscopy? In principle this is based on the theory behind the method, either some quantum mechanical model or an approximation of it. For some methods, like NMR, this might be fairly straightforward, at least in simple systems. But the frequencies involved in some spectroscopies not too different from others, and coupling is observed. Two examples which “interfere” with each other are:\n\nElectronic transitions in UV spectra which are broadened by interactions with vibrational states.\nVibrational transitions in IR spectroscopy (bonds stretching and bond angles bending in various ways) are coupled to electronic transitions.\n\nAfter theoretical considerations, we should keep in mind that all spectroscopies have some sort of detector, electronic components and basic data processing that can affect peak shape. A CCD on a UV detector is one of the simpler situations. FT-IR has a mechanical interferometer, and the raw signal from both IR and NMR is Fourier-transformed prior to use. So there are not only theoretical issues to think about, but also engineering, instrument tuning, electrical engineering and mathematical issues to consider.\nEven with myriad theoretical and practical considerations, a Gaussian curve is a good approximation to a simple peak, and more complex peaks can be built by summing Gaussian curves. If we want to simulate a simple peak with a Gaussian shape, we can use the dnorm function, which gives us the “density” of the distribution:\n\nstd_deviations <- seq(-5, 5, length.out = 100)\nGaussian_1 <- dnorm(std_deviations)\nplot(std_deviations, Gaussian_1, type = \"l\",\n  xlab = \"standard deviations\", ylab = \"Gaussian Density\")\n\n\n\n\nIf we want this to look more like a “real” peak, we can increase the x range and use x values with realistic frequency values. And if we want our spectrum to be more complex, we can add several of these curves together. Keep in mind that the area under the density curve is 1.0, and the peak width is determined by the value of argument sd (the standard deviation). For example if you want to simulate the UV spectrum of vanillin, which has maxima at about 230, 280 and 315 nm, one can do something along these lines:\n\nwavelengths <- seq(220, 350, by = 1.0)\nPeak1 <- dnorm(wavelengths, 230, 22)\nPeak2 <- dnorm(wavelengths, 280, 17)\nPeak3 <- dnorm(wavelengths, 315, 17)\nPeaks123 <- colSums(rbind(1.6 * Peak1, Peak2, Peak3))\nplot(wavelengths, Peaks123, type = \"l\",\n  xlab = \"wavelengths (nm)\", ylab = \"arbitrary intensity\")\n\n\n\n\nThe coefficient on Peak1 is needed to increase the contribution of that peak in order to better resemble the linked spectrum (note that the linked spectrum y-axis is \\(log \\epsilon\\); we’re just going for a rough visual approximation).\nIt’s a simple, if tedious, task to add Gaussian curves in this manner to simulate a single spectrum. One can also create several different spectra, and then combine them in various ratios to create a data set representing samples composed of mixtures of compounds. UV spectra are tougher due to the vibrational coupling; NMR spectra are quite straightforward since we know the area of each magnetic environment in the structure (but we also have to deal with doublets etc.). If you plan to do a lot of this, take a look at the SpecHelpers package, which is designed to streamline these tasks.\nA relatively minor exception to the typical Gaussian peak shape is NMR. Peaks in NMR are typically described as “Lorentzian”, which corresponds to the Cauchy distribution (Goldenberg 2016). This quick comparison shows that NMR peaks are expected to be less sharp and have fatter tails:\n\nGaussian_1 <- dnorm(std_deviations)\nCauchy_1 <- dcauchy(std_deviations)\nplot(std_deviations, Gaussian_1, type = \"l\",\n  xlab = \"standard deviations\", ylab = \"density\")\nlines(std_deviations, Cauchy_1, col = \"red\")"
  },
  {
    "objectID": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html#baselines",
    "href": "posts/2020-06-28-Sim-Spec-Data-Pt1/2020-06-28-Sim-Spec-Data-Pt1.html#baselines",
    "title": "Simulating Spectroscopic Data Part 1",
    "section": "Baselines",
    "text": "Baselines\nFor many types of spectroscopies there is a need to correct the baseline when processing the data. But if you are simulating spectroscopic (or chromatographic) data, how can you introduce baseline anomalies? Such anomalies can take many forms, for instance a linear dependence on wavelength (i.e. a steadily rising baseline without curvature). But more often one sees complex rolling baseline issues.\nLet’s play with introducing different types of baseline abberations. First, let’s create a set of three simple spectra. We’ll use a simple function to scale the set of spectra so the range is on the interval [0…1] for ease of further manipulations.\n\nwavelengths <- 200:800\nSpec1 <- dnorm(wavelengths, 425, 30)\nSpec2 <- dnorm(wavelengths, 550, 20) * 3 # boost the area\nSpec3 <- dnorm(wavelengths, 615, 15)\nSpec123 <- rbind(Spec1, Spec2, Spec3)\ndim(Spec123) # matrix with samples in rows\n\n[1]   3 601\n\n\n\nscale01 <- function(M) {\n  # scales the range of the matrix to [0...1]\n  mn <- min(M)\n  M <- M - mn\n  mx <- max(M)\n  M <- M/mx\n}\n\nHere are the results; the dotted line is the sum of the three spectra, offset vertically for ease of comparison.\n\nSpec123 <- scale01(Spec123)\nplot(wavelengths, Spec123[1,], col = \"black\", type = \"l\",\n  xlab = \"wavelength (nm)\", ylab = \"intensity\",\n  ylim = c(0, 1.3))\nlines(wavelengths, Spec123[2,], col = \"red\")\nlines(wavelengths, Spec123[3,], col = \"blue\")\nlines(wavelengths, colSums(Spec123) + 0.2, lty = 2)\n\n\n\n\nOne clever way to introduce baseline anomalies is to use a Vandermonde matrix. This is a trick I learned while working with the team on the hyperSpec overhaul funded by GSOC.3 It’s easiest to explain by an example:\n\nvander <- function(x, order) outer(x, 0:order, `^`)\nvdm <- vander(wavelengths, 2)\ndim(vdm)\n\n[1] 601   3\n\nvdm[1:5, 1:3]\n\n     [,1] [,2]  [,3]\n[1,]    1  200 40000\n[2,]    1  201 40401\n[3,]    1  202 40804\n[4,]    1  203 41209\n[5,]    1  204 41616\n\nvdm <- scale(vdm, center = FALSE, scale = c(1, 50, 2000))\n\nLooking at the first few rows of vdm, you can see that the first column is a simple multiplier, in this case an identity vector. This can be viewed as an offset term.4 The second column contains the original wavelength values, in effect a linear term. The third column contains the square of the original wavelength values. If more terms had been requested, they would be the cubed values etc. In the code above we also scaled the columns of the matrix so that the influence of the linear and especially the squared terms don’t dominate the absolute values of the final result. Scaling does not affect the shape of the curves.\nTo use this Vandermonde matrix, we need another matrix which will function as a set of coefficients.\n\ncoefs <- matrix(runif(nrow(Spec123) * 3), ncol = 3)\ncoefs\n\n           [,1]      [,2]      [,3]\n[1,] 0.81126877 0.9094830 0.2902550\n[2,] 0.15101528 0.9878546 0.3244570\n[3,] 0.05994409 0.5978804 0.9532016\n\n\nIf we multiply the coefficients by the tranposed Vandermonde matrix, we get back a set of offsets which are the rows of the Vandermonde matrix modified by the coefficients. We’ll scale things so that Spec123 and offsets are on the same overall scale and then further scale so that the spectra are not overwhelmed by the offsets in the next step.\n\noffsets <- coefs %*% t(vdm)\ndim(offsets) # same dimensions as Spec123 above\n\n[1]   3 601\n\noffsets <- scale01(offsets) * 0.1\n\nThese offsets can then be added to the original spectrum to obtain our spectra with a distorted baseline. Here we have summed the individual spectra. We have added a line based on extrapolating the first 20 points of the distorted data, which clearly shows the influence of the squared term.\n\nFinalSpec1 <- offsets + Spec123\nplot(wavelengths, colSums(FinalSpec1), type = \"l\", col = \"red\",\n  xlab = \"wavelength (nm)\", ylab = \"intensity\")\nlines(wavelengths, colSums(Spec123))\nfit <- lm(colSums(FinalSpec1)[1:20] ~ wavelengths[1:20])\nlines(wavelengths, fit$coef[2]*wavelengths + fit$coef[1],\n  col = \"red\", lty = 2) # good ol' y = mx + b\n\n\n\n\nThe Vandermonde matrix approach works by creating offsets that are added to the original spectrum. However, it is limited to creating baseline distortions that generally increase at higher values. To create other types of distortions, you can use your imagination. For instance, you could reverse the order of the rows of offsets and/or use higher terms, scale a row, etc. One could also play with various polynomial functions to create the desired effect over the wavelength range of interest. For instance, the following code adds a piece of an inverted parabola to the original spectrum to simulate a baseline hump.\n\nhump <- -1*(15*(wavelengths - 450))^2 # piece of a parabola\nhump <- scale01(hump)\nFinalSpec2 <- hump * 0.1 + colSums(Spec123)\nplot(wavelengths, FinalSpec2, type = \"l\",\n  xlab = \"wavelengths (nm)\", ylab = \"intensity\")\nlines(wavelengths, hump * 0.1, lty = 2) # trace the hump\n\n\n\n\nIn the plot, the dotted line traces out the value of hump * 0.1, the offset.\nIn the next post we’ll look at ways to introduce noise into simulated spectra."
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html",
    "title": "Notes on Linear Algebra Part 2",
    "section": "",
    "text": "Linear algebra is complex. We need a way to penetrate the thicket. Here’s one.\nLinear systems of equations are at the heart, not surprisingly, of linear algebra.\nA key application is linear regression, which has a matrix solution.\nSolving the needed equations requires inverting a matrix.\nInverting a matrix is more easily done after decomposing the matrix into upper and lower triangular matrices.\nThe upper and lower triangular matrices are individually easy to invert, giving access to the inverse of the original matrix.\nChanges in notations and symbols as you move between presentations add significantly to the cognitive burden in learning this material.\n\n\nFor Part 1 of this series, see here.\nIf you open a linear algebra text, it’s quickly apparent how complex the field is. There are so many special types of matrices, so many different decompositions of matrices. Why are all these needed? Should I care about null spaces? What’s really important? What are the threads that tie the different concepts together? As someone who is trying to improve their understanding of the field, especially with regard to its applications in chemometrics, it can be a tough slog.\nIn this post I’m going to try to demonstrate how some simple chemometric tasks can be solved using linear algebra. Though I cover some math here, the math is secondary right now – the conceptual connections are more important. I’m more interested in finding (and sharing) a path through the thicket of linear algebra. We can return as needed to expand the basic math concepts. The cognitive effort to work through the math details is likely a lot lower if we have a sense of the big picture.\nIn this post, matrices, including row and column vectors, will be shown in bold e.g. \\(\\mathbf{A}\\) while scalars and variables will be shown in script, e.g. \\(n\\). Variables used in R code will appear like A."
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#systems-of-equations",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#systems-of-equations",
    "title": "Notes on Linear Algebra Part 2",
    "section": "Systems of Equations",
    "text": "Systems of Equations\nIf you’ve had algebra, you have certainly run into “system of equations” such as the following:\n\\[\n\\begin{multline}\nx + 2y -3z = 3 \\\\\n2x - y - z = 11 \\\\\n3x + 2y + z = -5 \\\\\n\\end{multline}\n\\tag{1}\\]\nIn algebra, such systems can be solved several ways, for instance by isolating one or more variables and substituting, or geometrically (particularly for 2D systems, by plotting the lines and looking for the intersection). Once there are more than a few variables however, the only manageable way to solve them is with matrix operations, or more explicitly, linear algebra. This sort of problem is the core of linear algebra, and the reason the field is called linear algebra.\nTo solve the system above using linear algebra, we have to write it in the form of matrices and column vectors:\n\\[\n\\begin{bmatrix}\n1 & 2 & -3 \\\\ 2 & -1 & -1 \\\\ 3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\ y \\\\ z\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 \\\\ 11 \\\\ -5\n\\end{bmatrix}\n\\tag{2}\\]\nor more generally\n\\[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}\n\\tag{3}\\]\nwhere \\(\\mathbf{A}\\) is the matrix of coefficients, \\(\\mathbf{x}\\) is the column vector of variable names1 and \\(\\mathbf{b}\\) is a column vector of constants. Notice that these matrices are conformable:2\n\\[\n\\mathbf{A}^{3 \\times 3}\\mathbf{x}^{3 \\times 1} = \\mathbf{b}^{3 \\times 1}\n\\tag{4}\\]\nTo solve such a system, when we have \\(n\\) unknowns, we need \\(n\\) equations.3 This means that \\(\\mathbf{A}\\) has to be a square matrix, and square matrices play a special role in linear algebra. I’m not sure this point is always conveyed clearly when this material is introduced. In fact, it seems like many texts on linear algebra seem to bury the lede.\nTo find the values of \\(\\mathbf{x} = x, y, z\\)4, we can do a little rearranging following the rules of linear algebra and matrix operations. First we pre-multiply both sides by the inverse of \\(\\mathbf{A}\\), which then gives us the identity matrix \\(\\mathbf{I}\\), which drops out.5\n\\[\n\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{x} = \\mathbf{I}\\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}\n\\tag{5}\\]\nSo it’s all sounding pretty simple right? Ha. This is actually where things potentially break down. For this to work, \\(\\mathbf{A}\\) must be invertible, which is not always the case.6 If there is no inverse, then the system of equations either has no solution or infinite solutions. So finding the inverse of a matrix, or discovering it doesn’t exist, is essential to solving these systems of linear equations.7 More on this eventually, but for now, we know \\(\\mathbf{A}\\) must be a square matrix and we hope it is invertible."
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#a-key-application-linear-regression",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#a-key-application-linear-regression",
    "title": "Notes on Linear Algebra Part 2",
    "section": "A Key Application: Linear Regression",
    "text": "A Key Application: Linear Regression\nWe learn in algebra that a line takes the form \\(y = mx + b\\). If one has measurements in the form of \\(x, y\\) pairs that one expects to fit to a line, we need linear regression. Carrying out a linear regression is arguably one of the most important, and certainly a very common application of the linear systems described above. One can get the values of \\(m\\) and \\(b\\) by hand using algebra, but any computer will solve the system using a matrix approach.8 Consider this data:\n\\[\n\\begin{matrix}\nx & y \\\\\n2.1 & 11.8 \\\\\n0.9 & 7.2 \\\\\n3.9 & 21.5 \\\\\n3.2 & 17.2 \\\\\n5.1 & 26.8 \\\\\n\\end{matrix}\n\\tag{6}\\]\nTo express this in a matrix form, we recast\n\\[\ny = mx + b\n\\tag{7}\\]\ninto\n\\[\n\\mathbf{y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\n\\tag{8}\\]\nwhere:\n\n\\(\\mathbf{y}\\) is the column vector of \\(y\\) values. That seems sensible.\n\\(\\mathbf{X}\\) is a matrix composed of a column of ones plus a column of the \\(x\\) values. This is called a design matrix. At least here \\(\\mathbf{X}\\) contains only \\(x\\) values as variables.\n\\(\\mathbf{\\beta}\\) is a column vector of coefficients (including, as we will see, the values of \\(m\\) and \\(b\\) if we are thinking back \\(y = mx + b\\)).\n\\(\\mathbf{\\epsilon}\\) is new, it is a column vector giving the errors at each point.\n\nWith our data above, this looks like:\n\\[\n\\begin{bmatrix}\n11.8 \\\\\n7.2 \\\\\n21.5 \\\\\n17.2 \\\\\n26.8 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 2.1 \\\\\n1 & 0.9 \\\\\n1 & 3.9 \\\\\n1 & 3.2 \\\\\n1 & 5.1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5\n\\end{bmatrix}\n\\tag{9}\\]\nIf we multiply this out, each row works out to be an instance of \\(y_i = \\beta_1 x_i + \\beta_0\\). Hopefully you can appreciate that \\(\\beta_1\\) corresponds to \\(m\\) and \\(\\beta_0\\) corresponds to \\(b\\).9\nThis looks similar to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) seen in Equation 3, if you set \\(\\mathbf{b}\\) to \\(\\mathbf{y}\\), \\(\\mathbf{A}\\) to \\(\\mathbf{X}\\) and \\(\\mathbf{x}\\) to \\(\\beta\\):\n\\[\n\\begin{align}\n\\mathbf{b} &\\approx \\mathbf{A}\\mathbf{x} \\\\\n\\downarrow &\\phantom{\\approx} \\; \\downarrow \\; \\downarrow \\\\\n\\mathbf{y} &\\approx \\mathbf{X}\\mathbf{\\beta} \\\\\n\\end{align}\n\\tag{10}\\]\nThis contortion of symbols is pretty nasty, but honestly not uncommon when moving about in the world of linear algebra.\nAs it is composed of real data, presumably with measurement errors, there is not an exact solution to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) due to the error term. There is however, an approximate solution, which is what is meant when we say we are looking for the line of best fit. This is how linear regression is carried out on a computer. The relevant equation is:\n\\[\n\\hat{\\beta} = (\\mathbf{X}^{\\mathsf{T}}\\mathbf{X})^{-1}\\mathbf{X}^{\\mathsf{T}}y\n\\tag{11}\\]\nThe key point here is that once again we need to invert a matrix to solve this. The details of where Equation 11 comes from are covered in a number of places, but I will note here that \\(\\hat{\\beta}\\) refers to the best estimate of \\(\\beta\\).10"
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#inverting-matrices",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#inverting-matrices",
    "title": "Notes on Linear Algebra Part 2",
    "section": "Inverting Matrices",
    "text": "Inverting Matrices\nWe now have two examples where inverting a matrix is a key step: solving a system of linear equations, and approximating the solution to a system of linear equations (the regression case). These cases are not outliers, the ability to invert a matrix is very important. So how do we do this? The LU decomposition can do it, and is widely used so worth spending some time on. A decomposition is the process of breaking a matrix into pieces that are easier to handle, or that give us special insight, or both. If you are a chemometrician you have almost certainly carried out Principal Components Analysis (PCA). Under the hood, PCA requires either a singular value decomposition, or an eigen decomposition (more info here).\nSo, about the LU decomposition: it breaks a matrix into two matrices, \\(\\mathbf{L}\\), a “lower triangular matrix”, and \\(\\mathbf{U}\\), an “upper triangular matrix”. These special matrices contain only zeros except along the diagonal and the entries below it (in the lower case), or along the diagonal and the entries above it (in the upper case). The advantage of triangular matrices is that they are very easy to invert (all those zeros make many terms drop out). So the LU decomposition breaks the tough job of inverting \\(\\mathbf{A}\\) into two easier jobs.\n\\[\n\\begin{align}\n\\mathbf{A} &= \\mathbf{L}\\mathbf{U} \\\\\n\\mathbf{A}^{-1} &= (\\mathbf{L}\\mathbf{U})^{-1} \\\\\n\\mathbf{A}^{-1} &= \\mathbf{U}^{-1}\\mathbf{L}^{-1} \\\\\n\\end{align}\n\\tag{12}\\]\nWhen all is done, we only need to figure out \\(\\mathbf{U}^{-1}\\) and \\(\\mathbf{L}^{-1}\\) which as mentioned is straightforward.11\nTo summarize, if we want to solve a system of equations we need to carry out matrix inversion, which is turn is much easier to do if one uses the LU decomposition to get two easy to invert triangular matrices. I hope you are beginning to see how pieces of linear algebra fit together, and why it might be good to learn more."
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#r-functions",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#r-functions",
    "title": "Notes on Linear Algebra Part 2",
    "section": "R Functions",
    "text": "R Functions\n\nInverting Matrices\nLet’s look at how R does these operations, and check our understanding along the way. R makes this really easy. We’ll start with the issue of invertibility. Let’s create a matrix for testing.\n\nA1 <- matrix(c(3, 5, -1, 11, 2, 0, -5, 2, 5), ncol = 3)\nA1\n\n     [,1] [,2] [,3]\n[1,]    3   11   -5\n[2,]    5    2    2\n[3,]   -1    0    5\n\n\nIn the matlib package there is a function inv that inverts matrices. It returns the inverted matrix, which we can verify by multiplying the inverted matrix by the original matrix to give the identity matrix (if inversion was successful). diag(3) creates a 3 x 3 matrix with 1’s on the diagonal, in other words an identity matrix.\n\nlibrary(\"matlib\")\nA1_inv <- inv(A1)\nall.equal(A1_inv %*% A1, diag(3)) \n\n[1] \"Mean relative difference: 8.999999e-08\"\n\n\nThe difference here is really small, but not zero. Let’s use a different function, solve which is part of base R. If solve is given a single matrix, it returns the inverse of that matrix.\n\nA1_solve <- solve(A1) %*% A1\nall.equal(A1_solve, diag(3))\n\n[1] TRUE\n\n\nThat’s a better result. Why are there differences? inv uses a method called Gaussian elimination which is similar to how one would invert a matrix using pencil and paper. On the other hand, solve uses the LU decomposition discussed earlier, and no matrix inversion is necessary. Looks like the LU decomposition gives a somewhat better numerical result.\nNow let’s look at a different matrix, created by replacing the third column of A1 with different values.\n\nA2 <- matrix(c(3, 5, -1, 11, 2, 0, 6, 10, -2), ncol = 3)\nA2\n\n     [,1] [,2] [,3]\n[1,]    3   11    6\n[2,]    5    2   10\n[3,]   -1    0   -2\n\n\nAnd let’s compute its inverse using solve.\n\nsolve(A2)\n\nError in solve.default(A2): system is computationally singular: reciprocal condition number = 6.71337e-19\n\n\nWhen R reports that A2 is computationally singular, it is saying that it cannot be inverted. Why not? If you look at A2, notice that column 3 is a multiple of column 1. Anytime one column is a multiple of another, or one row is a multiple of another, then the matrix cannot be inverted because the rows or columns are not independent.12 If this was a matrix of coefficients from an experimental measurement of variables, this would mean that some of your variables are not independent, they must be measuring the same underlying phenomenon.\n\n\nSolving Systems of Linear Equations\nLet’s solve the system from Equation 2. It turns out that the solve function also handles this case, if you give it two arguments. Remember, solve is using the LU decomposition behind the scenes, no matrix inversion is required.\n\nA3 <- matrix(c(1, 2, 3, 2, -1, 2, -3, -1, 1), ncol = 3)\nA3\n\n     [,1] [,2] [,3]\n[1,]    1    2   -3\n[2,]    2   -1   -1\n[3,]    3    2    1\n\ncolnames(A3) <-c(\"x\", \"y\", \"z\") # naming the columns will label the answer\nb <- c(3, 11, -5)\nsolve(A3, b)\n\n x  y  z \n 2 -4 -3 \n\n\nThe answer is the values of \\(x, y, z\\) that make the system of equations true.\n\n\n\n\n\n\nHow does LU decomposition avoid inversion?\n\n\n\n\n\nWhile we’ve emphasized the importance and challenges of inverting matrices, we’ve also pointed out that to solve a linear system there are alternatives to looking at the problem from the perspective of Equation 5. Here’s an approach using the LU decomposition, starting with substituting \\(\\mathbf{A}\\) with \\(\\mathbf{L}\\mathbf{U}\\):\n\\[\n\\mathbf{A}\\mathbf{x} = \\mathbf{L}\\mathbf{U}\\mathbf{x} = \\mathbf{b}\n\\tag{13}\\]\nWe want to solve for \\(\\mathbf{x}\\) the column vector of variables. To do so, define a new vector \\(\\mathbf{y} = \\mathbf{U}\\mathbf{x}\\) and substitute it in:\n\\[\n\\mathbf{L}\\mathbf{U}\\mathbf{x} = \\mathbf{L}\\mathbf{y} = \\mathbf{b}\n\\tag{14}\\]\nNext we solve for \\(\\mathbf{y}\\). One way we could do this is to pre-multiply both sides by \\(\\mathbf{L}^{-1}\\) but we are looking for a way to avoid using the inverse. Instead, we evaluate \\(\\mathbf{L}\\mathbf{y}\\) to give a series of expressions using the dot product (in other words plain matrix multiplication). Because \\(\\mathbf{L}\\) is lower triangular, many of the terms we might have gotten actually disappear because of the zero coefficients. What remains is simple enough that we can algebraically find each element of \\(\\mathbf{y}\\) starting from the first row (this is called forward substitution). Once we have \\(\\mathbf{y}\\), we can find \\(\\mathbf{x}\\) by solving \\(\\mathbf{y} = \\mathbf{U}\\mathbf{x}\\) using a similar approach, but working from the last row upward (this is backward substitution). This is a good illustration of the utility of triangular matrices: some operations can move from the linear algebra realm to the algebra realm. Wikipedia has a good illustration of forward and backward substitution.\n\n\n\n\n\nComputing Linear Regression\nLet’s compute the values for \\(m, b\\) in our regression data shown in Equation 6. First, let’s set up the needed matrices and plot the data since visualizing the data is always a good idea.\n\ny = matrix(c(11.8, 7.2, 21.5, 17.2, 26.8), ncol = 1)\nX = matrix(c(rep(1, 5), 2.1, 0.9, 3.9, 3.2, 5.1), ncol = 2) # design matrix\nX\n\n     [,1] [,2]\n[1,]    1  2.1\n[2,]    1  0.9\n[3,]    1  3.9\n[4,]    1  3.2\n[5,]    1  5.1\n\nplot(X[,2], y, xlab = \"x\") # column 2 of X has the x values\n\n\n\n\nThe value of \\(\\hat{\\beta}\\) can be found via Equation 11:\n\nsolve((t(X) %*% X)) %*%  t(X) %*% y\n\n         [,1]\n[1,] 2.399618\n[2,] 4.769862\n\n\nThe first value is for \\(b\\) or \\(\\beta_0\\) or intecept, the second value is for \\(m\\) or \\(\\beta_1\\) or slope.\nLet’s compare this answer to R’s built-in lm function (for linear model):\n\nfit <- lm(y ~ X[,2])\nfit\n\n\nCall:\nlm(formula = y ~ X[, 2])\n\nCoefficients:\n(Intercept)       X[, 2]  \n       2.40         4.77  \n\n\nWe have good agreement! If you care to learn about the goodness of the fit, the residuals etc, then you can look at the help file ?lm and str(fit). lm returns pretty much all one needs to know about the results, but if you wish to calculate all the interesting values yourself you can do so by manipulating Equation 11 and its relatives.\nFinally, let’s plot the line of best fit found by lm to make sure everything looks reasonable.\n\nplot(X[,2], y, xlab = \"x\")\nabline(coef = coef(fit), col = \"red\")\n\n\n\n\n\nThat’s all for now, and a lot to digest. I hope you are closer to finding your own path through linear algebra. Remember that investing in learning the fundamentals prepares you for tackling the more complex topics. Thanks for reading!"
  },
  {
    "objectID": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#annotated-bibliography",
    "href": "posts/2022-09-01-Linear-Alg-Notes-Pt2/Linear-Alg-Notes-Pt2.html#annotated-bibliography",
    "title": "Notes on Linear Algebra Part 2",
    "section": "Annotated Bibliography",
    "text": "Annotated Bibliography\nThese are the main sources I relied on for this post.\n\nThe No Bullshit Guide to Linear Algebra by Ivan Savov.\n\nSection 1.15: Solving systems of linear equations.\nSection 6.6: LU decomposition.\n\nLinear Algebra: step by step by Kuldeep Singh, Oxford Univerity Press, 2014.\n\nSection 1.8.5: Singluar (non-invertible) matrices mean there is no solution or infinite solutions to the linear system. For graphical illustration see sections 1.1.3 and 1.7.2.\nSection 1.6.4: Definition of the inverse and conceptual meaning.\nSection 1.8.4: Solving linear systems when \\(\\mathbf{A}\\) is invertible.\nSection 6.4: LU decomposition.\nSection 6.4.3: Solving linear systems without using inversion, via the LU decomposition.\n\nLinear Models with R by Julian J. Faraway, Chapman and Hall/CRC, 2005.\n\nSections 2.1-2.4: Linear regression from the algebraic and matrix perspectives, derivation of Equation 11.\n\nThe vignettes of the matlib package are very helpful."
  },
  {
    "objectID": "posts/2021-05-22-GSOC-hyperSpec-ChemoSpec/2021-05-22-GSOC-hyperSpec-ChemoSpec.html",
    "href": "posts/2021-05-22-GSOC-hyperSpec-ChemoSpec/2021-05-22-GSOC-hyperSpec-ChemoSpec.html",
    "title": "GSOC 2021: hyperSpec and ChemoSpec!",
    "section": "",
    "text": "I’m really happy to announce that this summer I’ll be a co-mentor on two Google Summer of Code spectroscopy projects:\n\nOnce again, I’ll co-mentor with Claudia and Vilmantas to continue the work Erick started last summer on hyperSpec (see here for Erick’s wrap up blog post at the end of last year). Sang Truong is the very talented student who will be joining us. Sang’s project is described here.\nNew this year: ChemoSpec will be upgraded to use ggplot2 graphics along with interactive graphics for many of the plots that are currently rendered in base graphics. Erick, who was the student working on hyperSpec last summer, will be my co-mentor on this project. We are looking forward to having Tejasvi Gupta as the student on this project.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2021,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {GSOC 2021: {hyperSpec} and {ChemoSpec!}},\n  date = {2021-05-22},\n  url = {http://chemospec.org/2021-05-22-GSOC-hyperSpec-ChemoSpec.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2021. “GSOC 2021: hyperSpec and ChemoSpec!”\nMay 22, 2021. http://chemospec.org/2021-05-22-GSOC-hyperSpec-ChemoSpec.html."
  },
  {
    "objectID": "posts/2020-01-22-F4S-update/2020-01-22-F4S-update.html",
    "href": "posts/2020-01-22-F4S-update/2020-01-22-F4S-update.html",
    "title": "FOSS for Spectroscopy Update",
    "section": "",
    "text": "FOSS for Spectroscopy has had a significant update! It’s really quite surprising how many projects are out there. There is a lot of variety and not too much overlap.\n\nAfter a lot of wrestling with Github access issues, the Status column in the table now gives the date of the most recent update to the project that I can find in an automated way.\nThe Notes column is now called Focus and reflects the focus of the projects as far as I can determine things. I’m using a more-or-less controlled vocabulary here, so sorting on the Focus column should bring related projects together.\nThe number of entries is greatly expanded (and I have more in the hopper).\nThe page is now automatically updated weekly, which will keep the links and dates fresh.\n\nAs always, I welcome your feedback in any form. You can use the comments below, or if you have additions/corrections to the page itself, there is info there about how to submit updates.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2020,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {FOSS for {Spectroscopy} {Update}},\n  date = {2020-01-22},\n  url = {http://chemospec.org/2020-01-22-F4S-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2020. “FOSS for Spectroscopy Update.” January\n22, 2020. http://chemospec.org/2020-01-22-F4S-update.html."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "",
    "text": "Last summer, a GSOC project was approved for work on the hyperSpec package which had grown quite large and hard to maintain.1 The essence of the project was to break the original hyperSpec package into smaller packages.2 As part of that project, we needed to be able to:\nIn this post I’ll describe how we used Dirk Eddelbuettel’s drat package and Github Actions to automate the deployment of packages between repositories."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#what-is-drat",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#what-is-drat",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "What is drat?",
    "text": "What is drat?\ndrat is a package that simplifies the creation and modification of CRAN-like repositories. The structure of a CRAN-like repository is officially described briefly here.3 Basically, there is required set of subdirectories, required files containing package metadata, and source packages that are the result of the usual build and check process. One can also have platform-specific binary packages. drat will create the directories and metadata for you, and provides utilities that will move packages to the correct location and update the corresponding metadata.4 The link above provides access to all sorts of documentation. My advice is to not overthink the concept. A repository is simply a directory structure and a couple of required metadata files, which must be kept in sync with the packages present. drat does the heavy-lifting for you."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#what-are-github-actions",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#what-are-github-actions",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "What are Github Actions?",
    "text": "What are Github Actions?\nGithub Actions are basically a series of tasks that one can have Github run when there is an “event” on a repo, like a push or pull. Github Actions are used extensively for continuous integrations tasks, but they are not limited to such use. Github Actions are written in a simply yaml-like script that is rather easy to follow even if the details are not familiar. Github Actions uses shell commands, but much of the time the shell simply calls Rscript to run native R functions. One can run tasks on various hardware and OS versions."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#the-package-repo",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#the-package-repo",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "The Package Repo",
    "text": "The Package Repo\nThe deployed packages reside on the gh-pages branch of r-hyperspec/pkg-repo in the form of the usual .tar.gz source archives, ready for users to install. One of the important features of this repo is the table of hosted packages displayed in the README. The table portion of README.md file is generated automatically whenever someone, or something, pushes to this repo. I include the notion that something might push because as you will see next, the deploy process will automatically push archives to this repo from the repo where they are created. The details of how this README.md is generated are in drat--update-readme.yaml. If you take a look, you’ll see that we use some shell-scripting to find any .tar.gz archives and create a markdown-ready table structure, which Github then automatically displays (as it does with all README.md files at the top level of a repo). The yaml file also contains a little drat action that will refresh the repo in case that someone manually removes an archive file by git operations. Currently we do not host binary packages at this repo, but that is certainly possible by extension of the methods used for the source packages."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#the-automatic-deploy-process",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#the-automatic-deploy-process",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "The Automatic Deploy Process",
    "text": "The Automatic Deploy Process\nThe automatic deploy process is used in several r-hyperSpec repos. I’ll use the chondro repo to illustrate the process. chondro is a simple package containing a > 2 Mb data set. If the package is updated, the package is built and checked and then deployed automatically to r-hyperSpec/pkg-repo (described above). The magic is in drat--insert-package.yaml. The first part of this file does the standard build and check process.5 The second part takes care of deploying to r-hyperspec/pkg-repo. The basic steps are given next (study the file for the details). It is essential to keep in mind that each task in Github Actions starts from the same top level directory.6 Tasks are set off by the syntax - name: task description.\n\nConfigure access to Github. Note that we employ a Github user name and e-mail that will uniquely identify the repo that is pushing to r-hyperSpec/pkg-repo. This is helpful for troubleshooting.\nClone r-hyperSpec/pkg-repo into a temporary directory and checkout the gh-pages branch.\nSearch for any .tar.gz files in the check folder, which is where we directed Github Actions to carry out the build and check process (the first half of this workflow).7 Note that the argument full.names = TRUE is essential to getting the correct path. Use drat to insert the .tar.gz files into the cloned r-hyperSpec/pkg-repo temporary directory.\nMove to the temporary directory, then use git commands to send the updated r-hyperspec/pkg-repo branch back to its home, now with the new .tar.gz files included. Use a git commit message that will show where the new tar ball came from.\n\nThanks for reading. Let me know if you have any questions, via the comments, e-mail, etc."
  },
  {
    "objectID": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#acknowledgements",
    "href": "posts/2021-04-11-GHA-drat/2021-04-11-GHA-drat.html#acknowledgements",
    "title": "Using Github Actions and drat to Deploy R Packages",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis portion of the hyperSpec GSOC 2020 project was primarily the work of hyperSpec team members Erick Oduniyi, Bryan Hanson and Vilmantas Gegzna. Erick was supported by GSOC in summer 2020."
  },
  {
    "objectID": "posts/2022-05-03-LearnPCA-Intro/2022-05-03-LearnPCA-Intro.html",
    "href": "posts/2022-05-03-LearnPCA-Intro/2022-05-03-LearnPCA-Intro.html",
    "title": "Introducing LearnPCA",
    "section": "",
    "text": "PCA, or principal components analysis, is one of the most wide-spread statistical methods in use. It shows up in many disciplines, from political science and psychology, to chemistry and biology. PCA is also really challenging to understand.\nI’m pleased to announce that my colleague David Harvey and I have recently released LearnPCA, an R package to help people with understanding PCA. In LearnPCA we’ve tried to integrate our years of experience teaching the topic, along with the best insights we can find in books, tutorials and the nooks and crannies of the internet. Though our experience is in a chemometrics context, we use examples from different disciplines so that the package will be broadly helpful.\nThe package contains seven vignettes that proceed from the conceptual basics to advanced topics. As of version 0.2.0, there is also a Shiny app to help visualize the process of finding the principal component axes. The current vignettes are:\n\nA Guide to the LearnPCA Package\nA Conceptual Introduction to PCA\nStep-by-Step PCA\nUnderstanding Scores and Loadings\nVisualizing PCA in 3D\nThe Math Behind PCA\nA Comparison of Functions for PCA\n\nYou can access the vignettes at the Github Site, you don’t even have to install the package. For the Shiny app, do the following:\n\ninstall.packages(\"LearnPCA\") # you'll need version 0.2.0\nlibrary(\"LearnPCA\")\nPCsearch()\n\nWe would really appreciate your feedback on this package. You can do so in the comments below, or open an issue. \n\n\n\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2022,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {Introducing {LearnPCA}},\n  date = {2022-05-03},\n  url = {http://chemospec.org/2022-05-03-LearnPCA-Intro.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2022. “Introducing LearnPCA.” May 3, 2022. http://chemospec.org/2022-05-03-LearnPCA-Intro.html."
  },
  {
    "objectID": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html",
    "href": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html",
    "title": "Aligning 2D NMR Spectra Part 3",
    "section": "",
    "text": "This is Part 3 of a series on aligning 2D NMR, as implemented in the package ChemoSpec2D. Part 1 Part2\nLet’s get to work. The function to carry out alignment is hats_alignSpectra2D. The arguments maxF1 and maxF2 define the space that will be considered as the two spectra are shifted relative to each other. The space potentially covered is -maxF1 to maxF1 and similarly for the F2 dimension. dist_method, thres and minimize refer to the objective function, as described in Part 1. In this example we will consider two spectra succcessfully aligned when we get below the threshold. When one shifts one spectrum relative to the other, part of the shifted spectrum gets cutoff and part of it is empty space. fill = \"noise\" instructs the function to fill the empty space with an estimate of the noise from the original spectrum. We’ll set plot = FALSE here because the output is extensive. I’ll provide sample plotting output in a moment.\nAs the alignment proceeds, updates from the function are prefixed with [ChemoSpec2D]. In the first step we get a message that row 1 of 9 of the guide tree is being processed, in which sample 7 is being aligned with sample 4. The guide tree is shown below. One can see that samples 7 and 4 are very similar, so they are aligned first. If you inspect the output above, you can see that the four most similar pairs of spectra are aligned first, followed by groups of spectra according to similarity. For each alignment the needed shifts are reported. The last part of the output is a summary of all the alignments carried out. Note that the vertical scale on the guide tree is the same as the scale on the sampleDist plot in Part 1 (using the Euclidean distance)."
  },
  {
    "objectID": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html#diagnostics-on-space",
    "href": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html#diagnostics-on-space",
    "title": "Aligning 2D NMR Spectra Part 3",
    "section": "Diagnostics on Space",
    "text": "Diagnostics on Space\nTo save space, I suppressed the plotting of the results. However, there are plots! In fact there is a set of plots for each alignment step. Here are two of the plots produced if plot = TRUE; these deal with the X-Space which is the search space (the terminology comes from the mlrMBO package which is designed to handle many types of optimization). This plot is for Step 7. The upper plot shows the search space. Axis x1 corresponds to the F1 dimension, and axis x2 the F2 dimension. The red squares represent the initial experimental design, using the results from the objective function. The blue circles represent additional points added as the search proceeds. These represent new points on the response surface defined by the surrogate function (see Part 2 for background). The orange diamond is the best alignment, which in this case has no shift along F2 but a three data point shift along F1; this corresponds to the output above. The green triangle is the last position tested.\nThe lower plot represents the progress of the search over time. Axis “dob” stands for “date of birth” which is basically the time index of when the test point was added."
  },
  {
    "objectID": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html#diagnostics-on-the-objective-function",
    "href": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html#diagnostics-on-the-objective-function",
    "title": "Aligning 2D NMR Spectra Part 3",
    "section": "Diagnostics on the Objective Function",
    "text": "Diagnostics on the Objective Function\nThis second set of plots deals with what mlrMBO considers the Y-Space, which concerns the values of the objective function. The top plot is a histogram of the distance (objective function) values; in this case most of them were pretty bad (high, meaning a larger distance between the spectra). The middle plot is the value of the distance over time (dob). In this example the optimal alignment is found at dob = 4, but there is no particular significance to when the optimum is found. The lower plot shows the expected improvement (ei) at each dob. It is lowest when the optimum has been found. For more details about what’s going on under the hood, see the Arxiv paper."
  },
  {
    "objectID": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html#the-aligned-spectra",
    "href": "posts/2020-03-04-NMR-Align-Pt3/2020-03-04-NMR-Align-Pt3.html#the-aligned-spectra",
    "title": "Aligning 2D NMR Spectra Part 3",
    "section": "The Aligned Spectra",
    "text": "The Aligned Spectra\nDid this process work? This final plot shows that it did. Let’s be clear that the task here was not terribly hard: MUD2 is an artificial example in which the shifts are pretty modest and global in nature. But still, it’s satisfying. I welcome everyone to give hats_alignSpectra2D a try and report any problems or suggestions.\n\nmylvls <- seq(0, 30, length.out = 10)\nplotSpectra2D(MUD2a, which = c(1, 6), showGrid = TRUE,\n    lvls = LofL(mylvls, 2),\n    cols = LofC(c(\"red\", \"black\"), 2, length(mylvls), 2),\n  main = \"Aligned MUD2 Spectra 1 & 6\")"
  },
  {
    "objectID": "posts/2020-05-07-GSOC-hyperSpec/2020-05-07-GSOC-hyperSpec.html",
    "href": "posts/2020-05-07-GSOC-hyperSpec/2020-05-07-GSOC-hyperSpec.html",
    "title": "Fortifying hyperSpec: Getting Ready for GSOC",
    "section": "",
    "text": "hyperSpec is an R package for working with hyperspectral data sets. Hyperspectral data can take many forms, but a common application is a series of spectra collected over an x, y grid, for instance Raman imaging of medical specimens. hyperSpec was originally written by Claudia Beleites and she currently guides a core group of contributors.1\nClaudia, regular hyperSpec contributor Roman Kiselev and myself have joined forces this summer in a Google Summer of Code project to fortify hyperSpec. We are pleased to report that the project was accepted by R-GSOC administrators, and, as of a few days ago, the excellent proposal written by Erick Oduniyi was approved by Google. Erick is a senior computer engineering major at Wichita State University in Kansas. Erick gravitates toward interdisciplinary projects. This, and his experience with R, Python and related skills gives him an excellent background for this project.\nThe focus of this project is to fortify the infrastructure of hyperSpec. Over the years, keeping hyperSpec up-to-date has grown a bit unwieldy. While to-do lists always evolve, the current interrelated goals include:\n\nDistill2 hyperSpec: Prune hyperSpec back to it’s core functionality to keep it lightweight. Relocate portions, such as importing data, into their own dedicated packages.\nShield hyperSpec: Analyze the ecosystem of hyperSpec with an eye to reducing dependencies as much as possible and ensuring that necessary dependencies are the best choices. Avoid “re-inventing the wheel”, as long as the available “wheels” are computationally efficient and stable (code base and API).\nBridge hyperSpec: Having decided on how to reorganize hyperSpec and which dependencies are necessary and optimal, ensure that hyperSpec, the constellation of new sub-packages, and all dependencies are integrated efficiently. There are a number of data pre-processing and plotting functions that need to be streamlined and interfaced to external packages more consistently. Some portions may need substantial refactoring.\n\nAddressing each of these goals will make hyperSpec much easier to maintain, less fragile, and easier for others to contribute. Every step will bring enhanced documentation and vignettes, along with new unit tests. Work will begin in earnest on June 1st, and we are looking forward to a very productive summer.\nFinally, on behalf of all participants, let me just say how grateful we are to Google for establishing the GSOC program and for supporting Erick’s work this summer!\n\n\n\n\nFootnotes\n\n\nA little history for the curious: the hyperSpec and ChemoSpec packages were written around the same time, independent of each other (~2009). Eventually, Claudia and I became aware of each other’s work, and we have collaborated in ways large and small ever since (I like working with Claudia because I always learn from her!). We have jointly mentored GSOC students twice before. One side project is hyperChemoBridge, a small package that converts hyperSpec objects into Spectra objects (the native ChemoSpec format) and vice-versa.↩︎\nThe descriptors here are Erick’s clever choice of words.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2020,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {Fortifying {hyperSpec:} {Getting} {Ready} for {GSOC}},\n  date = {2020-05-07},\n  url = {http://chemospec.org/2020-05-07-GSOC-hyperSpec.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2020. “Fortifying hyperSpec: Getting Ready for\nGSOC.” May 7, 2020. http://chemospec.org/2020-05-07-GSOC-hyperSpec.html."
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "",
    "text": "One of the projects I maintain is the FOSS for Spectroscopy web site. The table at that site lists various software for use in spectroscopy. Historically, I have used the Github or Python Package Index search engines to manually search by topic such as “NMR” to find repositories of interest. Recently, I decided to try to automate at least some of this process. In this post I’ll present the code and steps I developed to search Github by topics. Fortunately, I wasn’t starting from scratch, as I had learned some basic web-scraping techniques when I wrote the functions that get the date of the most recent repository update. All the code for this website and project can be viewed here. The steps reported here are current as of the publication of this post, but are subject to change in the future.1\nFirst off, did you know Github allows repository owners to tag their repositories using topical keywords? I didn’t know this for a long time. So add topics to your repositories if you don’t have them already. By the way, the Achilles heel of this project is that good pieces of software may not have any topical tags at all. If you run into this, perhaps you would consider creating an issue to ask the owner to add tags."
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#the-overall-approach",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#the-overall-approach",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "The Overall Approach",
    "text": "The Overall Approach\nIf you look at the Utilities directory of the project, you’ll see the scripts and functions that power this search process.\n\nSearch Repos for Topics Script.R supervises the whole process. It sources:\nsearchRepos.R (a function)\nsearchTopic.R (a function)\n\nFirst let’s look at the supervising script. First, the necessary preliminaries:\n\nlibrary(\"jsonlite\")\nlibrary(\"httr\")\nlibrary(\"stringr\")\nlibrary(\"readxl\")\nlibrary(\"WriteXLS\")\n\nsource(\"Utilities/searchTopic.R\")\nsource(\"Utilities/searchRepos.R\")\n\nNote that this assumes one has the top level directory, FOSS4Spectroscopy, as the working directory (this is a bit easier than constantly jumping around).\nNext, we pull in the Excel spreadsheet that contains all the basic data about the repositories that we already know about, so we can eventually remove those from the search results.\n\nknown <- as.data.frame(read_xlsx(\"FOSS4Spec.xlsx\"))\nknown <- known$name\n\nNow we define some topics and run the search (more on the search functions in a moment):\n\ntopics <- c(\"NMR\", \"EPR\", \"ESR\")\nres <- searchRepos(topics, \"github_token\", known.repos = known)\n\nWe’ll also talk about that github_token in a moment. With the search results in hand, we have a few steps to make a useful file name and save it in the Searches folder for future use.\n\nfile_name <- paste(topics, collapse = \"_\")\nfile_name <- paste(\"Search\", file_name, sep = \"_\")\nfile_name <- paste(file_name, \"xlsx\", sep = \".\")\nfile_name <- paste(\"Searches\", file_name, sep = \"/\")\nWriteXLS(res, file_name,\n      row.names = FALSE, col.names = TRUE, na = \"NA\")\n\nAt this point, one can open the spreadsheet in Excel and check each URL (the links are live in the spreadsheet). After vetting each site,2 one can append the new results to the existing FOSS4Spec.xlsx data base and refresh the entire site so the table is updated.\nTo make this job easier, I like to have the search results spreadsheet open and then open all the URLs using the as follows. Then I can quickly clean up the spreadsheet (it helps to have two monitors for this process).\n\nfound <- as.data.frame(read_xlsx(file_name))\nfor (i in 1:nrow(found)) {\n  if (grepl(\"^https?://\", found$url[i], ignore.case = TRUE)) BROWSE(found$url[i])\n}"
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#authentificating",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#authentificating",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "Authentificating",
    "text": "Authentificating\nIn order to use the Github API, you have to authenticate. Otherwise you will be severely rate-limited. If you are authenticated, you can make up to 5,000 API queries per hour.\nTo authenticate, you need to first establish some credentials with Github, by setting up a “key” and a “secret”. You can set these up here by choosing the “Oauth Apps” tab. Record these items in a secure way, and be certain you don’t actually publish them by pushing.\nNow you are ready to authenticate your R instance using “Web Application Flow”.3\n\nmyapp <- oauth_app(\"FOSS\", key = \"put_your_key_here\", secret = \"put_your_secret_here\")\ngithub_token <- oauth2.0_token(oauth_endpoints(\"github\"), myapp)\n\nIf successful, this will open a web page which you can immediately close. In the R console, you’ll need to choose whether to do a one-time authentification, or leave a hidden file behind with authentification details. I use the one-time option, as I don’t want to accidently publish the secrets in the hidden file (since they are easy to overlook, being hidden and all)."
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#searchtopic",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#searchtopic",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "searchTopic",
    "text": "searchTopic\nsearchTopic is a function that accesses the Github API to search for a single topic.4 This function is “pretty simple” in that it is short, but there are six helper functions defined in the same file. So, “short not short”. This function does all the heavy lifting; the major steps are:\n\nCarry out an authenticated query of the topics associated with all Github repositories. This first “hit” returns up to 30 results, and also a header than tells how many more pages of results are out there.\nProcess that first set of results by converting the response to a JSON structure, because nice people have already built functions to handle such things (I’m looking at you httr).\n\nCheck that structure for a message that will tell us if we got stopped by Github access issues (and if so, report access stats).\nExtract only the name, description and repository URL from the huge volume of information captured.\n\nInspect the first response to see how many more pages there are, then loop over page two (we already have page 1) to the number of pages, basically repeating step 2.\n\nAlong the way, all the results are stored in a data.frame."
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#searchrepos",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#searchrepos",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "searchRepos",
    "text": "searchRepos\nsearchRepos does two simple things:\n\nLoops over all topics, since searchTopic only handles one topic at a time.\nOptionally, dereplicates the results by excluding any repositories that we already know about."
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#other-stuff-to-make-life-easier",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#other-stuff-to-make-life-easier",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "Other Stuff to Make Life Easier",
    "text": "Other Stuff to Make Life Easier\nThere are two other scripts in the Utilities folder that streamline maintenance of the project.\n\nmergeSearches.R which will merge several search results into one, removing duplicates along the way.\nmergeMaintainers.R which will query CRAN for the maintainers of all packages in FOSS4Spec.xlsx, and add this info to the file.5 Maintainers are not currently displayed on the main website. However, I hope to eventually e-mail all maintainers so they can fine-tune the information about their entries."
  },
  {
    "objectID": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#future-work-contributing",
    "href": "posts/2021-04-19-Search-GH-Topics/2021-04-19-Search-GH-Topics.html#future-work-contributing",
    "title": "Automatically Searching Github Repos by Topic",
    "section": "Future Work / Contributing",
    "text": "Future Work / Contributing\nClearly it would be good for someone who knows Python to step in and write the analogous search code for PyPi.org. Depending upon time contraints, I may use this as an opportunity to learn more Python, but really, if you want to help that would be quicker!\nAnd that folks, is how the sausage is made."
  },
  {
    "objectID": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html",
    "href": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html",
    "title": "Metabolic Phenotyping Protocol Part 1",
    "section": "",
    "text": "If you aren’t familiar with ChemoSpec, you might wish to look at the introductory vignette first."
  },
  {
    "objectID": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html#process-the-raw-data",
    "href": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html#process-the-raw-data",
    "title": "Metabolic Phenotyping Protocol Part 1",
    "section": "Process the Raw Data",
    "text": "Process the Raw Data\nFirst, we’ll take the results in raw and convert them to the proper form. Each element of raw is a data frame.\n\n# frequencies are in the 1st list element\nfreq <- unlist(raw[[1]], use.names = FALSE)\n\n# intensities are in the 2nd list element\ndata <- as.matrix(raw[[2]])\ndimnames(data) <- NULL  # remove the default data frame col names\nns <- nrow(data)  # ns = number of samples - used later\n\n# get genotype & lifestage, recode into something more readible\nyvars <- raw[[3]]\nnames(yvars) <- c(\"genotype\", \"stage\")\nyvars$genotype <- ifelse(yvars$genotype == 1L, \"WT\", \"Mut\")\nyvars$stage <- ifelse(yvars$stage == 1L, \"L2\", \"L4\")\ntable(yvars)  # quick look at how many in each group\n\n        stage\ngenotype L2 L4\n     Mut 32 33\n     WT  34 40"
  },
  {
    "objectID": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html#assemble-the-spectra-object-by-hand",
    "href": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html#assemble-the-spectra-object-by-hand",
    "title": "Metabolic Phenotyping Protocol Part 1",
    "section": "Assemble the Spectra Object by Hand",
    "text": "Assemble the Spectra Object by Hand\nNext we’ll construct some useful sample names, create the groups vector, assign the colors and symbols, and finally put it all together into a Spectra object.\n\n# build up sample names to include the group membership\nsample_names <- as.character(1:ns)\nsample_names <- paste(sample_names, yvars$genotype, sep = \"_\")\nsample_names <- paste(sample_names, yvars$stage, sep = \"_\")\nhead(sample_names)\n\n[1] \"1_WT_L4\"  \"2_Mut_L4\" \"3_Mut_L4\" \"4_WT_L4\"  \"5_Mut_L4\" \"6_WT_L4\" \n\n# use the sample names to create the groups vector\ngrp <- gsub(\"[0-9]+_\", \"\", sample_names)  # remove 1_ etc, leaving WT_L2 etc\ngroups <- as.factor(grp)\nlevels(groups)\n\n[1] \"Mut_L2\" \"Mut_L4\" \"WT_L2\"  \"WT_L4\" \n\n# set up the colors based on group membership\ndata(Col12)  # see ?colorSymbol for a swatch\ncolors <- grp\ncolors <- ifelse(colors == \"WT_L2\", Col12[1], colors)\ncolors <- ifelse(colors == \"WT_L4\", Col12[2], colors)\ncolors <- ifelse(colors == \"Mut_L2\", Col12[3], colors)\ncolors <- ifelse(colors == \"Mut_L4\", Col12[4], colors)\n\n# set up the symbols based on group membership\nsym <- grp  # see ?points for the symbol codes\nsym <- ifelse(sym == \"WT_L2\", 1, sym)\nsym <- ifelse(sym == \"WT_L4\", 16, sym)\nsym <- ifelse(sym == \"Mut_L2\", 0, sym)\nsym <- ifelse(sym == \"Mut_L4\", 15, sym)\nsym <- as.integer(sym)\n\n# set up the alt symbols based on group membership\nalt.sym <- grp\nalt.sym <- ifelse(alt.sym == \"WT_L2\", \"w2\", alt.sym)\nalt.sym <- ifelse(alt.sym == \"WT_L4\", \"w4\", alt.sym)\nalt.sym <- ifelse(alt.sym == \"Mut_L2\", \"m2\", alt.sym)\nalt.sym <- ifelse(alt.sym == \"Mut_L4\", \"m4\", alt.sym)\n\n# put it all together; see ?Spectra for requirements\nWorms <- list()\nWorms$freq <- freq\nWorms$data <- data\nWorms$names <- sample_names\nWorms$groups <- groups\nWorms$colors <- colors\nWorms$sym <- sym\nWorms$alt.sym <- alt.sym\nWorms$unit <- c(\"ppm\", \"intensity\")\nWorms$desc <- \"C. elegans metabolic phenotyping study (Blaise 2007)\"\nclass(Worms) <- \"Spectra\"\nchkSpectra(Worms)  # verify we have everything correct\nsumSpectra(Worms)\n\n\n C. elegans metabolic phenotyping study (Blaise 2007) \n\n    There are 139 spectra in this set.\n    The y-axis unit is intensity.\n\n    The frequency scale runs from\n    8.9995 to 5e-04 ppm\n    There are 8600 frequency values.\n    The frequency resolution is\n    0.001 ppm/point.\n\n    This data set is not continuous\n    along the frequency axis.\n    Here are the data chunks:\n\n  beg.freq end.freq   size beg.indx end.indx\n1   8.9995   5.0005 -3.999        1     4000\n2   4.5995   0.0005 -4.599     4001     8600\n\n    The spectra are divided into 4 groups: \n\n   group no.     color symbol alt.sym\n1 Mut_L2  32 #FB0D16FF      0      m2\n2 Mut_L4  33 #FFC0CBFF     15      m4\n3  WT_L2  34 #511CFCFF      1      w2\n4  WT_L4  40 #2E94E9FF     16      w4\n\n\n*** Note: this is an S3 object\nof class 'Spectra'"
  },
  {
    "objectID": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html#plot-it-to-check-it",
    "href": "posts/2022-02-01-Protocol-Pt1/2022-02-01-Protocol-Pt1.html#plot-it-to-check-it",
    "title": "Metabolic Phenotyping Protocol Part 1",
    "section": "Plot it to check it",
    "text": "Plot it to check it\nLet’s look at one sample from each group to make sure everything looks reasonable (Figure Figure 1). At least these four spectra look good. Note that we are using the latest ChemoSpec that uses ggplot2 graphics by default (announced here).\n\np <- plotSpectra(Worms, which = c(35, 1, 34, 2), lab.pos = 7.5, offset = 0.008, amplify = 35,\n    yrange = c(-0.05, 1.1))\np\n\n\n\n\nFigure 1: Sample spectra from each group.\n\n\n\n\nIn the next post we’ll continue with some basic exploratory data analysis.\n\nThis post was created using ChemoSpec version 6.1.3 and ChemoSpecUtils version 1.0.0."
  },
  {
    "objectID": "posts/2020-09-08-GSOC-hyperSpec/2020-09-08-GSOC-hyperSpec.html",
    "href": "posts/2020-09-08-GSOC-hyperSpec/2020-09-08-GSOC-hyperSpec.html",
    "title": "GSOC Wrap Up",
    "section": "",
    "text": "Well, things have been busy lately! As reported back in May, I’ve been participating in Google Summer of Code which has now wrapped up. This was very rewarding for me, but today I want to share a guest post by Erick Oduniyi, the very talented student on the project. Bryan\n\n\n\n\n\nChecking in from Kansas!\nThis past summer (2020) I had the amazing opportunity to participate in the Google Summer of Code (GSoC or GSOC). As stated on the the GSOC website, GSOC is a “global program focused on bringing more student developers into open source software development. Students work with an open-source organization on a 3-month programming project during their break from school.”\nThis was a particularly meaningful experience as it was my last undergraduate summer internship. I’m a senior studying computer engineering at the University of Kansas, and at the beginning of the summer I still didn’t feel super comfortable working on public (open-source) projects. So, I thought this program would help build my confidence as a computer and software engineer. Moreover:\n\nI wanted to work with the R organization because that is my favorite programming language.\nI wanted to work with r-hyperspec because I thought that would be the most impactful in terms of practicing project management and software ecosystem development.\n\nIn the process I hoped to:\n\nBecome proficient using Git/Github, including continuous integration\nBecome proficient in using Trello\nBecome proficient in using R\nBecome familiar with the spectroscopy community\nBecome inspired to code more\nBecome inspired to document and write more open source projects.\nBecome excited to collaborate more across various industrial, academic, and community domains.\n\nAnd through a lot of hard work all of those things came to be! Truthfully, even though the summer project was successful there is still a lot of work to do:\n\nFortify hyperSpec for baseline with bridge packages\nFortify hyperSpec for EMSC with bridge packages\nFortify hyperSpec for matrixStats with bridge packages.\n\nSo, I’m excited to continue to work with the team! I think there are a ton of ideas I and the team have and hopefully we will get to explore them in deeper context. Speaking of the team, I have them to thank for an awesome GSOC 2020 experience. If you are interested in the journey that was the GSoC 2020 experience (perhaps you might be interested in trying the program next year), then please feel free to jump around here to get a feel for the things that I learned and how I worked with the r-hyperspec team this summer.\nBest, E. Oduniyi\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{oduniyi2020,\n  author = {Erick Oduniyi},\n  editor = {},\n  title = {GSOC {Wrap} {Up}},\n  date = {2020-09-08},\n  url = {http://chemospec.org/2020-09-08-GSOC-hyperSpec.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nErick Oduniyi. 2020. “GSOC Wrap Up.” September 8, 2020. http://chemospec.org/2020-09-08-GSOC-hyperSpec.html."
  },
  {
    "objectID": "posts/2020-01-25-GH-Topics/2020-01-25-GH-Topics.html",
    "href": "posts/2020-01-25-GH-Topics/2020-01-25-GH-Topics.html",
    "title": "Exploring Github Topics",
    "section": "",
    "text": "As the code driving FOSS for Spectroscopy has matured, I began to think about how to explore Github in a systematic way for additional repositories with tools for spectroscopy. It turns out that a Github repo can have topics assigned to it, and you can use the Github API to search them. Wait, what? I didn’t know one could add topics to a repo, even though there is a little invite right there under the repo name:\n\nNaturally I turned to StackOverflow to find out how to do this, and quickly encountered this question. It was asked when the topics feature was new, so one needs to do things just a bit differently now, but there is a way forward.\nBefore we get to implementation, let’s think about limitations:\n\nThis will only find repositories where topics have been set. I don’t know how broadly people use this feature, I had missed it when it was added.\nGithub topics are essentially tags with a controlled vocabulary, so for the best results you’ll need to manually explore the tags and then use these as your search terms.\nThe Github API only returns 30 results at a time for most types of queries. For our purposes this probably doesn’t matter much. The documentation explains how to iterate to get all possible results.\nThe Github API also limits the number of queries you can make to 60/hr unless you authenticate, in which case the limit goes to 6000/hr.\n\nLet’s get to it! First, create a Github access token on your local machine using the instructions in this gist. Next, load the needed libraries:\n\n\n\n\nset.seed(123)\nlibrary(\"httr\")\nlibrary(\"knitr\")\nlibrary(\"kableExtra\")\n\nSpecify your desired search terms, and create a list structure to hold the results:\n\nsearch_terms <- c(\"NMR\", \"infrared\", \"raman\", \"ultraviolet\", \"visible\", \"XRF\", \"spectroscopy\")\nresults <- list()\n\nCreate the string needed to access the Github API, then GET the results, and stash them in the list we created:\n\nnt <- length(search_terms) # nt = no. of search terms\nfor (i in 1:nt) {\n  search_string <- paste0(\"https://api.github.com/search/repositories?q=topic:\", search_terms[i])\n    request <- GET(search_string, config(token = github_token))\n  stop_for_status(request) # converts http errors to R errors or warnings\n  results[[i]] <- content(request)\n}\nnames(results) <- search_terms\n\nFigure out how many results we have found, set up a data frame and then put the results into the table. The i, j, and k counters required a little experimentation to get right, as content(request) returns a deeply nested list and only certain items are desired.\n\nnr <- 0L # nr = no. of responses\nfor (i in 1:nt) { # compute total number of results/items found\n    nr <- nr + length(results[[i]]$items)\n}\n\nDF <- data.frame(term = rep(NA_character_, nr),\n  repo_name = rep(NA_character_, nr),\n  repo_url = rep(NA_character_, nr),\n  stringsAsFactors = FALSE)\n\nk <- 1L\nfor (i in 1:nt) {\n    ni <- length(results[[i]]$items) # ni = no. of items\n    for (j in 1:ni) {\n        DF$term[k] <- names(results)[[i]]\n        DF$repo_name[k] <- results[[i]]$items[[j]]$name\n        DF$repo_url[k] <- results[[i]]$items[[j]]$html_url\n        k <- k + 1L\n    }\n}\n# remove duplicated repos which result when repos have several of our\n# search terms of interest.\nDF <- DF[-which(duplicated(DF$repo_name)),]\n\nNow put it all in a table we can inspect manually, send to a web page so it’s clickable, or potentially write it out as a csv (If you want this as a csv you should probably write the results out a bit differently). In this case I want the results as a table in web page so I can click the repo links and go straight to them.\n\nnamelink <- paste0(\"[\", DF$repo_name, \"](\", DF$repo_url, \")\")\nDF2 <- data.frame(DF$term, namelink, stringsAsFactors = FALSE)\nnames(DF2) <- c(\"Search Term\", \"Link to Repo\")\n\nWe’ll show just 10 random rows as an example:\nkeep <- sample(1:nrow(DF2), 10)\noptions(knitr.kable.NA = '')\nkable(DF2[keep, ]) %>%\n  kable_styling(c(\"striped\", \"bordered\"))\n\n\n\n\n\n\n\n\nSearch Term\n\n\nLink to Repo\n\n\n\n\n\n\n31\n\n\ninfrared\n\n\npycroscopy\n\n\n\n\n79\n\n\nultraviolet\n\n\nwoudc-data-registry\n\n\n\n\n51\n\n\ninfrared\n\n\nir-repeater\n\n\n\n\n14\n\n\nNMR\n\n\nspectra-data\n\n\n\n\n67\n\n\nraman\n\n\nRaman-spectra\n\n\n\n\n42\n\n\ninfrared\n\n\nPrecIR\n\n\n\n\n50\n\n\ninfrared\n\n\nesp32-room-control-panel\n\n\n\n\n118\n\n\nspectroscopy\n\n\nLiveViewLegacy\n\n\n\n\n43\n\n\ninfrared\n\n\narduino-primo-tutorials\n\n\n\n\n101\n\n\nXRF\n\n\nweb_geochemistry\n\n\n\n\n\nObviously, these results must be inspected carefully as terms like “infrared” will pick up projects that deal with infrared remote control of robots and so forth. As far as my case goes, I have a lot of new material to look through…\nA complete .Rmd file that carries out the search described above, and has a few enhancements, can be found at this gist.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2020,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {Exploring {Github} {Topics}},\n  date = {2020-01-25},\n  url = {http://chemospec.org/2020-01-25-GH-Topics.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2020. “Exploring Github Topics.” January 25,\n2020. http://chemospec.org/2020-01-25-GH-Topics.html."
  },
  {
    "objectID": "posts/2020-01-24-CS-update/2020-01-24-CS-update.html",
    "href": "posts/2020-01-24-CS-update/2020-01-24-CS-update.html",
    "title": "ChemoSpec Update",
    "section": "",
    "text": "ChemoSpec has just been updated to version 5.2.12, and should be available on the mirrors shortly.\nThe most significant user-facing changes are actually in the update to ChemoSpecUtils from a few days ago. In addition, the following documentation changes were made:\n\nAdded documentation for updateGroups which has been in ChemoSpecUtils for a while but effectively hidden from users of ChemoSpec.\nFixed the example in mclustSpectra which had an error and used data that was not a good illustration.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2020,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {ChemoSpec {Update}},\n  date = {2020-01-24},\n  url = {http://chemospec.org/2020-01-24-CS-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2020. “ChemoSpec Update.” January 24, 2020.\nhttp://chemospec.org/2020-01-24-CS-update.html."
  },
  {
    "objectID": "posts/2021-03-27-Spec-Suite-update/2021-03-27-Spec-Suite-update.html",
    "href": "posts/2021-03-27-Spec-Suite-update/2021-03-27-Spec-Suite-update.html",
    "title": "Spectroscopy Suite Update",
    "section": "",
    "text": "My suite of spectroscopy R packages has been updated on CRAN. There are only a few small changes, but they will be important to some of you:\n\nChemoSpecUtils now provides a set of colorblind-friendly colors, see ?colorSymbol. These are available for use in ChemoSpec and ChemoSpec2D.\nAt the request of several folks, readJDX now includes a function, splitMultiblockDX, that will split a multiblock JCAMP-DX file into separate files, which can then be imported via the usual functions in the package.\nAll packages are built against the upcoming R 4.1 release (due in April).\n\nHere are the links to the documentation:\n\nChemoSpec\nChemSpec2D\nChemoSpecUtils\nreadJDX\n\nAs always, let me know if you discover trouble or have questions.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2021,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {Spectroscopy {Suite} {Update}},\n  date = {2021-03-27},\n  url = {http://chemospec.org/2021-03-27-Spec-Suite-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2021. “Spectroscopy Suite Update.” March 27,\n2021. http://chemospec.org/2021-03-27-Spec-Suite-update.html."
  },
  {
    "objectID": "posts/2021-10-13-GSOC-CS-Graphics/2021-10-13-GSOC-CS-Graphics.html",
    "href": "posts/2021-10-13-GSOC-CS-Graphics/2021-10-13-GSOC-CS-Graphics.html",
    "title": "GSOC 2021: New Graphics for ChemoSpec",
    "section": "",
    "text": "It’s been quiet around this blog because supervising two students for Google Summer of Code has kept me pretty busy! But we have some news…\n\n\n\n\nThanks to Mr. Tejasvi Gupta and the support of GSOC, ChemoSpec and ChemoSpec2D were extended to produce ggplot2 graphics and plotly graphics! ggplot2 is now the default output, and the ggplot2 object is returned, so if one doesn’t like the choice of theme or any other aspect, one can customize the object to one’s desire. The ggplot2 graphics output are generally similar in layout and spirit to the base graphics output, but significant improvements have been made in labeling data points using the ggrepel package. The original base graphics are still available as well. Much of this work required changes in ChemoSpecUtils which supports the common needs of both packages.\nTejasvi did a really great job with this project, and I think users of these packages will really like the results. We have greatly expanded the pre-release testing of the graphics, and as far as we can see every thing works as intended. Of course, please file an issue if you see any problems or unexpected behavior.\nTo see more about how the new graphics options work, take a look at GraphicsOptions. Here are the functions that were updated:\n\nplotSpectra\nsurveySpectra\nsurveySpectra2\nreviewAllSpectra (formerly loopThruSpectra)\nplotScree (resides in ChemoSpecUtils)\nplotScores (resides in ChemoSpecUtils)\nplotLoadings (uses patchwork and hence plotly isn’t available)\nplot2Loadings\nsPlotSpectra\npcaDiag\nplotSampleDist\naovPCAscores\naovPCAloadings (uses patchwork and hence plotly isn’t available)\n\nTejasvi and I are looking forward to your feedback. There are many other smaller changes that we’ll let users discover as they work. And there’s more work to be done, but other projects need attention and I need a little rest!\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2021,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {GSOC 2021: {New} {Graphics} for {ChemoSpec}},\n  date = {2021-10-13},\n  url = {http://chemospec.org/2021-10-13-GSOC-CS-Graphics.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2021. “GSOC 2021: New Graphics for\nChemoSpec.” October 13, 2021. http://chemospec.org/2021-10-13-GSOC-CS-Graphics.html."
  },
  {
    "objectID": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html",
    "href": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html",
    "title": "Metabolic Phenotyping Protocol Part 2",
    "section": "",
    "text": "Part 1 of this series is here.\nIf you aren’t familiar with ChemoSpec, you might wish to look at the introductory vignette first.\nIn this series of posts we are following the protocol as described in the printed publication closely (Blaise et al. 2021). The authors have also provided a Jupyter notebook. This is well worth your time, even if Python is not your preferred lanaguage, as there are additional examples and discussion for study."
  },
  {
    "objectID": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html#normalization-scaling",
    "href": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html#normalization-scaling",
    "title": "Metabolic Phenotyping Protocol Part 2",
    "section": "Normalization & Scaling",
    "text": "Normalization & Scaling\nApply PQN normalization; scaling in ChemoSpec is applied at the PCA stage (next).\n\nWorms <- normSpectra(Worms)  # PQN is the default"
  },
  {
    "objectID": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html#pca",
    "href": "posts/2022-03-24-Protocol-Pt2/2022-03-24-Protocol-Pt2.html#pca",
    "title": "Metabolic Phenotyping Protocol Part 2",
    "section": "PCA",
    "text": "PCA\nConduct classical PCA using autoscaling.1 Note that ChemoSpec includes several different variants of PCA, each with scaling options. See the introductory vignette for more details. For more about what PCA is and how it works, please see the LearnPCA package.\n\nc_pca <- c_pcaSpectra(Worms, choice = \"autoscale\")  # no scaling is the default\n\n\nComponents to Retain\nA key question at this stage is how many components are needed to describe the data set. Keep in mind that this depends on the choice of scaling. Figure 1 and Figure 2 are two different types of scree plots, which show the residual variance. This is the R2x value in the protocol (see protocol Figure 7a). Another approach to answering this question is to do a cross-validated PCA.2 The results are shown in Figure 3. These are the Q2x values in protocol Figure 7a. All of these ways of looking at the variance explained suggest that retaining three or possibly four PCs is adequate.\n\nplotScree(c_pca)\n\n\n\n\nFigure 1: Scree plot (recommended style).\n\n\n\n\n\nplotScree(c_pca, style = \"trad\")\n\n\n\n\nFigure 2: Scree plot (traditional style).\n\n\n\n\n\ncv_pcaSpectra(Worms, choice = \"autoscale\", pcs = 10)\n\n\n\n\nFigure 3: Scree plot using cross validation.\n\n\n\n\n\n\nScore Plots\nNext, examine the score plots (Figure 4, Figure 5). In these plots, each data point is colored by its group membership (keep in mind this is completely independent of the PCA calculation). In addition, robust confidence ellipses are shown for each group. Inspection of these plots is one way to identify potential outliers. The other use is of course to see if the sample classes separate, and by how much.\nExamination of these plots shows that separation by classes has not really been achieved using autoscaling. In Figure 4 we see four clear outlier candidates (samples 37, 101, 107, and 118). In Figure 5 we see some of these samples and should probably add sample 114 for a total of five candidates.\n\np <- plotScores(Worms, c_pca, pcs = 1:2, ellipse = \"rob\", tol = 0.02)\np\n\n\n\n\nFigure 4: Score plot for PCs 1 and 2. Compare to protocol figure 7a.\n\n\n\n\n\np <- plotScores(Worms, c_pca, pcs = 2:3, ellipse = \"rob\", leg.loc = \"topright\", tol = 0.02)\np\n\n\n\n\nFigure 5: Score plot for PCS 2 and 3.\n\n\n\n\nTo label more sample points, you can increase the value of the argument tol.\n\n\nOutliers\nThe protocol recommends plotting Hotelling’s T2 ellipse for the entire data set; this is not implemented in ChemoSpec but we can easily do it if we are using ggplot2 plots (which is the default in ChemoSpec). We need the ellipseCoord function from the HotellingsEllipse package.3\n\nsource(\"ellipseCoord.R\")\nxy_coord <- ellipseCoord(as.data.frame(c_pca$x), pcx = 1, pcy = 2, conf.limit = 0.95,\n    pts = 500)\np <- plotScores(Worms, c_pca, which = 1:2, ellipse = \"none\", tol = 0.02)\np <- p + geom_path(data = xy_coord, aes(x = x, y = y)) + scale_color_manual(values = \"black\")\np\n\n\n\n\nFigure 6: Score plot for PCs 1 and 2 with Hotelling’s T2 ellipse. Compare to protocol figure 7a.\n\n\n\n\nWe can see many of the same outliers by this approach as we saw in Figure 4 and Figure 5.\nAnother way to identify outliers is to use the approach described in Varmuza and Filzmoser (2009) section 3.7.3. Figure 7 and Figure 8 give the plots. Please see Filzmoser for the details, but any samples that are above the plotted threshold line are candidate outliers, and any samples above the threshold in both plots should be looked at very carefully. Though we are using classical PCA, Filzmoser recommends using these plots with robust PCA. These plots are a better approach than “eye balling it” on the score plots.\n\np <- pcaDiag(Worms, c_pca, plot = \"OD\")\np\n\n\n\n\nFigure 7: Orthogonal distance plot based on the first three PCs.\n\n\n\n\n\np <- pcaDiag(Worms, c_pca, plot = \"SD\")\np\n\n\n\n\nFigure 8: Score distance plot based on the first three PCs.\n\n\n\n\nComparison of these plots suggest that samples 37, 101, 107, 114 and 118 are likely outliers. These spectra should be examined to see if the reason for their outlyingness can be deduced. If good reason can be found, they can be removed as follows.4\n\nWorms2 <- removeSample(Worms, rem.sam = c(\"37_\", \"101_\", \"107_\", \"114_\", \"118_\"))\n\nAt this point one should repeat the PCA, score plots and diagnostic plots to get a good look at how removing these samples affected the results. Those tasks are left to the reader.\n\n\n\n\nWe will continue in the next post with a discussion of loadings.\n\n\n\n\nThis post was created using ChemoSpec version 6.1.3 and ChemoSpecUtils version 1.0.0."
  },
  {
    "objectID": "posts/2020-01-01-Intro-F4S/2021-01-01-Intro-F4S.html",
    "href": "posts/2020-01-01-Intro-F4S/2021-01-01-Intro-F4S.html",
    "title": "FOSS for Spectroscopy",
    "section": "",
    "text": "For this inaugural blog post, I’m pleased to share a project I have been working on recently: FOSS4Spectroscopy is an attempt to catalog FOSS spectroscopy software. The page is designed to be updated easily with new or edited entries – see the page for details, and please contribute!\nFor this initial version, I searched the CRAN ecosystem via packagefinder and the Python world via PyPi.org using spectroscopy-related keywords. My expertise is in R so I’m pretty confident I have most of the R packages included. I’m not so confident about coverage of the Python packages (where else should I look?).\nCurrently, the “status” column in the main table is empty. I am working on a version which will update the status with a date giving some indication of the age and activity of the repository. Right now the page is updated when I push to Github, but in the long run I hope to get Travis-CI to run it as a weekly cron job.\nI welcome your feedback in any form!\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2020,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {FOSS for {Spectroscopy}},\n  date = {2020-01-01},\n  url = {http://chemospec.org/2021-01-01-Intro-F4S.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2020. “FOSS for Spectroscopy.” January 1,\n2020. http://chemospec.org/2021-01-01-Intro-F4S.html."
  },
  {
    "objectID": "posts/2021-02-08-PLS/2021-02-08-PLS.html",
    "href": "posts/2021-02-08-PLS/2021-02-08-PLS.html",
    "title": "Interfacing ChemoSpec to PLS",
    "section": "",
    "text": "The ChemoSpec package carries out exploratory data analysis (EDA) on spectroscopic data. EDA is often described as “letting that data speak”, meaning that one studies various descriptive plots, carries out clustering (HCA) as well as dimension reduction (e.g. PCA), with the ultimate goal of finding any natural structure in the data.\nAs such, ChemoSpec does not feature any predictive modeling functions because other packages provide the necessary tools. I do however hear from users several times a year about how to interface a ChemoSpec object with these other packages, and it seems like a post about how to do this is overdue. I’ll illustrate how to carry out partial least squares (PLS) using data stored in a ChemoSpec object and the package chemometrics by Peter Filzmoser and Kurt Varmuza (Filzmoser and Varmuza 2017). One can also use the pls package (Mevik, Wehrens, and Liland 2020).\nPLS is a technique related to regression and PCA that tries to develop a mathematical model between a matrix of sample vectors, in our case, spectra, and one or more separately measured dependent variables that describe the same samples (typically, chemical analyses). If one can develop a reliable model, then going forward one can measure the spectrum of a new sample and use the model to predict the value of the dependent variables, presumably saving time and money. This post will focus on interfacing ChemoSpec objects with the needed functions in chemometrics. I won’t cover how to evaluate and refine your model, but you can find plenty on this in Varmuza and Filzmoser (2009) chapter 4, along with further background (there’s a lot of math in there, but if you aren’t too keen on the math, gloss over it to get the other nuggets). Alternatively, take a look at the vignette that ships with chemometrics via browseVignettes(\"chemometrics\").\nAs our example we’ll use the marzipan NIR data set that one can download in Matlab format from here.1 The corresponding publication is (Christensen et al. 2004). This data set contains NIR spectra of marzipan candies made with different recipes and recorded using several different instruments, along with data about moisture and sugar content. We’ll use the data recorded on the NIRSystems 6500 instrument, covering the 400-2500 nm range. The following code chunk gives a summary of the data set and shows a plot of the data. Because we are focused on how to carry out PLS, we won’t worry about whether this data needs to be normalized or otherwise pre-processed (see the Christensen paper for lots of details).\n\nlibrary(\"ChemoSpec\")\n\nLoading required package: ChemoSpecUtils\n\n\n\nAs of version 6, ChemoSpec offers new graphics output options\n\n\nFor details, please see ?GraphicsOptions\n\n\n\nThe ChemoSpec graphics option is set to 'ggplot2'\n\n\nTo change it, do\n    options(ChemoSpecGraphics = 'option'),\n    where 'option' is one of 'base' or 'ggplot2' or'plotly'.\n\nload(\"Marzipan.RData\")\nsumSpectra(Marzipan)\n\n\n Marzipan NIR data set from www.models.life.ku.dk/Marzipan \n\n    There are 32 spectra in this set.\n    The y-axis unit is absorbance.\n\n    The frequency scale runs from\n    450 to 2448 wavelength (nm)\n    There are 1000 frequency values.\n    The frequency resolution is\n    2 wavelength (nm)/point.\n\n\n    The spectra are divided into 9 groups: \n\n  group no.     color symbol alt.sym\n1     a   5 #FB0D16FF      1       a\n2     b   4 #FFC0CBFF     16       b\n3     c   4 #2AA30DFF      2       c\n4     d   4 #9BCD9BFF     17       d\n5     e   3 #700D87FF      3       e\n6     f   3 #A777F2FF      8       f\n7     g   2 #FD16D4FF      4       g\n8     h   3 #B9820DFF      5       h\n9     i   4 #B9820DFF      5       i\n\n\n*** Note: this is an S3 object\nof class 'Spectra'\n\nplotSpectra(Marzipan, which = 1:32, lab.pos = 3000)\n\n\n\n\nIn order to carry out PLS, one needs to provide a matrix of spectroscopic data, with samples in rows (let’s call it \\(X\\), you’ll see why in a moment). Fortunately this data is available directly from the ChemoSpec object as Marzipan$data.2 One also needs to provide a matrix of the additional dependent data (let’s call it \\(Y\\)). It is critical that the order of rows in \\(Y\\) correspond to the order of rows in the matrix of spectroscopic data, \\(X\\).\nSince we are working in R we know there are a lot of ways to do most tasks. Likely you will have the additional data in a spreadsheet, so let’s see how to bring that into the workspace. You’ll need samples in rows, and variables in columns. For your sanity and error-avoidance, you should include a header of variable names and the names of the samples in the first column. Save the spreadsheet as a csv file. I did these steps using the sugar and moisture data from the original paper. Read the file in as follows.\n\nY <- read.csv(\"Marzipan.csv\", header = TRUE)\nstr(Y)\n\n'data.frame':   32 obs. of  3 variables:\n $ sample  : chr  \"a1\" \"a2\" \"a3\" \"a4\" ...\n $ sugar   : num  32.7 34.9 33.9 33.2 33.2 ...\n $ moisture: num  15 14.9 14.7 14.9 14.9 ...\n\n\nThe function we’ll be using wants a matrix as input, so convert the data frame that read.csv generates to a matrix. Note that we’ll select only the numeric variables on the fly, as unlike a data frame, a matrix can only be composed of one data type.\n\nY <- as.matrix(Y[, c(\"sugar\", \"moisture\")])\nstr(Y)\n\n num [1:32, 1:2] 32.7 34.9 33.9 33.2 33.2 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:2] \"sugar\" \"moisture\"\n\n\nNow we are ready to carry out PLS. Since we have a multivariate \\(Y\\), we need to use the appropriate function (use pls1_nipals if your \\(Y\\) matrix is univariate).\n\nlibrary(\"chemometrics\")\n\nLoading required package: rpart\n\npls_out <- pls2_nipals(X = Marzipan$data, Y, a = 5)\n\nAnd we’re done! Be sure to take a look at str(pls_out) to see what you got back from the calculation. For the next steps in evaluating your model, see section 3.3 in the chemometrics vignette.\n\n\n\n\n\nReferences\n\nChristensen, Jakob, Lars Nørgaard, Hanne Heimdal, Joan Grønkjær Pedersen, and Søren Balling Engelsen. 2004. “Rapid Spectroscopic Analysis of Marzipan-Comparative Instrumentation.” Journal of Near Infrared Spectroscopy 12 (1): 63–75. https://doi.org/10.1255/jnirs.408.\n\n\nFilzmoser, Peter, and Kurt Varmuza. 2017. Chemometrics: Multivariate Statistical Analysis in Chemometrics. https://CRAN.R-project.org/package=chemometrics.\n\n\nMevik, Bjørn-Helge, Ron Wehrens, and Kristian Hovde Liland. 2020. Pls: Partial Least Squares and Principal Component Regression. https://CRAN.R-project.org/package=pls.\n\n\nVarmuza, K., and P. Filzmoser. 2009. Introduction to Multivariate Statistical Analysis in Chemometrics. CRC Press.\n\nFootnotes\n\n\nI have converted the data from Matlab to a ChemoSpec object; if anyone wants to know how to do this let me know and I’ll put up a post on that process.↩︎\nstr(Marzipan) will show you the structure of the ChemoSpec object (or in general, any R object). The official definition of a ChemoSpec object can be seen via ?Spectra.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2021,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {Interfacing {ChemoSpec} to {PLS}},\n  date = {2021-02-08},\n  url = {http://chemospec.org/2021-02-08-PLS.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2021. “Interfacing ChemoSpec to PLS.”\nFebruary 8, 2021. http://chemospec.org/2021-02-08-PLS.html."
  },
  {
    "objectID": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html",
    "href": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html",
    "title": "Aligning 2D NMR Spectra Part 2",
    "section": "",
    "text": "This is Part 2 of a series on aligning 2D NMR, as implemented in the package ChemoSpec2D. Part 1 Part 3"
  },
  {
    "objectID": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#the-hats-pr-algorithm",
    "href": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#the-hats-pr-algorithm",
    "title": "Aligning 2D NMR Spectra Part 2",
    "section": "The HATS-PR Algorithm",
    "text": "The HATS-PR Algorithm\nIn Part 1 I briefly mentioned that we would be using the HATS-PR algorithm of Robinette et al. (Robinette et al. 2011). I also discussed the choice of objective function which is used to report on the quality of the alignment. HATS-PR stands for “Hierachical Alignment of Two-Dimensional Spectra - Pattern Recognition”. In ChemoSpec2D the algorithm is implemented in the hats_alignSpectra2D function. Here are the major steps of the HATS-PR algorithm:\n\nContruct a guide tree using hierarchical clustering (HCA): compute the distance between the spectra, and use these distances to construct a dendrogram (the guide tree). As the name suggests, this tree is used to guide the alignment. The most similar spectra are aligned first, then the next most similar, and so on. In later rounds one applies the alignment procedure to sets of spectra that have already been aligned. In Robinette et al. they use the Pearson correlation coefficient as the distance measure. In ChemoSpec2D you can choose from a number of distance measures. I encourage you to experiment with the choices and see how they affect the alignment process for your data sets.\nFor each alignment event, check the alignment using the objective function, which recall is a distance measure. If the objective function is below the threshold, no alignment is needed (“below” assumes we are minimizing the objective function, but we might also be maximizing and hence trying to exceed the threshold). If alignment is necessary, move one of the spectra relative to the other in some fashion, checking each new postion with the objective function until the best alignment is found. This is an exercise in optimization."
  },
  {
    "objectID": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#finding-the-optimal-alignment",
    "href": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#finding-the-optimal-alignment",
    "title": "Aligning 2D NMR Spectra Part 2",
    "section": "Finding the Optimal Alignment",
    "text": "Finding the Optimal Alignment\nThe heart of the task is in the phrase in some fashion. At one extreme, one can imagine holding one spectrum fixed, and sliding the other spectrum left and right, up and down, over some range of values – essentially a grid of data points. At each position on the virtual grid, evaluate the objective function and keep track of the results. This will always find the answer, but such a brute force search will be very time-consuming and undesirable, especially if the search space is large. Alternatively, do something more efficient! Robinette et al. use a “simple gradient ascent” approach, but there is a vast literature on optimization strategies that we can consider. In ChemoSpec2D we use a machine learning approach (details next), but the function is written in such a way that one can add other optimization approaches seamlessly. Anything is better than a brute force approach."
  },
  {
    "objectID": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#optimizing-with-mlrmbo",
    "href": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#optimizing-with-mlrmbo",
    "title": "Aligning 2D NMR Spectra Part 2",
    "section": "Optimizing with mlrMBO",
    "text": "Optimizing with mlrMBO\nThe name mlrMBO comes from “machine learning with R model-based optimization.” mlrMBO is a powerful and flexible package for general purpose optimization, especially in the cases where the objective function is computationally expensive. There is a nice introductory vignette.\nThe basic steps in the model-based optimization using mlrMBO as implemented in hats_alignSpectra2D in package ChemoSpec2D are as follows:\n\nDefine your objective function. Our choice of the Euclidean distance was described in Part 1, along with other options. Most distance measures are not computationally expensive in terms of code. However, the huge number of data points in a typical 2D NMR spectrum bogs things down considerably. The approach taken in model-based optimation mitigates this to a great deal, since the objective function is only used for the initial response surface.\nGenerate an “initial design”, by which we mean a strategy to search the possible optimization space. hats_alignSpectra2D takes arguments maxF1 and maxF2 which define the space that will be considered as the two spectra are shifted relative to each other. The space potentially covered is -maxF1 to maxF1 and similarly for the F2 dimension. We take advantage of concepts from the design of experiments field, and use the lhs package to generate a Latin Hypercube Sample of our space.\nThe sample points selected by lhs are evaluated using the objective function.\nThe values of the objective function at the sample points are used to create a surrogate model, essentially a response surface. The key here is that the surrogate model is computationally fast and will stand in for the actual objective function during the optimization. mlrMBO provides many options for the surrogate model. For hats_alignSpectra2D we use a response surface based on kriging, which is a means of interpolating values that was originally developed in the geospatial statistics world.\nNew samples points are suggested by the kriging algorithm, evaluated using the surrogate function, and used to update (improve) the model. Each iteration improves the quality of the model.\nAfter reaching the designated threshold or the number of iterations specified, the best answer is returned. In this case the best answer is the optimal shift of one spectrum relative to the other, in each dimension."
  },
  {
    "objectID": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#other-details",
    "href": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#other-details",
    "title": "Aligning 2D NMR Spectra Part 2",
    "section": "Other Details",
    "text": "Other Details\nIn addition to the differences noted above, the implementation of HATS-PR in ChemoSpec2D carries out only global alignment. The algorithm described by Robinette et al. includes local alignment steps which I have not implemented. Local alignment is a possible future addition."
  },
  {
    "objectID": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#configure-your-workspace",
    "href": "posts/2020-03-02-NMR-Align-Pt2/2020-03-02-NMR-Align-Pt2.html#configure-your-workspace",
    "title": "Aligning 2D NMR Spectra Part 2",
    "section": "Configure Your Workspace",
    "text": "Configure Your Workspace\nIf you are going to actually execute the code here (as opposed to just reading along), you’ll need the development version of ChemoSpec2D (I improved some of the plots that track the alignment progress since the last CRAN release). And you’ll need certain packages. Here are the steps to install everything:\n\nchooseCRANmirror() # choose a CRAN mirror\ninstall.packages(\"remotes\")\nlibrary(\"remotes\")\n# devel branch -- you need 0.4.156 or higher\ninstall_github(repo = \"bryanhanson/ChemoSpec2D@devel\")\nlibrary(\"ChemoSpec2D\")\n# other packages needed\ninstall.packages(\"mlrMBO\") # will also install mlr, smoof, ParamHelpers\ninstall.packages(\"lhs\")\n\nNow you are ready for the main event! Part 3"
  },
  {
    "objectID": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html",
    "href": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html",
    "title": "Notes on Linear Algebra Part 1",
    "section": "",
    "text": "If you are already familiar with much of linear algebra, as well as the relevant functions in R, read no further and do something else!\nIf you are like me, you’ve had no formal training in linear algebra, which means you learn what you need to when you need to use it. Eventually, you cobble together some hard-won knowledge. That’s good, because almost everything in chemometrics involves linear algebra.\nThis post is essentially a set of personal notes about the dot product and the cross product, two important manipulations in linear algebra. I’ve tried to harmonize things I learned way back in college physics and math courses, and integrate information I’ve found in various sources I have leaned on more recently. Without a doubt, the greatest impediment to really understanding this material is the use of multiple terminology and notations. I’m going to try really hard to be clear and to the point in my dicussion.\nThe main sources I’ve relied on are:\nLet’s get started. For sanity and consistency, let’s define two 3D vectors and two matrices to illustrate our examples. Most of the time I’m going to write vectors with an arrow over the name, as a nod to the treatment usually given in a physics course. This reminds us that we are thinking about a quantity with direction and magnitude in some coordinate system, something geometric. Of course in the R language a vector is simply a list of numbers with the same data type; R doesn’t care if a vector is a vector in the geometric sense or a list of states.\n\\[\n\\vec{u} = (u_x, u_y, u_z)\n\\tag{1}\\]\n\\[\n\\vec{v} = (v_x, v_y, v_z)\n\\tag{2}\\]\n\\[\n\\mathbf{A} =\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\n\\end{bmatrix}\n\\tag{3}\\]\n\\[\n\\mathbf{B} =\n\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} \\\\\nb_{21} & b_{22} & b_{23} \\\\\nb_{31} & b_{32} & b_{33} \\\\\n\\end{bmatrix}\n\\tag{4}\\]"
  },
  {
    "objectID": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#dot-product",
    "href": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#dot-product",
    "title": "Notes on Linear Algebra Part 1",
    "section": "Dot Product",
    "text": "Dot Product\n\nTerminology\nThe dot product goes by these other names: inner product, scalar product. Typical notations include:1\n\n\\(\\vec{u} \\cdot \\vec{v}\\) (the \\(\\cdot\\) is the origin of the name “dot” product)\n\\(u \\cdot v\\)\n\\(u^\\mathsf{T}v\\) (when thinking of the vectors as column vectors)\n\\(\\langle u, v \\rangle\\) (typically used when \\(u, v\\) are complex)\n\\(\\langle u | v \\rangle\\)\n\n\n\nFormulas\nThere are two main formulas for the dot product with vectors, the algebraic formula (Equation 5) and the geometric formula (Equation 6).\n\\[\n\\vec{u} \\cdot \\vec{v} = \\sum{u_i v_i}\n\\tag{5}\\]\n\\[\n\\vec{u} \\cdot \\vec{v} = \\| \\vec{u} \\| \\| \\vec{v} \\| cos \\theta\n\\tag{6}\\]\n\\(\\| \\vec{x} \\|\\) refers to the \\(L_2\\) or Euclidian norm, namely the length of the vector:2\n\\[\n\\| \\vec{x} \\| =  \\sqrt{x^{2}_1 + \\ldots + x^{2}_n}\n\\tag{7}\\]\nThe result of the dot product is a scalar. The dot product is also commutative: \\(\\vec{u} \\cdot \\vec{v} = \\vec{v} \\cdot \\vec{u}\\).\n\n\n\n\n\n\nWatch out when using row or column vectors\n\n\n\n\n\nFrom the perspective of matrices, if we think of \\(\\vec{u}\\) and \\(\\vec{v}\\) as column vectors with dimensions 3 x 1, then transposing \\(\\vec{u}\\) gives us conformable matrices and we find the result of matrix multiplication is the dot product (compare to Equation 5):\n\\[\n\\vec{u} \\cdot \\vec{v} = \\vec{u}^\\mathsf{T} \\ \\vec{v} = \\begin{bmatrix} u_x & u_y & u_z\\end{bmatrix} \\begin{bmatrix} v_x \\\\ v_y \\\\ v_z \\end{bmatrix} = u_x v_x + u_y v_y + u_z v_z\n\\tag{8}\\]\nEven though this is matrix multiplication, the answer is still a scalar.\nNow, rather confusingly, if we think of \\(\\vec{u}\\) and \\(\\vec{v}\\) as row vectors, and we transpose \\(\\vec{v}\\),then we get the dot product:\n\\[\n\\vec{u} \\cdot \\vec{v} = \\vec{u} \\ \\vec{v}^\\mathsf{T} = \\begin{bmatrix} u_x & u_y & u_z\\end{bmatrix} \\begin{bmatrix} v_x \\\\ v_y \\\\ v_z \\end{bmatrix} = u_x v_x + u_y v_y + u_z v_z\n\\tag{9}\\]\nEquations Equation 8 and Equation 9 can be a source of real confusion at first. They give the impression that the dot product can be either \\(\\vec{u}^\\mathsf{T} \\ \\vec{v}\\) or \\(\\vec{u} \\ \\vec{v}^\\mathsf{T}\\). However, this is only true in the limited contexts defined above. To summarize:\n\nThinking of the vectors as column vectors with dimensions \\(n \\times 1\\) then one can use \\(\\vec{u}^\\mathsf{T} \\ \\vec{v}\\)\nThinking of the vectors as row vectors with dimensions \\(1 \\times n\\) then one can use \\(\\vec{u} \\ \\vec{v}^\\mathsf{T}\\)\n\nUnfortunately I think this distinction is not always clearly made by authors, and is a source of great confusion to linear algebra learners. Be careful when working with row and column vectors.\n\n\n\n\n\nMatrix Multiplication\nSuppose we wanted to compute \\(\\mathbf{A}\\mathbf{B} = \\mathbf{C}\\).3 We use the idea of row and column vectors to accomplish this task. In the process, we discover that matrix multiplication is a series of dot products:\n\\[\n\\begin{multline}\n\\mathbf{A}\\mathbf{B} = \\mathbf{C} =\n\\begin{bmatrix}\n\\textcolor{red}{a_{11}} & \\textcolor{red}{a_{12}} & \\textcolor{red}{a_{13}} \\\\\na_{21} & a_{22} & a_{23} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\textcolor{red}{b_{11}} & b_{12} & b_{13} \\\\\n\\textcolor{red}{b_{21}} & b_{22} & b_{23} \\\\\n\\textcolor{red}{b_{31}} & b_{32} & b_{33} \\\\\n\\end{bmatrix} = \\\\\n\\begin{bmatrix}\n\\textcolor{red}{a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31}} & a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32} & a_{11}b_{13} + a_{12}b_{23} + a_{13}b_{33}\\\\\na_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31} & a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32} & a_{21}b_{13} + a_{22}b_{23} + a_{23}b_{33}\\\\\n\\end{bmatrix}\n\\end{multline}\n\\tag{10}\\]\nThe red color shows how the dot product of the first row of \\(\\mathbf{A}\\) and the first column of \\(\\mathbf{B}\\) gives the first entry in \\(\\mathbf{C}\\). Every entry in \\(\\mathbf{C}\\) results from a dot product. Every entry is a scalar, embedded in a matrix.\n\n\nWhat Can We Do With the Dot Product?\n\nDetermine the angle between two vectors, as in Equation 6.\nAs such, determine if two vectors intersect at a right angle (at least in 2-3D). More generally, two vectors of any dimension are orthogonal if their dot product is zero.\nMatrix multiplication, when applied repeatedly.\nCompute the length of a vector, via \\(\\sqrt{v \\cdot v}\\)\nCompute the projection of one vector on another, for instance how much of a force is along the \\(x\\)-direction? A verbal interpretation of \\(\\vec{u} \\cdot \\vec{v}\\) is it gives the amount of \\(\\vec{v}\\) in the direction of \\(\\vec{u}\\)."
  },
  {
    "objectID": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#cross-product",
    "href": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#cross-product",
    "title": "Notes on Linear Algebra Part 1",
    "section": "Cross Product",
    "text": "Cross Product\n\nTerminology and Notation\nThe cross product goes by these other names: outer product4, tensor product, vector product.\n\n\nFormulas\nThe cross product of two vectors returns a vector rather than a scalar. Vectors are defined in terms of a basis which is a coordinate system. Earlier, when we defined \\(\\vec{u} = (u_x, u_y, u_z)\\) it was intrinsically defined in terms of the standard basis set \\(\\hat{i}, \\hat{j}, \\hat{k}\\) (in some fields this would be called the unit coordinate system). Thus a fuller definition of \\(\\vec{u}\\) would be:\n\\[\n\\vec{u} = u_x\\hat{i} + u_y\\hat{j} + u_z\\hat{k}\n\\tag{11}\\]\nIn terms of vectors, the cross product is defined as: \\[\n\\vec{u} \\times \\vec{v} = (a_{y}b_{z} - a_{z}b_{y})\\hat{i} + (a_{z}b_{x} - a_{x}b_{z})\\hat{j} + (a_{x}b_{y} -a_{y}b_{x})\\hat{k}\n\\tag{12}\\]\nIn my opinion, this is not exactly intuitive, but there is a pattern to it: notice that the terms for \\(\\hat{i}\\) don’t involve the \\(x\\) component. The details of how this result is computed relies on some properties of the basis set; this Wikipedia article has a nice explanation. We need not dwell on it however.\nThere is also a geometric formula for the cross product:\n\\[\n\\vec{u} \\times \\vec{v} = \\| \\vec{u} \\| \\| \\vec{v} \\| sin \\theta \\hat{n}\n\\tag{13}\\]\nwhere \\(\\hat{n}\\) is the unit vector perpendicular to the plane defined by \\(\\vec{u}\\) and \\(\\vec{v}\\). The direction of \\(\\hat{n}\\) is defined by the right-hand rule. Because of this, the cross product is not commutative, i.e. \\(\\vec{u} \\times \\vec{v} \\ne \\vec{v} \\times \\vec{u}\\). The cross product is however anti-commutative: \\(\\vec{u} \\times \\vec{v} = - (\\vec{v} \\times \\vec{u})\\)\n\n\n\n\n\n\nCross product using column vectors\n\n\n\n\n\nAs we did for the dot product, we can look at the cross product from the perspective of column vectors. Instead of transposing the first matrix as we did for the dot product, we transpose the second one: \\[\n\\vec{u} \\times \\vec{v} = \\vec{u} \\ \\vec{v}^\\mathsf{T} = \\begin{bmatrix} u_x \\\\ u_y \\\\ u_z\\end{bmatrix} \\begin{bmatrix} v_x & v_y & v_z \\end{bmatrix} = \\begin{bmatrix} u_{x}v_{x} & u_{x}v_{y} & u_{x}v_{z} \\\\ u_{y}v_{x} & u_{y}v_{y} & u_{y}v_{z} \\\\ u_{z}v_{x} & u_{z}v_{y} & u_{z}v_{z}\\\\ \\end{bmatrix}\n\\tag{14}\\]\nInterestingly, we are using the dot product to compute the cross product.\nThe case where we treat \\(\\vec{u}\\) and \\(\\vec{v}\\) as row vectors is left to the reader.5\n\n\n\nFinally, there is a matrix definition of the cross product as well. Evaluation of the following determinant gives the cross product:\n\\[\n\\vec{u} \\times \\vec{v} = \\begin{vmatrix} \\hat{i} & \\hat{j} & \\hat{k}\\\\ u_{x} & u_{y} & u_{z} \\\\ v_{x} & v_{y} & v_{z}\\\\\\end{vmatrix}\n\\]\n\n\nWhat Can We Do With the Cross Product?\n\nIn 3D, the result of the cross product is perpendicular or normal to the plane defined by the two input vectors.\nIf however, the two vectors are parallel or anti-parallel, the cross product is zero.\nThe length of the cross product is the area of the parallelogram defined by the two input vectors: \\(\\| \\vec{u} \\times \\vec{v} \\|\\)"
  },
  {
    "objectID": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#r-functions",
    "href": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#r-functions",
    "title": "Notes on Linear Algebra Part 1",
    "section": "R Functions",
    "text": "R Functions\n\n%*%\nThe workhorse for matrix multiplication in R is the %*% function. This function will accept any combination of vectors and matrices as inputs, so it is flexible. It is also smart: given a vector and a matrix, the vector will be treated as row or column matrix as needed to ensure conformity, if possible. Let’s look at some examples:\n\n# Some data for examples\np <- 1:5\nq <- 6:10\nM <- matrix(1:15, nrow = 3, ncol = 5)\nM\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    4    7   10   13\n[2,]    2    5    8   11   14\n[3,]    3    6    9   12   15\n\n\n\n# A vector times a vector\np %*% q\n\n     [,1]\n[1,]  130\n\n\nNotice that R returns a data type of matrix, but it is a \\(1 \\times 1\\) matrix, and thus a scalar value. That means we just computed the dot product, a descision R made internally. We can verify this by noting that q %*% p gives the same answer. Thus, R handled these vectors as column vectors and computed \\(p^{\\intercal}q\\).\n\n# A vector times a matrix\nM %*% p\n\n     [,1]\n[1,]  135\n[2,]  150\n[3,]  165\n\n\nAs M had dimensions \\(3 \\times 5\\), R treated p as a \\(5 \\times 1\\) column vector in order to be conformable. The result is a \\(3 \\times 1\\) vector, so this is the cross product.\nIf we try to compute p %*% M we get an error, because there is nothing R can do to p which will make it conformable to M.\n\np %*% M\n\nError in p %*% M: non-conformable arguments\n\n\nWhat about multiplying matrices?\n\nM %*% M\n\nError in M %*% M: non-conformable arguments\n\n\nAs you can see, when dealing with matrices, %*% will not change a thing, and if your matrices are non-conformable then it’s an error. Of course, if we transpose either instance of M we do have conformable matrices, but the answers are different, and this is neither the dot product or the cross product, just matrix multiplication.\n\nt(M) %*% M\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   14   32   50   68   86\n[2,]   32   77  122  167  212\n[3,]   50  122  194  266  338\n[4,]   68  167  266  365  464\n[5,]   86  212  338  464  590\n\nM %*% t(M)\n\n     [,1] [,2] [,3]\n[1,]  335  370  405\n[2,]  370  410  450\n[3,]  405  450  495\n\n\nWhat can we take from these examples?\n\nR will give you the dot product if you give it two vectors. Note that this is a design decision, as it could have returned the cross product (see Equation 14).\nR will promote a vector to a row or column vector if it can to make it conformable with a matrix you provide. If it cannot, R will give you an error. If it can, the cross product is returned.\nWhen it comes to two matrices, R will give an error when they are not conformable.\nOne function, %*%, does it all: dot product, cross product, or matrix multiplication, but you need to pay attention.\nThe documentation says as much, but more tersely: “Multiplies two matrices, if they are conformable. If one argument is a vector, it will be promoted to either a row or column matrix to make the two arguments conformable. If both are vectors of the same length, it will return the inner product (as a matrix)”\n\n\n\nOther Functions\nThere are other R functions that do some of the same work:\n\ncrossprod equivalent to t(M) %*% M but faster.\ntcrossprod equivalent to M %*% t(M) but faster.\nouter or %o%\n\nThe first two functions will accept combinations of vectors and matrices, as does %*%. Let’s try it with two vectors:\n\ncrossprod(p, q)\n\n     [,1]\n[1,]  130\n\n\nHuh. crossprod is returning the dot product! So this is the case where “the cross product is not the cross product.” From a clarity perspective, this is not ideal. Let’s try the other function:\n\ntcrossprod(p, q)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    6    7    8    9   10\n[2,]   12   14   16   18   20\n[3,]   18   21   24   27   30\n[4,]   24   28   32   36   40\n[5,]   30   35   40   45   50\n\n\nThere’s the cross product!\nWhat about outer? Remember that another name for the cross product is the outer product. So is outer the same as tcrossprod? In the case of two vectors, it is:\n\nidentical(outer(p, q), tcrossprod(p, q))\n\n[1] TRUE\n\n\nWhat about a vector with a matrix?\n\ntst <- outer(p, M)\ndim(tst)\n\n[1] 5 3 5\n\n\nAlright, that clearly is not a cross product. The result is an array with dimensions \\(5 \\times 3 \\times 5\\), not a matrix (which would have only two dimensions). outer does correspond to the cross product in the case of two vectors, but anything with higher dimensions gives a different beast. So perhaps using “outer” as a synonym for cross product is not a good idea."
  },
  {
    "objectID": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#advice",
    "href": "posts/2022-08-14-Linear-Alg-Notes/2022-08-14-Linear-Alg-Notes.html#advice",
    "title": "Notes on Linear Algebra Part 1",
    "section": "Advice",
    "text": "Advice\nGiven what we’ve seen above, make your life simple and stick to %*%, and pay close attention to the dimensions of the arguments, especially if row or column vectors are in use. In my experience, thinking about the units and dimensions of whatever it is you are calculating is very helpful. Later, if speed is really important in your work, you can use one of the faster alternatives."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "",
    "text": "If you aren’t familiar with it, the FOSS for Spectroscopy web site lists Free and Open Source Software for spectroscopic applications. The collection is of course never really complete, and your package suggestions are most welcome (how to contribute). My methods for finding packages are improving and at this point the major repositories have been searched reasonably well.\nA few days ago I pushed a major update, and at this point Python packages outnumber R packages more than two to one. The update was made possible because I recently had time to figure out how to search the PyPi.org site automatically.\nIn a previous post I explained the methods I used to find packages related to spectroscopy. These have been updated considerably and the rest of this post will cover the updated methods."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#repos-topics",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#repos-topics",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Repos & Topics",
    "text": "Repos & Topics\nThere are four places I search for packages related to spectroscopy.1\n\nCRAN, searched manually using the packagefinder package.2\nGithub, searched using custom functions and scripts, detailed below.\nPyPi.org, searched as for Github.\njuliapackages.org, searched manually.\n\nThe topics I search are as follows:\n\nNMR\nEPR\nESR\nUV\nVIS\nspectrophotometry\nNIR (IR search terms overlap a lot, and also generate many false positives dealing with IR communications, e.g. TV remotes)\nFT-IR\nFTIR\nRaman\nXRF\nXAS\nLIBS (on PyPi.org one must use “laser induced breakdown spectroscopy” because LIBS is the name of a popular software and generates hundreds of false positives)"
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-cran",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-cran",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Searching CRAN",
    "text": "Searching CRAN\nI search CRAN using packagefinder; the process is quite straightforward and won’t be covered here. However, it is not an automated process (I should probably work on that)."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-github",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-github",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Searching Github",
    "text": "Searching Github\nThe broad approach used to search Github is the same as described in the original post. However, the scripts have been refined and updated, and now exist as functions in a new package I created called webu (for “webutilities”, but that name is taken on CRAN). The repo is here. webu is not on CRAN and I don’t currently intend to put it there, but you can install from the repo of course if you wish to try it out.\nSearching Github is now carried out by a supervising script called /utilities/run_searches.R (in the FOSS4Spectroscopy repo). The script contains some notes about finicky details, but is pretty simple overall and should be easy enough to follow."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-pypi.org",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-pypi.org",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Searching PyPi.org",
    "text": "Searching PyPi.org\nUnlike Github, it is not necessary to authenticate to use the PyPi.org API. That makes things simpler than the Github case. The needed functions are in webu and include some deliberate delays so as to not overload their servers. As for Github, searches are supervised by /utilities/run_searches.R.\nOne thing I observed at PyPi.org is that authors do not always fill out all the fields that PyPi.org can accept, which means some fields are NULL and we have to trap for that possibility. Package information is accessed via a JSON record, for instance the entry for nmrglue can be seen here. This package is pretty typical in that the author_email field is filled out, but the maintainer_email field is not (they are presumably the same). If one considers these JSON files to be analogous to DESCRIPTION in R packages, it looks like there is less oversight on PyPi.org compared to CRAN."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-julia",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#searching-julia",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Searching Julia",
    "text": "Searching Julia\nJulia packages are readily searched manually at juliapackages.org."
  },
  {
    "objectID": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#cleaning-final-vetting",
    "href": "posts/2022-07-06-F4S-Update/2022-07-06-F4S-Update.html#cleaning-final-vetting",
    "title": "FOSS4Spectroscopy: R vs Python",
    "section": "Cleaning & Final Vetting",
    "text": "Cleaning & Final Vetting\nThe raw results from the searches described above still need a lot of inspection and cleaning to be usable. The PyPi.org and Github results are saved in an Excel worksheet with the relevant URLs. These links can be followed to determine the suitability of each package. In the /Utilities folder there are additional scripts to remove entries that are already in the main database (FOSS4Spec.xlsx), as well as to check the names of the packages: Python authors and/or policies seem to lead to cases where different packages can have names differing by case, but also authors are sometimes sloppy when referring to their own packages, sometimes using mypkg and at other times myPkg to refer to the same package."
  },
  {
    "objectID": "posts/2020-02-19-CS2D-update/2020-02-19-CS2D-update.html",
    "href": "posts/2020-02-19-CS2D-update/2020-02-19-CS2D-update.html",
    "title": "ChemoSpec2D Update",
    "section": "",
    "text": "I’m pleased to announce that ChemoSpec2D, a package for exploratory data analysis of 2D NMR spectra, has been updated on CRAN and is coming to a mirror near you. Barring user reports to the contrary, I feel like the package has pretty much stabilized and is pretty robust. The main area for future expansion is to add additional data import routines. Please feel free to ask about your specific use case!\nThe most noteworthy user-facing improvements are:\n\nFunction import2DSpectra can now handle JCAMP-DX files, Bruker files exported via the TopSpin “totxt” command, and JEOL spectra exported as “generic ascii”. The design allows additional formats to be added if I have test files to play with (hint hint).\nfiles2Spectra2DObject gains a new argument allowSloppy. This experimental feature will allow one to import data sets that do not have the same dimensions. The intent here is to deal with data sets where the number of points in each dimension is similar but not identical. Additional functions will be needed to handle this kind of data. See the documentation for details.\nfiles2Spectra2DObject has also been modified to allow arguments to be passed to list.files and readJDX. This means for instance you can specify a path other than the current working directory, and have the function recurse through sub-directories. This brings files2Spectra2DObject into line with ChemoSpec::files2SpectraObject.\nFunction hats_alignSpectra2D gains new arguments dist_method and maximize which allows the user to pass their choice of distance measure through to the objective function used to evaluate the overlap of the spectra. This greatly improves the quality of the alignment.\nPlotting is simplified with the addition of two new functions to create Lists of Colors, LofC and Lists of Levels, LofL.\nThe basic color scheme for contours was updated to use a perceptually consistent low/blue -> high/red scheme, based on the principles in the colorspace package. The color-handling infrastructure was also changed to allow easy introduction of different color schemes in the future, though the user cannot yet make changes on the fly.\n\nIn addition, a number of small bugs and annoyances were taken care of, arguments tweaked and documentation improved and expanded. Several functions were rebuilt to make them more robust.\nPlease see the package website for the full changelog and all documentation.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2020,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {ChemoSpec2D {Update}},\n  date = {2020-02-19},\n  url = {http://chemospec.org/2020-02-19-CS2D-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2020. “ChemoSpec2D Update.” February 19,\n2020. http://chemospec.org/2020-02-19-CS2D-update.html."
  },
  {
    "objectID": "posts/2020-03-21-Data-Formats-Pt1/2020-03-21-Data-Formats-Pt1.html",
    "href": "posts/2020-03-21-Data-Formats-Pt1/2020-03-21-Data-Formats-Pt1.html",
    "title": "Data Sharing in the Age of Coronavirus, Part 1",
    "section": "",
    "text": "This is Part 1 of a series of posts about data formats for sharing spectroscopic data. Many folks are working from home due to a certain global pandemic. I hope you are all healthy and practicing proper social distancing!\nSharing data is intrinsic to any spectroscopic work. For many tasks, the data need never leave the instrument’s native format. Nowadays the data often goes immediately to some type of shared server, to be available for multiple users. So much of the time we don’t need to worry about format at all, especially if the acquisition software can do the chemometric analysis you need.\nIncreasingly however, publishers want all data deposited and documented somewhere in machine-readable, vendor-neutral form. This is one aspect of reproducible research, where all data and the scripts or steps needed to analyze them are provided electronically with every paper. Or, your data may be headed to one of the many databases out there, where specific formats are required for submission. And while most data acquisition softwares provide some analysis options, if you need to do serious chemometric analysis you likely need to get the data off the machine in a vendor-neutral form. So there are several reasons one should be familiar with the various means of sharing spectroscopic data.\nData sharing/exchange is admittedly a potentially mundane topic. After all, we just want to get on with the scientific question. However, it’s worth knowing something about the options and considering the future of the field. In general I’d say things are a bit of a mess with no clear path to a common format. This series of posts will cover several different vendor-neutral data sharing formats, their pros and cons and their future prospects.\n\nASCII Files\nAlmost all spectroscopic instruments have some means of exporting data as simple ASCII format files. For 1D data, these usually take the form of columns of wavelengths (or the equivalent) and some form of intensity values. There may or may not be metadata and/or headers in the file. The resolution of the data in the file is usually sufficient, but it can be as low as 8-bit precision. Simple inspection is usually enough to understand these files, and eventually, read them in with R or Python, since other than the metadata these files are simply x and y values in columns.\n2D NMR data in ASCII format are a bit more tedious to decipher. Assuming we are talking about data that has been processed, there are choices to be made about ordering the data and no standardization is evident in the wild. Do you export the data by rows (F2 values at fixed F1 value), by columns (F1 values at fixed F2 value), or an entire matrix? Do you export in a format that mirrors how we typically look at the data, namely the lowest F1 values are first and the lowest F2 values last? 2D NMR is unique among 2D plots in not having 0,0 in any corner. Or do you export in an increasing order, as though you were starting from 0,0? While there are a lot of combinations possible, through trial-and-error one can determine how the data was exported. This is naturally easier if you have a reference spectrum for comparison. I can say from experience that this task is do-able but annoying. Some vendors also export hypercomplex data, in which there is a copy of the data that has been transformed only along F2 and a copy in which transformation has occurred on both dimensions.\nIn addition to deciphering how the data is stored in an exported ASCII file, one needs to keep in mind file size, because ASCII values are not compressed. If one is dealing with IR or UV-Vis data, the typically small number of data points means the files are not large, making ASCII export a good option. For 1D NMR data with typically > 16K data points, the size of the files begins to matter a bit, especially if you have large collections of spectra, which are becoming increasingly common with autosamplers. With 2D NMR, spectra in ASCII format begin to take up some serious space, and the time needed to read in the data becomes noticable.\n\nPros & Cons of the ASCII Format\n\n\n\n\n\n\n\n  \n    Pros \n    Cons \n  \n  \n    Near-universal availability \n    Rarely any metadata \n  \n  \n    Human readible \n    Rarely any documentation \n  \n  \n     \n    Slow to parse for large data sets \n  \n  \n     \n    For 2D NMR, internal order must be deciphered \n  \n\n\n\n\n\n\nThe Future of the ASCII Format\nBecause of it’s relative simplicity, and near-universal implementation in vendor software, ASCII formatted export files are here to stay.\n\n\n\nJCAMP-DX Files\n\nThe History of JCAMP-DX Format\nThe JCAMP-DX format and standard began at a time when hard drive space was expensive and read/write/transmission errors by hardware were a real issue. This was way before the internet: we are talking about transferring data via telephone/modem, magnetic tape and simple OCR. Hence, three key concerns were to compress the data, to build in data integrity checks and to be flexible for future expansion. Two spectroscopists working with IR data, Robert McDonald and Paul Wilks Jr., published the first standard in 1988 (McDonald and Wilks 1988), with input from instrument manufacturers. From the begininng JCAMP-DX was a project of JCAMP, the Joint Committee on Atomic and Physical Data, a committee of the IUPAC. Refinements were published in 1991 (Grasselli 1991), support for NMR was added in 1993 (A. Davies and Lampen 1993), and MS in 1994 (Lampen et al. 1994) by which time the standard was at version 5 (Lampen et al. 1999). Extensions for CD (Woollett et al. 2012), ion mobility spectrometry (Baumbach et al. 2001) and electron magnetic resonance have been proposed (Cammack et al. 2006). Interestingly, there was also an attempt to describe structure (connectivity) using the format (Gasteiger et al. 1991). In 2001 a JCAMP-DX standard for NMR pulse sequences was published (A. N. Davies et al. 2001).\n\n\nAn Example\nAnother goal for the format was to have the format be both human and machine readible. The format is composed of metadata describing the data and then the compressed data. There are several compression formats possible; some are more human readible than others! Here is a simple example of a JCAMP-DX file containing part of an IR spectrum. The blue box contains the metadata, which is clearly human readible and indeed, most meanings are immediately obvious. The orange box contains the compressed data in the “DIFFDUP” format. In another post we might dissect how that works, but for now, we can clearly read the characters but they need to be translated into actual numerical values.\n\n\n\nPros & Cons of the JCAMP-DX Format\n\n\n\n\n  \n    Pros \n    Cons \n  \n  \n    Near-universal availability \n    Minimal compression by modern standards \n  \n  \n    Metadata human readible \n    Error checking makes parsing slow \n  \n  \n    Compression formats can be manually detangled for checking \n    Error checking probably no longer needed \n  \n  \n     \n    Vendors do not always follow the standard exactly \n  \n\n\n\n\n\n\nFuture of the JCAMP-DX Format\nBecause of its long history and universal availability, the JCAMP-DX format appears to be here for the long-haul in spite of its limitations. Future posts in this series will cover data sharing formats that may eventually replace JCAMP-DX.\n\n\n\n\n\n\nReferences\n\nBaumbach, JI, AN Davies, P Lampen, and H Schmidt. 2001. “JCAMP-DX. A Standard Format for the Exchange of Ion Mobility Spectrometry Data - (IUPAC recommendations 2001).” Pure and Applied Chemistry 73 (11): 1765–82. https://doi.org/10.1351/pac200173111765.\n\n\nCammack, R, Y Fann, RJ Lancashire, JP Maher, PS McIntyre, and R Morse. 2006. “JCAMP-DX for electron magnetic resonance(EMR).” Pure and Applied Chemistry 78 (3): 613–31. https://doi.org/10.1351/pac200678030613.\n\n\nDavies, AN, and P Lampen. 1993. “JCAMP-DX for NMR.” Applied Spectroscopy 47 (8): 1093–99. https://doi.org/10.1366/0003702934067874.\n\n\nDavies, Antony N., Jörg Lambert, Robert J. Lancashire, and Peter Lampen. 2001. “Guidelines for the Representation of Pulse Sequences for Solution-State Nuclear Magnetic Resonance Spectroscopy.” Pure and Applied Chemistry 73 (11): 1749–64.\n\n\nGasteiger, J., B. M. P. Hendricks, Hoever P., Jochum C., and Somberg H. 1991. “JCAMP-CS: A Standard Exchange Format for Chemical Structure Information in a Computer-Readible Form.” Applied Spectroscopy 45 (1): 4–11.\n\n\nGrasselli, JG. 1991. “JCAMP-DX, A Standard Format for Exchange of Infrared-Spectra in Computer Readible Form.” Pure and Applied Chemistry 63 (12): 1781–92. https://doi.org/10.1351/pac199163121781.\n\n\nLampen, P, H Hillig, AN Davies, and M Linscheid. 1994. “JCAMP-DX for Mass Spectrometry.” Applied Spectroscopy 48 (12): 1545–52.\n\n\nLampen, P, J Lambert, RJ Lancashire, RS McDonald, PS McIntyre, DN Rutledge, T Frohlich, and AN Davies. 1999. “An Extension to the JCAMP-DX Standard File Format, JCAMP-DX V.5.01 (IUPAC Recommendations 1999).” Pure and Applied Chemistry 71 (8): 1549–56. https://doi.org/10.1351/pac199971081549.\n\n\nMcDonald, RS, and PA Wilks. 1988. “JCAMP-DX, A Standard Format for Exchange of Infrared-Spectra in Computer Readible Form.” Applied Spectroscopy 42 (1): 151–62. https://doi.org/10.1366/0003702884428734.\n\n\nWoollett, Benjamin, Daniel Klose, Richard Cammack, Robert W. Janes, and B. A. Wallace. 2012. “JCAMP-DX for circular dichroism spectra and metadata (IUPAC Recommendations 2012).” Pure and Applied Chemistry 84 (10): 2171–82. https://doi.org/10.1351/PAC-REC-12-02-03.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2020,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {Data {Sharing} in the {Age} of {Coronavirus,} {Part} 1},\n  date = {2020-03-21},\n  url = {http://chemospec.org/2020-03-21-Data-Formats-Pt1.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2020. “Data Sharing in the Age of Coronavirus, Part\n1.” March 21, 2020. http://chemospec.org/2020-03-21-Data-Formats-Pt1.html."
  },
  {
    "objectID": "posts/2022-02-18-Key-References/2022-02-18-Key-References.html",
    "href": "posts/2022-02-18-Key-References/2022-02-18-Key-References.html",
    "title": "Chemometrics in Spectroscopy: Key References",
    "section": "",
    "text": "Jerome Workman Jr. and Howard Mark have published a very useful series of articles in Spectroscopy Online, summarizing common chemometric methods in spectroscopy, and giving the key publications on each. These are a great resource if one is learning about a technique, or if one wants to check out the fundamentals of a method you think you already know. Posting here for convenience!\nProtip: These pages load slowly in some browsers. I had the best luck with Chrome. Try the reader view for a user-friendly version that prints well (if you are in to that).\n\nSurvey of Chemometric Methods in Spectroscopy\nKey References Part 1\nKey References Part 2\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2022,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {Chemometrics in {Spectroscopy:} {Key} {References}},\n  date = {2022-02-18},\n  url = {http://chemospec.org/2022-02-18-Key-References.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2022. “Chemometrics in Spectroscopy: Key\nReferences.” February 18, 2022. http://chemospec.org/2022-02-18-Key-References.html."
  },
  {
    "objectID": "posts/2020-02-20-NMR-Align-Pt1/2020-02-20-NMR-Align-Pt1.html",
    "href": "posts/2020-02-20-NMR-Align-Pt1/2020-02-20-NMR-Align-Pt1.html",
    "title": "Aligning 2D NMR Spectra Part 1",
    "section": "",
    "text": "In this series of posts, I’ll discuss the alignment process for the case of 2D NMR, as implemented in the package ChemoSpec2D. This is Part 1. Part 2. Part 3.\n\nIn one-dimensional \\(^1\\)H NMR spectroscopy, particularly biomolecular NMR, it is frequently necessary to align spectra before chemometric or metabolomics analysis. Poor alignment arises largely from pH and ionic strength induced shifts in aqueous samples. There are a number of published alignment algorithms for the one-dimensional case. The same issue presumably exists for 2D NMR spectra, but alignment options are limited. Instead, for 2D NMR people often work with tables of peaks. Creating these tables is an extra step and decisions about what to include may leave useful information behind.\nNo doubt you’ve compared two spectra by overlaying them on the screen, or printing them out, placing them on top of each other, and holding them up to the light. Conceptually, one can “align” spectra by a similar method: just slide one of the pieces of paper up/down and left/right until the spectra are optimally aligned. But how would one do this algorithmically? A literature searched turned up only a few publications on this topic. Among these, there was only one that I felt I could implement using the description in the paper: the HATS-PR algorithm of Robinette et al. (Robinette et al. 2011).\nWe’ll discuss the HATS algorithm in a future post. As a first step however, we need to consider how we know when two spectra are properly aligned. Visual inspection won’t work, as we will encounter cases where peaks in one region align, but only at the expense of peaks in another region. How would we rank such cases? To automate this process, we need to use an objective function, basically some kind of equation, that we evaluate as we explore the alignment space. A simple but effective option is to compute the distance between the two spectra. This is done by concatenating each row of the 2D spectra to give a long vector of intensities. The distance between these vectors can then be computed using any of the standard distance definitions. Let’s illustrate, starting by taking a look at some mis-aligned data. ChemoSpec2D contains a mis-aligned data set, MUD2, for just this purpose. Here are two spectra from MUD2; note we are using the new convenience functions LofC and LofL to make it easy to overlay the spectra.\n\n\n\n\nlibrary(\"ChemoSpec2D\")\n\nLoading required package: ChemoSpecUtils\n\n\n\nAs of version 6, ChemoSpec2D offers new graphics output options\n\n\n\nFunctions plotScores and plotScree will work with the new options\n\n\nFor details, please see ?GraphicsOptions\n\n\n\nThe ChemoSpec graphics option is set to 'ggplot2'\n\n\nTo change it, do\n    options(ChemoSpecGraphics = 'option'),\n    where 'option' is one of 'base' or 'ggplot2' or'plotly'.\n\ndata(MUD2)\nmylvls <- seq(0, 30, length.out = 10)\nplotSpectra2D(MUD2, which = c(1, 6), showGrid = TRUE,\n  lvls = LofL(mylvls, 2),\n  cols = LofC(c(\"red\", \"black\"), 2, length(mylvls), 2),\n  main = \"MUD2 Spectra 1 & 6\")\n\n\n\n\nThe function sampleDist allows us to compute the distance between every pair of spectra in the MUD2 data set, and present the results as a heat map. Here are the results using cosine as the distance measure.\n\ncos_dist <- sampleDist(MUD2, method = \"cosine\",\n  main = \"Cosine Distance\")\n\n\n\n\nThe actual numerical values are in cos_dist, a matrix. Looking at the heatmap, there are some modest differences among the spectra. However, if one looks at the scale, cosine distances are only defined on [-1 … 1]. While the cosine distance is popular in many spectroscopic contexts, it’s not the best objective function for our purpose because there is little absolute difference between -1 and 1 (and for MUD2 the absolute differences are even smaller, as the range is only 0, 0.99). This limited range affects the alignment process in a subtle way that we won’t cover here (alignment is still successful, however).\nLet’s consider instead the Euclidean distance.\n\neu_dist <- sampleDist(MUD2, method = \"euclidean\",\n  main = \"Euclidean Distance\")\n\n\n\n\nIt turns of that the Euclidean distance gives a wider span of distances, which will serve us well in the next steps. Here, the range is roughly 80, 150. Note that in this plot the distance between identical spectra is zero, plotted as a white squares along the diagonal. When we used cosine as the distance, identical spectra were perfectly correlated and hence the diagonal in that plot was red.\nIn the next post I’ll discuss the general flow of the HATS algorithm, and how to carry it out using ChemoSpec2D.\n\n\n\n\nReferences\n\nRobinette, Steven L., Ramadan Ajredini, Hasan Rasheed, Abdulrahman Zeinomar, Frank C. Schroeder, Aaron T. Dossey, and Arthur S. Edison. 2011. “Hierarchical Alignment and Full Resolution Pattern Recognition of 2D NMR Spectra: Application to Nematode Chemical Ecology.” Analytical Chemistry 83 (5): 1649–57. https://doi.org/10.1021/ac102724x.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2020,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {Aligning {2D} {NMR} {Spectra} {Part} 1},\n  date = {2020-02-20},\n  url = {http://chemospec.org/2020-02-20-NMR-Align-Pt1.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2020. “Aligning 2D NMR Spectra Part 1.”\nFebruary 20, 2020. http://chemospec.org/2020-02-20-NMR-Align-Pt1.html."
  },
  {
    "objectID": "posts/2022-05-01-Protocol-Pt3/2022-05-01-Protocol-Pt3.html",
    "href": "posts/2022-05-01-Protocol-Pt3/2022-05-01-Protocol-Pt3.html",
    "title": "Metabolic Phenotyping Protocol Part 3",
    "section": "",
    "text": "If you aren’t familiar with ChemoSpec, you might wish to look at the introductory vignette first.\nIn this series of posts we are following the protocol as described in the printed publication closely (Blaise et al. 2021). The authors have also provided a Jupyter notebook. This is well worth your time, even if Python is not your preferred language, as there are additional examples and discussion for study."
  },
  {
    "objectID": "posts/2022-05-01-Protocol-Pt3/2022-05-01-Protocol-Pt3.html#supervised-analysis-with-pls-da",
    "href": "posts/2022-05-01-Protocol-Pt3/2022-05-01-Protocol-Pt3.html#supervised-analysis-with-pls-da",
    "title": "Metabolic Phenotyping Protocol Part 3",
    "section": "Supervised Analysis with PLS-DA",
    "text": "Supervised Analysis with PLS-DA\nChemoSpec carries out exploratory data analysis, which is an unsupervised process. The next step in the protocol is PLS-DA (partial least squares - discriminant analysis). I have written about ChemoSpec + PLS here if you would like more background on plain PLS. However, PLS-DA is a technique that combines data reduction/variable selection along with classification. We’ll need the mixOmics package (F et al. (2017)) package for this analysis; note that loading it replaces the plotLoadings function from ChemoSpec.\n\nlibrary(\"mixOmics\")\n\nLoading required package: MASS\n\n\nLoading required package: lattice\n\n\n\nLoaded mixOmics 6.20.0\nThank you for using mixOmics!\nTutorials: http://mixomics.org\nBookdown vignette: https://mixomicsteam.github.io/Bookdown\nQuestions, issues: Follow the prompts at http://mixomics.org/contact-us\nCite us:  citation('mixOmics')\n\n\n\nAttaching package: 'mixOmics'\n\n\nThe following object is masked from 'package:ChemoSpec':\n\n    plotLoadings\n\n\nFigure 6 shows the score plot; the results suggest that classification and modeling may be successful. The splsda function carries out a single sparse computation. One computation should not be considered the ideal answer; a better approach is to use cross-validation, for instance the bootsPLS function in the bootsPLS package (Rohart, Le Cao, and Wells (2018) which uses splsda under the hood). However, that computation is too time-consuming to demonstrate here.\n\nX <- Worms2$data\nY <- Worms2$groups\nsplsda <- splsda(X, Y, ncomp = 8)\n\n\nplotIndiv(splsda,\n  col.per.group = c(\"#FB0D16FF\", \"#FFC0CBFF\", \"#511CFCFF\", \"#2E94E9FF\"),\n  title = \"sPLS-DA Score Plot\", legend = TRUE, ellipse = TRUE)\n\n\n\n\nFigure 6: sPLS-DA plot showing classification.\n\n\n\n\nTo estimate the number of components needed, the perf function can be used. The results are in Figure 7 and suggest that five components are sufficient to describe the data.\n\nperf.splsda <- perf(splsda, folds = 5, nrepeat = 5)\nplot(perf.splsda)\n\n\n\n\nFigure 7: Evaluation of the PLS-DA performance.\n\n\n\n\nAt this point, we have several ideas of how to proceed. Going forward, one might choose to focus on accurate classification, or on determining which frequencies should be included in a predictive model. Any model will need to refined and more details extracted. The reader is referred to the case study from the mixOmics folks which covers these tasks and explains the process.\n\n\n\n\nThis post was created using ChemoSpec version 6.1.3 and ChemoSpecUtils version 1.0.0."
  },
  {
    "objectID": "posts/2022-09-10-Linear-Alg-Notes-Pt3/Linear-Alg-Notes-Pt3.html",
    "href": "posts/2022-09-10-Linear-Alg-Notes-Pt3/Linear-Alg-Notes-Pt3.html",
    "title": "Notes on Linear Algebra Part 3",
    "section": "",
    "text": "Series: Part 1 Part 2\nUpdate 19 September 2022: in “Use of outer() for Matrix Multiplication”, corrected use of “cross” to be “outer” and added example in R Also added links to work by Hiranabe.\nThis post is a survey of the linear algebra-related functions from base R. Some of these I’ve disccused in other posts and some I may discuss in the future, but this post is primarily an inventory: these are the key tools we have available. “Notes” in the table are taken from the help files.\nMatrices, including row and column vectors, will be shown in bold e.g. \\(\\mathbf{A}\\) or \\(\\mathbf{a}\\) while scalars and variables will be shown in script, e.g. \\(n\\). R code will appear like x <- y.\nIn the table, \\(\\mathbf{R}\\) or \\(\\mathbf{U}\\) is an upper/right triangular matrix. \\(\\mathbf{L}\\) is a lower/left triangular matrix (triangular matrices are square). \\(\\mathbf{A}\\) is a generic matrix of dimensions \\(m \\times n\\). \\(\\mathbf{M}\\) is a square matrix of dimensions \\(n \\times n\\).\n\n\n\nFunction\nUses\nNotes\n\n\n\n\n\noperators\n\n\n\n\n\n*\nscalar multiplication\n\n\n\n\n%*%\nmatrix multiplication\ntwo vectors \\(\\rightarrow\\) the dot product; vector + matrix \\(\\rightarrow\\) cross product (vector will be promoted as needed)1\n\n\n\nbasic functions\n\n\n\n\n\nt()\ntranspose\ninterchange rows and columns\n\n\n\ncrossprod()\nmatrix multiplication\nfaster version of t(A) %*% A\n\n\n\ntcrossprod()\nmatrix multiplication\nfaster version of A %*% t(A)\n\n\n\nouter()\nouter product & more\nsee discussion below\n\n\n\ndet()\ncomputes determinant\nuses the LU decomposition; determinant is a volume\n\n\n\nisSymmetric()\nname says it all\n\n\n\n\nConj()\ncomputes complex conjugate\n\n\n\n\ndecompositions\n\n\n\n\n\nbacksolve()\nsolves \\(\\mathbf{Rx} = \\mathbf{b}\\)\n\n\n\n\nforwardsolve()\nsolves \\(\\mathbf{Lx} = \\mathbf{b}\\)\n\n\n\n\nsolve()\nsolves \\(\\mathbf{Mx} = \\mathbf{b}\\) and \\(\\mathbf{M}^{-1}\\)\ne.g. linear systems; if given only one matrix returns the inverse\n\n\n\nqr()\nsolves \\(\\mathbf{A} = \\mathbf{QR}\\)\n\\(\\mathbf{Q}\\) is an orthogonal matrix; can be used to solve \\(\\mathbf{Ax} = \\mathbf{b}\\); see ?qr for several qr.* extractor functions\n\n\n\nchol()\nsolves \\(\\mathbf{M} = \\mathbf{L}\\mathbf{L}^{\\mathsf{T}} = \\mathbf{U}^{\\mathsf{T}}\\mathbf{U}\\)\nOnly applies to positive semi-definite matrices (where \\(\\lambda \\ge 0\\)); related to LU decomposition\n\n\n\nchol2inv()\ncomputes \\(\\mathbf{M}^{-1}\\) from the results of chol(M)\n\n\n\n\nsvd()\nsingular value decomposition\ninput \\(\\mathbf{A}^{(m \\times n)}\\); can compute PCA; details\n\n\n\neigen()\neigen decomposition\nrequires \\(\\mathbf{M}^{(n \\times n)}\\); can compute PCA; details\n\n\n\n\nOne thing to notice is that there is no LU decomposition in base R. It is apparently used “under the hood” in solve() and there are versions available in contributed packages.2\n\n\n\n\n\n\nWhat is the use of outer()?\n\n\n\n\n\nAs seen in Part 1 calling outer() on two vectors does indeed give the cross product (technically corresponding to tcrossprod()). This works because the defaults carry out multiplication.3 However, looking through the R source code for uses of outer(), the function should really be thought of in simple terms as creating all possible combinations of the two inputs. In that way it is similar to expand.grid(). Here are two illustrations of the flexibility of outer():\n\n# generate a grid of x,y values modified by a function\n# from ?colorRamp\nm <- outer(1:20, 1:20, function(x,y) sin(sqrt(x*y)/3))\nstr(m)\n\n num [1:20, 1:20] 0.327 0.454 0.546 0.618 0.678 ...\n\n\n\n# generate all combinations of month and year\n# modified from ?outer; any function accepting 2 args can be used\nouter(month.abb, 2000:2002, FUN = paste)\n\n      [,1]       [,2]       [,3]      \n [1,] \"Jan 2000\" \"Jan 2001\" \"Jan 2002\"\n [2,] \"Feb 2000\" \"Feb 2001\" \"Feb 2002\"\n [3,] \"Mar 2000\" \"Mar 2001\" \"Mar 2002\"\n [4,] \"Apr 2000\" \"Apr 2001\" \"Apr 2002\"\n [5,] \"May 2000\" \"May 2001\" \"May 2002\"\n [6,] \"Jun 2000\" \"Jun 2001\" \"Jun 2002\"\n [7,] \"Jul 2000\" \"Jul 2001\" \"Jul 2002\"\n [8,] \"Aug 2000\" \"Aug 2001\" \"Aug 2002\"\n [9,] \"Sep 2000\" \"Sep 2001\" \"Sep 2002\"\n[10,] \"Oct 2000\" \"Oct 2001\" \"Oct 2002\"\n[11,] \"Nov 2000\" \"Nov 2001\" \"Nov 2002\"\n[12,] \"Dec 2000\" \"Dec 2001\" \"Dec 2002\"\n\n\nBottom line: outer() can be used for linear algebra but its main uses lie elsewhere. You don’t need it for linear algebra!\n\n\n\n\n\n\n\n\n\nUsing outer() for matrix multiplication\n\n\n\n\n\nHere’s an interesting connection discussed in this Wikipedia entry. In Part 1 we demonstrated how the repeated application of the dot product underpins matrix multiplication. The first row of the first matrix is multiplied element-wise by the first column of the second matrix, shown in red, to give the first element of the answer matrix. This process is then repeated so that every row (first matrix) has been multiplied by every column (second matrix).\n\\[\n\\begin{multline}\n\\mathbf{A}\\mathbf{B} = \\mathbf{C} =\n\\begin{bmatrix}\n\\textcolor{red}{a_{11}} & \\textcolor{red}{a_{12}} & \\textcolor{red}{a_{13}} \\\\\na_{21} & a_{22} & a_{23} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\textcolor{red}{b_{11}} & b_{12} & b_{13} \\\\\n\\textcolor{red}{b_{21}} & b_{22} & b_{23} \\\\\n\\textcolor{red}{b_{31}} & b_{32} & b_{33} \\\\\n\\end{bmatrix} = \\\\\n\\begin{bmatrix}\n\\textcolor{red}{a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31}} & a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32} & a_{11}b_{13} + a_{12}b_{23} + a_{13}b_{33}\\\\\na_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31} & a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32} & a_{21}b_{13} + a_{22}b_{23} + a_{23}b_{33}\\\\\n\\end{bmatrix}\n\\end{multline}\n\\tag{1}\\]\nIf instead, we treat the first column of the first matrix as a column vector and outer multiply it by the first row of the second matrix as a row vector, we get the following matrix:\n\\[\n\\begin{multline}\n\\begin{bmatrix}\n\\textcolor{red}{a_{11}} & a_{12} & a_{13} \\\\\n\\textcolor{red}{a_{21}} & a_{22} & a_{23} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\textcolor{red}{b_{11}} & \\textcolor{red}{b_{12}} & \\textcolor{red}{b_{13}} \\\\\nb_{21} & b_{22} & b_{23} \\\\\nb_{31} & b_{32} & b_{33} \\\\\n\\end{bmatrix} \\Rightarrow\n\\begin{bmatrix}\n\\textcolor{red}{a_{11}b_{11}} & \\textcolor{red}{a_{11}b_{12}} & \\textcolor{red}{a_{11}b_{13}}\\\\\n\\textcolor{red}{a_{21}b_{11}} & \\textcolor{red}{a_{21}b_{12}} & \\textcolor{red}{a_{21}b_{13}}\\\\\n\\end{bmatrix}\n\\end{multline}\n\\tag{2}\\]\nNow if you repeat this process for the second column of the first matrix and the second row of the second matrix, you get another matrix. And if you do it one more time using the third column/third row, you get a third matrix. If you then add these three matrices together, you get \\(\\mathbf{C}\\) as seen in Equation 1. Notice how each element in \\(\\mathbf{C}\\) in Equation 1 is a sum of three terms? Each of those terms comes from one of the three matrices just described.\nTo sum up, one can use the dot product on each row (first matrix) by each column (second matrix) to get the answer, or you can use the outer product on the columns sequentially (first matrix) by rows sequentially (second matrix) to get several matrices, which one then sums to get the answer. It’s pretty clear which option is less work and easier to follow, but I think it’s an interesting connection between operations. The first case corresponds to view “MM1” in The Art of Linear Algebra while the second case is view “MM4”. See this work by Kenji Hiranabe.\nHere’s a simple proof in R.\n\nM1 <- matrix(1:6, nrow = 3, byrow = TRUE)\nM1\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nM2 <- matrix(7:10, nrow = 2, byrow = TRUE)\nM2\n\n     [,1] [,2]\n[1,]    7    8\n[2,]    9   10\n\ntst1 <- M1 %*% M2 # uses dot product\n# next line is sum of sequential outer products:\n# 1st col M1 by 1st row M2 + 2nd col M1 by 2nd row M2\ntst2 <- outer(M1[,1], M2[1,]) + outer(M1[,2], M2[2,])\n\nall.equal(tst1, tst2)\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nFor details see the discussion in Part 1.↩︎\nDiscussed in this Stackoverflow question, which also has an implementation.↩︎\nIn fact, for the default outer(), FUN = \"*\", outer() actually calls tcrossprod().↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2022,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {Notes on {Linear} {Algebra} {Part} 3},\n  date = {2022-09-10},\n  url = {http://chemospec.org/Linear-Alg-Notes-Pt3.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2022. “Notes on Linear Algebra Part 3.”\nSeptember 10, 2022. http://chemospec.org/Linear-Alg-Notes-Pt3.html."
  },
  {
    "objectID": "posts/2020-04-27-CSU-update/2020-04-27-CSU-update.html",
    "href": "posts/2020-04-27-CSU-update/2020-04-27-CSU-update.html",
    "title": "ChemoSpecUtils Update",
    "section": "",
    "text": "ChemoSpecUtils, a package that supports the common needs of ChemoSpec and ChemoSpec2D, has been updated to fix an unfortunate distance calculation error in version 0.4.38, released in January of this year. From the NEWS file for version 0.4.51:\n\nFunction rowDist, which supports a number of functions, was overhauled to address confusion in the documentation, and in my head, about distances vs. similarities. Also, different definitions found in the literature were documented more clearly. The Minkowski distance option was removed (ask if you want it back), code was cleaned up, documentation greatly improved, an example was added and unit tests were added. Plot scales were also corrected as necessary. Depending upon which distance option is chosen, this change affects hcaSpectra, plotSpectraDist, sampleDist and hcaScores in package ChemoSpec as well as hats_alignSpectra2D and hcaScores in package ChemoSpec2D.\n\nThis brings to mind a Karl Broman quote I think about frequently:\n\n“Open source means everyone can see my stupid mistakes. Version control means everyone can see every stupid mistake I’ve ever made.”\n– Karl Broman\n\nKarl Broman quote source\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2020,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {ChemoSpecUtils {Update}},\n  date = {2020-04-27},\n  url = {http://chemospec.org/2020-04-27-CSU-update.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2020. “ChemoSpecUtils Update.” April 27,\n2020. http://chemospec.org/2020-04-27-CSU-update.html."
  },
  {
    "objectID": "posts/2022-02-09-Imports-Suggests/2022-02-09-Imports-Suggests.html",
    "href": "posts/2022-02-09-Imports-Suggests/2022-02-09-Imports-Suggests.html",
    "title": "Do You have Stale Imports or Suggests?",
    "section": "",
    "text": "I’ve been developing packages in R for over a decade now. When adding new features to a package, I often import functions from another package, and of course that package goes in the Imports: field of the DESCRIPTION file. Later, I might change my approach entirely and no longer need that package. Do I remember to remove it from DESCRIPTION? Generally not. The same thing happens when writing a new vignette, and it can happen with the Suggests: field as well. It can also happen when one splits a packages into several smaller packages. If one forgets to delete a package from the DESCRIPTION file, the dependencies become bloated, because all the imported and suggested packages have to be available to install the package. This adds overhead to the project, and increases the possibility of a namespace conflict.\nIn fact this just happened to me again! The author of a package I had in Suggests: wrote to me and let me know their package would be archived. It was an easy enough fix for me, as it was a “stale” package in that I was no longer using it. I had added it for a vignette which I later deleted, as I decided a series of blog posts was a better approach.\nSo I decided to write a little function to check for such stale Suggests: and Import: entries. This post is about that function. As far as I can tell there is no built-in function for this purpose, and CRAN does not check for stale entries. So it was worth my time to automate the process.1\nThe first step is to read in the DESCRIPTION file for the package (so we want our working directory to be the top level of the package). There is a built in function for this. We’ll use the DESCRIPTION file from the ChemoSpec package as a demonstration.\n\n# setwd(\"...\") # set to the top level of the package\ndesc <- read.dcf(\"DESCRIPTION\", all = TRUE)\n\nThe argument all = TRUE is a bit odd in that it has a particular purpose (see ?read.dcf) which isn’t really important here, but has the side effect of returning a data frame, which makes our job simpler. Let’s look at what is returned.\n\nstr(desc)\n\n'data.frame':   1 obs. of  18 variables:\n $ Package         : chr \"ChemoSpec\"\n $ Type            : chr \"Package\"\n $ Title           : chr \"Exploratory Chemometrics for Spectroscopy\"\n $ Version         : chr \"6.1.2\"\n $ Date            : chr \"2022-02-08\"\n $ Authors@R       : chr \"c(\\nperson(\\\"Bryan A.\\\", \\\"Hanson\\\",\\nrole = c(\\\"aut\\\", \\\"cre\\\"), email =\\n\\\"hanson@depauw.edu\\\",\\ncomment = c(\"| __truncated__\n $ Description     : chr \"A collection of functions for top-down exploratory data analysis\\nof spectral data including nuclear magnetic r\"| __truncated__\n $ License         : chr \"GPL-3\"\n $ Depends         : chr \"R (>= 3.5),\\nChemoSpecUtils (>= 1.0)\"\n $ Imports         : chr \"plyr,\\nstats,\\nutils,\\ngrDevices,\\nreshape2,\\nreadJDX (>= 0.6),\\npatchwork,\\nggplot2,\\nplotly,\\nmagrittr\"\n $ Suggests        : chr \"IDPmisc,\\nknitr,\\njs,\\nNbClust,\\nlattice,\\nbaseline,\\nmclust,\\npls,\\nclusterCrit,\\nR.utils,\\nRColorBrewer,\\nser\"| __truncated__\n $ URL             : chr \"https://bryanhanson.github.io/ChemoSpec/\"\n $ BugReports      : chr \"https://github.com/bryanhanson/ChemoSpec/issues\"\n $ ByteCompile     : chr \"TRUE\"\n $ VignetteBuilder : chr \"knitr\"\n $ Encoding        : chr \"UTF-8\"\n $ RoxygenNote     : chr \"7.1.2\"\n $ NeedsCompilation: chr \"no\"\n\n\nWe are interested in the Imports and Suggests elements. Let’s look more closely.\n\nhead(desc$Imports)\n\n[1] \"plyr,\\nstats,\\nutils,\\ngrDevices,\\nreshape2,\\nreadJDX (>= 0.6),\\npatchwork,\\nggplot2,\\nplotly,\\nmagrittr\"\n\n\nYou can see there are a bunch of newlines in there (\\n), along with some version specifications, in parentheses. We need to clean this up so we have a simple list of the packages as a vector. For clean up we’ll use the following helper function.\n\nclean_up <- function(string) {\n  string <- gsub(\"\\n\", \"\", string) # remove newlines\n  string <- gsub(\"\\\\(.+\\\\)\", \"\", string) # remove parens & anything within them\n  string <- unlist(strsplit(string, \",\")) # split the long string into pieces\n  string <- trimws(string) # remove any white space around words\n}\n\nAfter we apply this to the raw results, we have what we are after, a clean list of imported packages.\n\nimp <- clean_up(desc$Imports)\nimp\n\n [1] \"plyr\"      \"stats\"     \"utils\"     \"grDevices\" \"reshape2\"  \"readJDX\"  \n [7] \"patchwork\" \"ggplot2\"   \"plotly\"    \"magrittr\" \n\n\nNext, we can search the entire package looking for these package names to see if they are used in the package. They might appear in import statements, vignettes, code and so forth, so it’s not sufficient to just look at code. This is a job for grep, but we’ll call grep from within R so that we don’t have to use the command line and transfer the results to R, that gets messy and is error-prone.\n\nif (length(imp) >= 1) { # Note 1\n  imp_res <- rep(\"FALSE\", length(imp)) # Boolean to keep track of whether we found a package or not\n  for (i in 1:length(imp)) {\n    args <- paste(\"-r -e '\", imp[i], \"' *\", sep = \"\") # assemble arguments for grep\n    g_imp <- system2(\"grep\", args, stdout = TRUE)\n    if (length(g_imp) > 1L) imp_res[i] <- TRUE # Note 2\n  }\n}\n\n\nNote 1: We ought to check if there are any imports at all. It would be a bit unusual, but it’s possible to have zero imports.\nNote 2: g_imp contains the results of the grep process. If there are imports in the package, each imported package name will be found by grep in the DESCRIPTION file. That’s not so interesting, so we don’t count it. For a package to be stale, it will be found in DESCRIPTION but no where else.\n\nWe can do the same process for the Suggests: field of DESCRIPTION. And then it would be nice to present the results in a more useable form. At this point we can put it all togther in an easy-to-use function.2\n\n# run from the package top level\ncheck_stale_imports_suggests <- function() {\n\n  # helper function: removes extra characters\n  # from strings read by read.dcf\n  clean_up <- function(string) {\n    string <- gsub(\"\\n\", \"\", string)\n    string <- gsub(\"\\\\(.+\\\\)\", \"\", string)\n    string <- unlist(strsplit(string, \",\"))\n    string <- trimws(string)\n  }\n\n  desc <- read.dcf(\"DESCRIPTION\", all = TRUE)\n\n  # look for use of imported packages\n  imp <- clean_up(desc$Imports)\n  if (length(imp) == 0L) message(\"No Imports: entries found\")\n  if (length(imp) >= 1) {\n    imp_res <- rep(\"FALSE\", length(imp))\n    for (i in 1:length(imp)) {\n      args <- paste(\"-r -e '\", imp[i], \"' *\", sep = \"\")\n      g_imp <- system2(\"grep\", args, stdout = TRUE)\n      # always found once in DESCRIPTION, hence > 1\n      if (length(g_imp) > 1L) imp_res[i] <- TRUE\n    }\n  }\n\n  # look for use of suggested packages\n  sug <- clean_up(desc$Suggests)\n  if (length(sug) == 0L) message(\"No Suggests: entries found\")\n  if (length(sug) >= 1) {\n    sug_res <- rep(\"FALSE\", length(sug))\n    for (i in 1:length(sug)) {\n      args <- paste(\"-r -e '\", sug[i], \"' *\", sep = \"\")\n      g_sug <- system2(\"grep\", args, stdout = TRUE)\n      # always found once in DESCRIPTION, hence > 1\n      if (length(g_sug) > 1L) sug_res[i] <- TRUE\n    }\n  }\n\n  # arrange output in easy to read format\n  role <- c(rep(\"Imports\", length(imp)), rep(\"Suggests\", length(sug)))\n\n  return(data.frame(\n    pkg = c(imp, sug),\n    role = role,\n    found = c(imp_res, sug_res)))\n}\n\nApplying this function to my ChemoSpec2D package (as of the date of this post), we see the following output. You can see a bunch of packages are imported but never used, so I have some work to do. This was the result of copying the DESCRIPTION file from ChemoSpec when I started ChemoSpec2D and obviously I never went back and cleaned things up.\n            pkg     role found\n1          plyr  Imports  TRUE\n2         stats  Imports  TRUE\n3         utils  Imports  TRUE\n4     grDevices  Imports  TRUE\n5      reshape2  Imports  TRUE\n6       readJDX  Imports  TRUE\n7     patchwork  Imports  TRUE\n8       ggplot2  Imports  TRUE\n9        plotly  Imports  TRUE\n10     magrittr  Imports  TRUE\n11      IDPmisc Suggests  TRUE\n12        knitr Suggests  TRUE\n13           js Suggests  TRUE\n14      NbClust Suggests  TRUE\n15      lattice Suggests  TRUE\n16     baseline Suggests  TRUE\n17       mclust Suggests  TRUE\n18          pls Suggests  TRUE\n19  clusterCrit Suggests  TRUE\n20      R.utils Suggests  TRUE\n21 RColorBrewer Suggests  TRUE\n22    seriation Suggests FALSE\n23         MASS Suggests FALSE\n24   robustbase Suggests FALSE\n25         grid Suggests  TRUE\n26        pcaPP Suggests FALSE\n27     jsonlite Suggests FALSE\n28       gsubfn Suggests FALSE\n29       signal Suggests  TRUE\n30        speaq Suggests FALSE\n31     tinytest Suggests FALSE\n32   elasticnet Suggests FALSE\n33        irlba Suggests FALSE\n34         amap Suggests FALSE\n35    rmarkdown Suggests  TRUE\n36     bookdown Suggests FALSE\n37 chemometrics Suggests FALSE\n38    hyperSpec Suggests FALSE\n\n\n\n\nFootnotes\n\n\nAs you will see in a moment, during testing I found a bunch of stale entries I need to remove from several packages!↩︎\nIn easy to use form as a Gist.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hanson2022,\n  author = {Bryan Hanson},\n  editor = {},\n  title = {Do {You} Have {Stale} {Imports} or {Suggests?}},\n  date = {2022-02-09},\n  url = {http://chemospec.org/2022-02-09-Imports-Suggests.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBryan Hanson. 2022. “Do You Have Stale Imports or\nSuggests?” February 9, 2022. http://chemospec.org/2022-02-09-Imports-Suggests.html."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Selected Projects",
    "section": "",
    "text": "R Software Packages\nAll my work can be seen at Github. Here are some highlights:\n\nChemoSpec: Exploratory Chemometrics for Spectroscopy\nChemoSpec2D: Exploratory Chemometrics for 2D Spectroscopy\nreadJDX: Import Data in the JCAMP-DX Format\nLearnPCA A series of vignettes explaining PCA from the very beginning.\nhyperSpec Tools for Spectroscopy. Joint project with Claudia Beleites, Vilmantas Gegzna, Erick Oduniyi, Sang Truong, and others.\nexCon: Interactive Exploration of Contour Data live demo\nSpecHelpers: Spectroscopy Related Utilities\nunmixR: Hyperspectral Unmixing with R, with Anton Belov, Conor McManus, Claudia Beleites and Simon Fuller\nHiveR: Hive Plots in 2D and 3D\nLindenmayeR: Functions to Explore L-Systems (Lindenmayer Systems)\n\n\n\nMisc Projects\n\nFOSS for Spectroscopy A collection of free and open source spectroscopy projects.\n\n\n\nFrom The Past!\nOver 32 years of teaching at DePauw University, I was honored to conduct research with a good number of very talented undergraduates. The last 15 years or so my focus was on plant metabolomics. Checkout this page presenting some of the work conducted by student researchers."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chemometrics and Spectroscopy Using R",
    "section": "",
    "text": "Notes on Linear Algebra Part 3\n\n\n\n\n\n\n\nR\n\n\nLinear Algebra\n\n\n\n\nBase R Functions Related to Linear Algebra\n\n\n\n\n\n\nSep 10, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nNotes on Linear Algebra Part 2\n\n\n\n\n\n\n\nR\n\n\nLinear Algebra\n\n\n\n\nMotivations - Bushwhacking through the thicket of linear algebra\n\n\n\n\n\n\nSep 1, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nNotes on Linear Algebra Part 1\n\n\n\n\n\n\n\nR\n\n\nLinear Algebra\n\n\n\n\nWhen is a cross product not a cross product? Terminology run amok!\n\n\n\n\n\n\nAug 14, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nFOSS4Spectroscopy: R vs Python\n\n\n\n\n\n\n\nFOSS\n\n\nR\n\n\nPython\n\n\nJulia\n\n\nGithub\n\n\nPyPi\n\n\n\n\nNow with more Python and Julia packages, and improved workflow\n\n\n\n\n\n\nJul 6, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nIntroducing LearnPCA\n\n\n\n\n\n\n\nR\n\n\nPCA\n\n\n\n\nImprove your understanding of PCA\n\n\n\n\n\n\nMay 3, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nMetabolic Phenotyping Protocol Part 3\n\n\n\n\n\n\n\nR\n\n\nChemoSpec\n\n\nMetabolomics\n\n\nPLS\n\n\nPLS-DA\n\n\n\n\nImplementing the Statistical Analysis in Metabolic Phenotyping Protocol of Blaise et al.\n\n\n\n\n\n\nMay 1, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nMetabolic Phenotyping Protocol Part 2\n\n\n\n\n\n\n\nR\n\n\nChemoSpec\n\n\nMetabolomics\n\n\n\n\nImplementing the Statistical Analysis in Metabolic Phenotyping Protocol of Blaise et al.\n\n\n\n\n\n\nMar 24, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nChemometrics in Spectroscopy: Key References\n\n\n\n\n\n\n\nLiterature\n\n\n\n\nKeep coming back to these sources\n\n\n\n\n\n\nFeb 18, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nDo You have Stale Imports or Suggests?\n\n\n\n\n\n\n\nR\n\n\nUtilities\n\n\nDevelopers\n\n\n\n\nHints For Package Developers\n\n\n\n\n\n\nFeb 9, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nMetabolic Phenotyping Protocol Part 1\n\n\n\n\n\n\n\nR\n\n\nChemoSpec\n\n\nMetabolomics\n\n\n\n\nImplementing the Statistical Analysis in Metabolic Phenotyping Protocol of Blaise et al.\n\n\n\n\n\n\nFeb 1, 2022\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nGSOC 2021: New Graphics for ChemoSpec\n\n\n\n\n\n\n\nR\n\n\nChemoSpec\n\n\nChemoSpecUtils\n\n\nChemoSpec2D\n\n\nGSOC\n\n\n\n\nMajor improvements!\n\n\n\n\n\n\nOct 13, 2021\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nGSOC 2021: hyperSpec and ChemoSpec!\n\n\n\n\n\n\n\nR\n\n\nhyperSpec\n\n\nChemoSpec\n\n\nGSOC\n\n\n\n\nI’m gonna be busy!\n\n\n\n\n\n\nMay 22, 2021\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nAutomatically Searching Github Repos by Topic\n\n\n\n\n\n\n\nR\n\n\nGithub\n\n\nFOSS\n\n\nhttr\n\n\n\n\nHow to find packages of interest\n\n\n\n\n\n\nApr 19, 2021\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nUsing Github Actions and drat to Deploy R Packages\n\n\n\n\n\n\n\nR\n\n\nGithub Actions\n\n\ndrat\n\n\nGSOC\n\n\nhyperSpec\n\n\n\n\nAutomating a tedious task\n\n\n\n\n\n\nApr 11, 2021\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nSpectroscopy Suite Update\n\n\n\n\n\n\n\nR\n\n\nChemoSpec\n\n\nChemoSpec2D\n\n\nChemoSpecUtils\n\n\nreadJDX\n\n\n\n\nUpdates for the new version of R\n\n\n\n\n\n\nMar 27, 2021\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nInterfacing ChemoSpec to PLS\n\n\n\n\n\n\n\nR\n\n\nPLS\n\n\nChemoSpec\n\n\n\n\nIt’s easy to connect the two packages\n\n\n\n\n\n\nFeb 8, 2021\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nGSOC Wrap Up\n\n\n\n\n\n\n\nR\n\n\nhyperSpec\n\n\nGSOC\n\n\nGuest Post\n\n\n\n\nGuest post by Erick Oduniyi\n\n\n\n\n\n\nSep 8, 2020\n\n\nErick Oduniyi\n\n\n\n\n\n\n\n\nSimulating Spectroscopic Data Part 1\n\n\n\n\n\n\n\nR\n\n\nSimulated Data\n\n\nSpecHelpers\n\n\nBaseline\n\n\n\n\nFaking it…\n\n\n\n\n\n\nJun 28, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nFortifying hyperSpec: Getting Ready for GSOC\n\n\n\n\n\n\n\nR\n\n\nhyperSpec\n\n\nGSOC\n\n\n\n\nRe-factoring hyperSpec\n\n\n\n\n\n\nMay 7, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nChemoSpecUtils Update\n\n\n\n\n\n\n\nR\n\n\nChemoSpecUtils\n\n\n\n\nImprovments to ChemoSpecUtils\n\n\n\n\n\n\nApr 27, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nSpectral Heatmaps\n\n\n\n\n\n\n\nR\n\n\nHeatmap\n\n\nSeriation\n\n\nChemoSpec\n\n\n\n\nInsights into how samples and frequencies affect clustering\n\n\n\n\n\n\nApr 25, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nData Sharing in the Age of Coronavirus, Part 1\n\n\n\n\n\n\n\nData Formats\n\n\nJCAMP-DX\n\n\nASCII\n\n\n\n\nASCII & JCAMP-DX formats\n\n\n\n\n\n\nMar 21, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nAligning 2D NMR Spectra Part 3\n\n\n\n\n\n\n\nR\n\n\nChemoSpec2D\n\n\nAlignment\n\n\nNMR\n\n\n\n\nPutting it all together\n\n\n\n\n\n\nMar 4, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nAligning 2D NMR Spectra Part 2\n\n\n\n\n\n\n\nR\n\n\nChemoSpec2D\n\n\nAlignment\n\n\nNMR\n\n\n\n\nImplementing the HATS-PR algorithm\n\n\n\n\n\n\nMar 2, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nAligning 2D NMR Spectra Part 1\n\n\n\n\n\n\n\nR\n\n\nChemoSpec\n\n\nAlignment\n\n\nNMR\n\n\n\n\nQuantifying the mis-alignment\n\n\n\n\n\n\nFeb 20, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nChemoSpec2D Update\n\n\n\n\n\n\n\nR\n\n\nChemoSpec2D\n\n\n\n\nImprovements to ChemoSpec2D\n\n\n\n\n\n\nFeb 19, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nExploring Github Topics\n\n\n\n\n\n\n\nR\n\n\nGithub\n\n\n\n\nScrape Github for Repos of Interest\n\n\n\n\n\n\nJan 25, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nChemoSpec Update\n\n\n\n\n\n\n\nR\n\n\nChemoSpec\n\n\n\n\nImprovements to ChemoSpec\n\n\n\n\n\n\nJan 24, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nChemoSpecUtils Update\n\n\n\n\n\n\n\nR\n\n\nChemoSpecUtils\n\n\n\n\nNew version of ChemoSpecUtils\n\n\n\n\n\n\nJan 22, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nFOSS for Spectroscopy Update\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\nImprovements to FOSS for Spectroscopy\n\n\n\n\n\n\nJan 22, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nreadJDX Overhaul\n\n\n\n\n\n\n\nR\n\n\nreadJDX\n\n\nData Formats\n\n\n\n\nImprovements to readJDX\n\n\n\n\n\n\nJan 2, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\n\n\nFOSS for Spectroscopy\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\nA Collection of Free and Open Source Spectroscopy Resources\n\n\n\n\n\n\nJan 1, 2020\n\n\nBryan Hanson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "The Back Story",
    "section": "",
    "text": "About This Blog\n\n“Open source means everyone can see my stupid mistakes. Version control means everyone can see every stupid mistake I’ve ever made.”\n– Karl Broman\n\nThe purpose of this blog is to provide a place to share my work in the area of Chemometrics & Spectroscopy using FOSS (free and open source) tools. Mostly I work with R but I use other tools as needed. Some areas I expect to write about from time-to-time include:\n\nMajor updates to R packages I author See here\nTutorials on how to accomplish certain tasks, generally inspired by user questions.\nDevelopments in the chemometrics and spectroscopy community.\nOccasionally, discussion of interesting papers.\n\n\n\nAbout Your Host\nYou can reach me at hanson@depauw.edu. Occasionally I am on Twitter.\n\nNow\nI am currently a freelance R consultant with expertise in:\n\nDevelopment of R packages supporting science, especially spectroscopy and chemometrics\nSpectroscopy (NMR, IR, UV-Vis etc)\nChemometrics\nExploratory Data Analysis\nData Visualizations\nData Management: organization, cleaning\nReproducible Research: the automated writing of research reports\nR Training\nWeb page creation and maintenance\nFamiliar with R, markdown, LaTeX, html, css, JavaScript\nResume\n\n\n\nThen\nI retired in June 2018 from DePauw University after 32 years of teaching chemistry and biochemistry. It was a good run. No regrets. Just time to do something else.\n\nKarl Broman quote source"
  },
  {
    "objectID": "PlantMetabolomics.html",
    "href": "PlantMetabolomics.html",
    "title": "Plant Metabolomics: Where Spectroscopy Meets Evolution & Ecology",
    "section": "",
    "text": "This page honors and makes available the research of my students. However, it is no longer being updated, as I retired in June 2018.\nIn my group, we conduct research on how plants respond to the stress of climate change. Such stress can take the form of too much heat, too much salt in the soil, or too little water. We use methods from metabolomics, chemistry and ecology, and benefit from collaboration with Prof. Dana Dudle in the Biology Department. We greatly appreciate support from DePauw’s Science Research Fellows program (now discontinued), the Faculty Development Committee, and the Mellon Foundation (through FDC).\nOur plant of choice is Portulaca oleracea, a weed more commonly known as purslane. We have chosen purslane because it is easy to grow and is interesting from a medicinal/nutritional perspective - it has the most omega-3 fatty acids of any plant. Our original objective was to determine whether purslane’s response to stress has a genetic component which also contributes to reproductive fitness. We have been able to confirm this, and we are now focussed on understanding the molecular nature of purslane’s response: What pathways are activated? What molecules are involved?\nOur approach is to blend metabolomic and ecological methods. Metabolomics is the study of an organism’s metabolites under a controlled set of conditions, in our case, normal versus stressful conditions. As far as possible, one tries to measure all the metabolites at once, in a holistic fashion, which is not an easy feat. Typically, this is done with NMR, MS, IR, or other forms of spectroscopy. We also supplement these instrumental techniques with more traditional single point chemical measurements such as antioxidant levels. From the ecological perspective, we record parameters of plant growth that represent measures of fitness, such as biomass produced, the number of flowers, and so forth. To be meaningful, we need to conduct these experiments on large numbers of plants. The resulting data sets, composed of very different sorts of measurements, represent the state of the plant under the conditions tested. We use various statistical methods to figure out which treatments have produced an interesting response, and whether those responses vary with genotype. Our statistical analyses are done with the open source computational statistical program R and in the case of spectroscopic data, with the ChemoSpec package for R, written by Bryan.\n\n\n\n\n\n\n\n\n\n\nThe following students have carried out this research. Click on their names to download the posters they prepared about their work:\n\nAcademic year 2016-2017\n\nEmma Veon\n\nSummer 2015\n\nShannon Jager\nBrian Saulnier\n\nSummer 2013\n\nKristina Mulry Kristina’s work has been published\n\nFall 2012\n\nPolly Haight\n\nSummer 2011\n\nMatt Kukurugya\nPolly Haight\nVincent Guzzetta\nMatt Keinsley\nPoster presented at the joint annual meeting of the Society for Economic Botany and The Botanical Society of America in St. Louis.\n\nSummer 2010\n\nElizabeth Botts\nMatt Keinsley\n\nSummer 2009 (The Pioneers!)\n\nTanner Miller\nCourtney Brimmer\nKelly Summers\n\n2008 (Bryan’s first metabolomics work!)\n\nPoster evaluating various brands of Saw Palmetto (Serenoa repens) by NMR and IR. Presented at the Society for Economic Botany meeting, July 2008."
  }
]